[
  {
    "objectID": "edu/2022-05-13-Cloud-Computing-for-HPC-users/index.html",
    "href": "edu/2022-05-13-Cloud-Computing-for-HPC-users/index.html",
    "title": "Cloud Computing for HPC Users",
    "section": "",
    "text": "Introductory webinar translating High Performance Computing (HPC) skills to cloud computing."
  },
  {
    "objectID": "edu/2022-05-13-Cloud-Computing-for-HPC-users/index.html#learning-objectives",
    "href": "edu/2022-05-13-Cloud-Computing-for-HPC-users/index.html#learning-objectives",
    "title": "Cloud Computing for HPC Users",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify similarities and differences between HPC and Cloud computing\nRun any script (BASH, R, Python) on the UKB RAP platform\nRun single and distributed jobs"
  },
  {
    "objectID": "edu/2022-05-13-Cloud-Computing-for-HPC-users/index.html#slides",
    "href": "edu/2022-05-13-Cloud-Computing-for-HPC-users/index.html#slides",
    "title": "Cloud Computing for HPC Users",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "edu/2022-01-28-Workflow-Description-Language/index.html",
    "href": "edu/2022-01-28-Workflow-Description-Language/index.html",
    "title": "Workflow Description Language (WDL) on RAP",
    "section": "",
    "text": "Two part Webinar about how to run workflows building using Workflow Description Lanage (WDL) on the UK Biobank Research Analysis Platform."
  },
  {
    "objectID": "edu/2022-01-28-Workflow-Description-Language/index.html#learning-objectives",
    "href": "edu/2022-01-28-Workflow-Description-Language/index.html#learning-objectives",
    "title": "Workflow Description Language (WDL) on RAP",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nList the advantages of the WDL apps and workflows\nUse dxCompiler to compile workflows\nDefine data types, and basic WDL syntax\nWrite tasks and workflows using the WDL Libraries built-in resources\nRefine workflows with metadata\nCompute dynamic resource requirements\nIntegrate the use of containers into workflows\nCreate a Docker image from scratch."
  },
  {
    "objectID": "edu/2022-01-28-Workflow-Description-Language/index.html#intro-to-wdl-part-1",
    "href": "edu/2022-01-28-Workflow-Description-Language/index.html#intro-to-wdl-part-1",
    "title": "Workflow Description Language (WDL) on RAP",
    "section": "Intro to WDL, Part 1",
    "text": "Intro to WDL, Part 1"
  },
  {
    "objectID": "edu/2022-01-28-Workflow-Description-Language/index.html#building-apps-on-ukb-rap",
    "href": "edu/2022-01-28-Workflow-Description-Language/index.html#building-apps-on-ukb-rap",
    "title": "Workflow Description Language (WDL) on RAP",
    "section": "Building Apps on UKB RAP",
    "text": "Building Apps on UKB RAP"
  },
  {
    "objectID": "edu/2021-03-20-r-bootcamp/index.html",
    "href": "edu/2021-03-20-r-bootcamp/index.html",
    "title": "R Bootcamp",
    "section": "",
    "text": "Course Description\nIn this course, we cover the basics of visualization, data munging, and basic statistics using the tidyverse in R. It is freely available to everyone and runs off the Binder cloud service. Written with Jessica Minnier.\n\n\nLink\nCourse Website\n\n\n\n\nCitationBibTeX citation:@online{laderas2019,\n  author = {Ted Laderas and Ted Laderas and Jessica Minnier},\n  title = {R {Bootcamp}},\n  date = {2019-01-20},\n  url = {https://laderast.github.io//edu/2021-03-20-r-bootcamp},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, Ted Laderas, and Jessica Minnier. 2019. “R\nBootcamp.” January 20, 2019. https://laderast.github.io//edu/2021-03-20-r-bootcamp."
  },
  {
    "objectID": "edu/2021-03-19-an-introduction-to-distill/index.html",
    "href": "edu/2021-03-19-an-introduction-to-distill/index.html",
    "title": "An Introduction to {distill} for websites",
    "section": "",
    "text": "Want to learn how to build a website using {distill}? This is a short blog post explaining the basics of Distill.\nhttps://github.com/laderast/distill_website/\n\nWorkshop Video\n\n\n\n\nLearning Objectives\n\nLearn the basic components of a distill website\nBuild and preview your website using the Build tab\nCustomize the website with your bio and your picture\nAdd links and entries to your Navbar Menu\nAdd knitted html files as article links\nGet your website live using Netlify Drop\nUpdate your website by claiming it and registering it\nChange your domain name to a better one.\n\n\n\nBefore You Start\n\nInstall R/RStudio Desktop to your Computer.\nInstall the following packages in R:\n\ninstall.packages(c(\"tidyverse\", \"postcards\", \"distill\", \"usethis\"))\n\nCreate a website project in your home directory using usethis.\n\nusethis::use_course(url=\"laderast/distill_website\",destdir=\".\")\n\nTake a look at the website before we get started:\n\nhttps://distill-example.netlify.app/\n\nGather up the follwing materials:\n\n\nLinks to Social Media (twitter, linkedin, etc)\nBio\nHeadshot\nKnitted .html files you want to share\n\n\nSign up for a netlify account and log in: https://netlify.com\n\n\n\nTaking a tour of the Project\nThese are the main files for the project.\n\nindex.Rmd - This is the main website page.\nabout.Rmd - This is a nice looking about page built using the {postcards} package.\nimage - A folder. Any images you put in here can be accessed by image/ted.jpg in your pages.\narticles/ - A folder. This is a nice place to park your articles. For right now, it’s probably easier to have self-contained articles (single html files)\n_site.yml - Customize this to change menus and links\n_site/ folder - this contains your rendered website - you’ll drop this folder into Netlify Drop and it will serve it.\ntheme.css - this is where you can set appearance options, such as font, font-size, and colors.\n\n\n\nHow does {distill} work?\n{distill} is what is called a static site generator. It takes Markdown and Rmarkdown and converts them to . .html files.\nMuch like any RMarkdown file, {distill} uses {knitr} and pandoc to build your website files that are contained in an RStudio Project. It knits your .Rmd files, converting them to .html files to a folder. The default name of this folder is called _site and it contains all of them files you need to upload to make a website.\n\n\nWhat is Netlify?\nNetlify is what is called a hosting service. This is a network of computers called web servers that are accessible via web addresses that will serve your website files when they are requested by a web browser.\nThe amazing thing about Netlify is that it is mostly free and it is very fast, no matter where you are (they have web servers almost everywhere).\nWe’ll use Netlify Drop to get our website files up and accessible as quickly as possible.\n\n\nCustomize your about links\nTake a look at about.Rmd and start filling out the front matter with your own links:\nlinks:\n  - label: LinkedIn\n    url: \"https://www.linkedin.com/in/ted-laderas-0714a92/\"\n  - label: Twitter\n    url: \"https://twitter.com/tladeras\"\n  - label: Portfolio\n    url: \"index.html\"\n  - label: Email\n    url: \"mailto:email@email.com\"\n\n\nAdding a photo\nAdd your photo to the images folder. Change the line in about.Rmd:\nimage: \"image/ted.jpg\"\nto the name of your file. For example, if your file is named jane.jpg and you put it in images:\nimage: \"image/jane.jpg\"\n\n\nBuilding your website using the Build Tab\nIn the top right panel in RStudio (next to Environment and History), there is a “Build” Tab.\nPress the Build Website to run knitr, which will knit your website to the _site folder. This is where your rendered content lives.\n\n\n\nBuild Website Tab\n\n\n\n\nPreviewing your website\n\nOpen the _site folder and click on the index.html file (make sure you’re viewing in web browser)\n\nThis is your main link to the website (the entry point). For example, if I was hosting my website at https://laderast.github.io/, this would be the first page that I would see.\n\nYour about page is available as about.html in the _site folder.\nClick away and make sure that everything works (links in menu, etc). If not, update the _site.yml and build it again.\n\n\n\nTry out different postcards themes\nThe postcards package has the following built in themes:\n\njolla\njolla_blue\nonofre\ntrestles - which your current site uses\n\nChange this line in your about.Rmd file to the theme of your interest and start building again:\noutput:\n  postcards::trestles\n\n\nAdd Your Rendered .html files to the articles/ folder\nYou can now add your articles to the articles/ folder.\nThere are a couple example articles here. Add your own files here.\nIn general, you’ll put knitted html articles here. Distill does not rebuild articles, it leaves that up to you.\nThe relative path to access articles is like this:\narticles/crops.html\nYou’ll use this when adding links to your menu.\n\n\nCustomize the Menu\nThe menu lives in the _site.yml file:\nnavbar:\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    - text: \"Articles\"\n      menu:\n          -  text: \"dplyr::slice()\"\n             href: articles/slice.html\n          -  text: \"Crop Yields\"\n             href: articles/crops.html\nAdd another menu entry under articles, or modify the above entries to have a link to your articles.\nBuild your website again and preview it to make sure the links work.\n\n\nGetting Your Website Online\nWe’ll take the _site folder with our generated website and drop this entire folder into Netlify Drop.\nhttps://drop.netlify.com\n\n\n\n\n\n\n\nUpdating Your Website\nThe first thing you want to do is claim your site and register for a Netlify account. That ties your newly created website to your account so you can update it.\nWhen you update your website with the Build Website button, you’ll drag the _site folder onto the deploy zone. This is under the deploy tab:\n\n\n\n\n\n\nMore info here: https://docs.netlify.com/site-deploys/create-deploys/#drag-and-drop\n\n\nCustomize Your Domain\nThat crazy name is the address of your site. To change it, you can click on the Domain Settings button:\n\n\n\nDomain Settings Button\n\n\nIn the following page, click the Options >> Edit Site Name button. You can change the first part of the domain, such as “myportfolio.netlify.app).\n\n\n\nEdit Site Name Button\n\n\n\n\nStyling your website\nNote that this only applies to the main distill website and not the about.html, since that is styled separately.\nIn _site.yml, try uncommenting this line and seeing how the site changes.\n#theme: theme.css\nYou can modify the appearance of your website by altering the theme.css file. Much more info about this here:\nhttps://rstudio.github.io/distill/website.html#theming\n\n\nCreating New Websites\nIf you want to start from scratch, I highly recommend the Distill tutorial here:\nhttps://rstudio.github.io/distill/website.html\nYou may want to setup your webpage as a blog, which lets you add posts by date:\nhttps://rstudio.github.io/distill/blog.html\n\n\nPutting your site code on GitHub\nThis is beyond the scope of this tutorial, but you can put your site code up on GitHub as well. This has the following advantages:\n\nLets others contribute to your website\nCan host on GitHub Pages as well\nLets others reuse your code for their own website\n\nIf you’re interested in this, I really recommend Happy Git with R as a way to get started.\n\n\nAcknowledgements\nThank you to RStudio for the {distill} package. It is so great!\nPortions of this tutorial are adapted from: https://rstudio.github.io/distill/website.html and from https://rstudio.com/resources/webinars/sharing-on-short-notice-how-to-get-your-materials-online-with-r-markdown/\n\n\n\n\nCitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas and Ted Laderas},\n  title = {An {Introduction} to \\{Distill\\} for Websites},\n  date = {2021-03-19},\n  url = {https://laderast.github.io//edu/2021-03-19-an-introduction-to-distill},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, and Ted Laderas. 2021. “An Introduction to {Distill}\nfor Websites.” March 19, 2021. https://laderast.github.io//edu/2021-03-19-an-introduction-to-distill."
  },
  {
    "objectID": "edu/2021-03-20-ready-for-r/index.html",
    "href": "edu/2021-03-20-ready-for-r/index.html",
    "title": "Ready for R",
    "section": "",
    "text": "Course Description\nThis course is meant to be a gentle introduction to using R/Rstudio in your daily work. It aims to teach useful skills (visualization, data loading, data filtering and manipulation, simple statistics) that students can immediately use in their work. No prerequisites or previous experience required.\nIt is not meant to be a substitute for a full programming course or a full course in statistics. In the end, students will apply these skills to a final project.\n\n\nLearning Objectives\n\nUnderstand and utilize R/RStudio.\nUnderstand basic data types and data structures in R.\nFamiliarize and load data files (Excel, Comma Separated Value files) into R/Rstudio, with tips on formatting.\nVisualize datasets using ggplot2 and understand how to build basic plots using ggplot2 syntax.\nFilter and format data in R for use with various routines.\nExecute and Interpret some basic statistics in R.\n\n\n\nCourse Links\nCourse Website Course Syllabus Course Mailing List\nSign up for the course at: https://ready4r.netlify.com/mailing/\n\n\n\n\nCitationBibTeX citation:@online{laderas2020,\n  author = {Ted Laderas and Ted Laderas and Aaron Coyner},\n  title = {Ready for {R}},\n  date = {2020-03-20},\n  url = {https://laderast.github.io//edu/2021-03-20-ready-for-r},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, Ted Laderas, and Aaron Coyner. 2020. “Ready for\nR.” March 20, 2020. https://laderast.github.io//edu/2021-03-20-ready-for-r."
  },
  {
    "objectID": "edu/2021-03-20-bmi-569-data-analytics/index.html",
    "href": "edu/2021-03-20-bmi-569-data-analytics/index.html",
    "title": "BMI 569: Data Analytics",
    "section": "",
    "text": "Practical coursework in R/SQL for our hybrid Data Analytics Course taught with Kaiser Permanente. This course has been taught from 2013 to the present during summer quarter for students in the Biomedical Informatics program at OHSU.\nThis repo includes the practical coursework in R/SQL for our hybrid Data Analytics Course taught with Kaiser Permanente. This course has been taught from 2013 to the present during summer quarter for students in the Biomedical Informatics program at OHSU.\nPlease note that this is only half of the course, which also includes discussions on organizational behavior, implementing analytics projects within an organization, and discussions of the LACE score.\n\n\n\nUnderstand the basics of using R/Rstudio\nLearn and apply basic SQL queries to a synthetic patient cohort\nImplement a metric for predicting 30 day hospital readmissions (LACE score) for this patient cohort.\nLearn, understand, and apply simple visualizations to communicate findings\nUnderstand how to build and interpret logistic regression models\n\n\n\n\nCourse Website Course Repo\n\n\n\nThis course material is released under an Apache 2.0 License."
  },
  {
    "objectID": "edu/2021-10-14-ukbiobank-webinar/index.html",
    "href": "edu/2021-10-14-ukbiobank-webinar/index.html",
    "title": "UK Biobank Overview Webinar",
    "section": "",
    "text": "Introductory Webinar for the UK Biobank Research Analysis Platform."
  },
  {
    "objectID": "edu/2021-10-14-ukbiobank-webinar/index.html#learning-objectives",
    "href": "edu/2021-10-14-ukbiobank-webinar/index.html#learning-objectives",
    "title": "UK Biobank Overview Webinar",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nArticulate the benefits of analyzing UKB data on RAP\nList the different kinds of analysis strategies on RAP and how they map to your research interests\nExplain projects, bulk data, and project structure on UKB RAP\nArticulate the relationship between a dataset and a cohort\nIdentify which best practices and applications are right for your research interests\nKnow about the UKB RAP community and further skills courses"
  },
  {
    "objectID": "edu/2021-10-14-ukbiobank-webinar/index.html#video",
    "href": "edu/2021-10-14-ukbiobank-webinar/index.html#video",
    "title": "UK Biobank Overview Webinar",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "edu/2022-10-10-bash-for-bioinformatics/index.html",
    "href": "edu/2022-10-10-bash-for-bioinformatics/index.html",
    "title": "Bash for Bioinformatics",
    "section": "",
    "text": "This is an online and free textbook that covers the essential Bash scripting skills you’ll need to do execute work on the DNAnexus cloud platform."
  },
  {
    "objectID": "edu/2022-10-10-bash-for-bioinformatics/index.html#learning-objectives",
    "href": "edu/2022-10-10-bash-for-bioinformatics/index.html#learning-objectives",
    "title": "Bash for Bioinformatics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply bash scripting to your own work\nArticulate basic Cloud Computing concepts that apply to the DNAnexus platform\nLeverage bash scripting and the dx-toolkit to execute jobs on the DNAnexus platform\nExecute batch processing of multiple files in a project on the DNAnexus platform\nMonitor, profile, terminate and retry jobs to optimize costs\nManage software dependencies reproducibly using container-based technologies such as Docker\n\nBook Repository"
  },
  {
    "objectID": "edu/2021-03-20-a-gradual-introduction-to-shiny/index.html",
    "href": "edu/2021-03-20-a-gradual-introduction-to-shiny/index.html",
    "title": "A gRadual intRoduction to Shiny",
    "section": "",
    "text": "Description\nThis was a two hour workshop given for OHSU BioData Club on 2021-01-28 introducing the basic concepts of Shiny. It runs about two hours long.\nThere are videos for each part, along with slides. We use an RStudio.cloud project to try the basic Shiny concepts.\n\n\nAudience\nIntermediate R users and students. Students should be familiar with {ggplot2}, {dplyr}, and how functions work.\n\n\nLearning Objectives\n\nLearn the basic architecture of Shiny Apps\nLearn how server and ui communicate with the input and output objects\nAdd ggplot2 code to our app and display it with plotOutput and renderOutput\nAdd a control to control the aesthetics of our ggplot\nMake a reactive dataset using reactive expressions and control it with a slider\nUse {plotly} to add tooltips to our data\nLearn about the Shiny Widget Gallery and how to use it to add controls to your app\n\n\n\nLink\nWorkshop Website\n\n\n\n\nCitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas and Ted Laderas and Jessica Minnier and Dar’ya\n    Pozhidayeva and Pierrette Lo},\n  title = {A {gRadual} {intRoduction} to {Shiny}},\n  date = {2021-01-28},\n  url = {https://laderast.github.io//edu/2021-03-20-a-gradual-introduction-to-shiny},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, Ted Laderas, Jessica Minnier, Dar’ya Pozhidayeva, and\nPierrette Lo. 2021. “A gRadual intRoduction to Shiny.”\nJanuary 28, 2021. https://laderast.github.io//edu/2021-03-20-a-gradual-introduction-to-shiny."
  },
  {
    "objectID": "edu/2021-03-22-data-scavenger-hunt-exploring-nhanes/index.html",
    "href": "edu/2021-03-22-data-scavenger-hunt-exploring-nhanes/index.html",
    "title": "Data Scavenger Hunt: Exploring NHANES",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2019,\n  author = {Ted Laderas and Ted Laderas and Jessica Minnier and Thomas\n    Frohwein},\n  title = {Data {Scavenger} {Hunt:} {Exploring} {NHANES}},\n  date = {2019-01-25},\n  url = {https://laderast.github.io//edu/2021-03-22-data-scavenger-hunt-exploring-nhanes},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, Ted Laderas, Jessica Minnier, and Thomas Frohwein. 2019.\n“Data Scavenger Hunt: Exploring NHANES.” January 25, 2019.\nhttps://laderast.github.io//edu/2021-03-22-data-scavenger-hunt-exploring-nhanes."
  },
  {
    "objectID": "edu/2021-03-20-hpc-tutorial/index.html",
    "href": "edu/2021-03-20-hpc-tutorial/index.html",
    "title": "HPC Tutorial",
    "section": "",
    "text": "Description\nSelf paced tutorial for learning about cluster computing using exacloud, the exascale computing cluster at OHSU. Covers simple jobs, batch jobs, and interactive jobs using SLURM.\n\n\nLearning Objectives\n\nLearn about cluster computing architectures, including compute nodes and head nodes\nUnderstand the role of the head node in a SLURM architecture.\nQuery and learn about job queues, nodes\nRun a simple SLURM job using srun\nBatch multiple jobs using sbatch\nExecute interactive jobs using srun\n\n\n\nLinks\nWorkshop Website Workshop Repo\n\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {HPC {Tutorial}},\n  date = {2017-06-22},\n  url = {https://laderast.github.io//edu/2021-03-20-hpc-tutorial},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “HPC Tutorial.” June 22, 2017. https://laderast.github.io//edu/2021-03-20-hpc-tutorial."
  },
  {
    "objectID": "edu/2021-03-20-data-storytelling/index.html",
    "href": "edu/2021-03-20-data-storytelling/index.html",
    "title": "Data Storytelling",
    "section": "",
    "text": "You are making a figure for your paper and want it to be the best it can be. Come and learn techniques for communicating your findings clearly. Learn about the role of color, annotations, and simplifying your figures to communicate effectively.\nRecording Presentation Code\n\n\n\nCitationBibTeX citation:@online{laderas2020,\n  author = {Ted Laderas and Ted Laderas},\n  title = {Data {Storytelling}},\n  date = {2020-04-01},\n  url = {https://laderast.github.io//edu/2021-03-20-data-storytelling},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, and Ted Laderas. 2020. “Data Storytelling.”\nApril 1, 2020. https://laderast.github.io//edu/2021-03-20-data-storytelling."
  },
  {
    "objectID": "edu/2021-10-19-Jupyter-Notebooks/index.html",
    "href": "edu/2021-10-19-Jupyter-Notebooks/index.html",
    "title": "UK Biobank Webinar: Using JupyterLab",
    "section": "",
    "text": "Two part webinar on utilizing JupyterLab on the DNAnexus platform."
  },
  {
    "objectID": "edu/2021-10-19-Jupyter-Notebooks/index.html#learning-objectives-part-1",
    "href": "edu/2021-10-19-Jupyter-Notebooks/index.html#learning-objectives-part-1",
    "title": "UK Biobank Webinar: Using JupyterLab",
    "section": "Learning Objectives (Part 1)",
    "text": "Learning Objectives (Part 1)\n\nExplain the relationship between JupyterLab and a UKB RAP project \nSelect  the appropriate JupyterLab instance given your workflow requirements\nAccess  files and Jupyter Notebooks from a project into the JupyterLab environment\nRun a Jupyter Notebook in the JupyterLab environment and upload results back into the project \nUtilize and install software dependencies into the JupyterLab environment"
  },
  {
    "objectID": "edu/2021-10-19-Jupyter-Notebooks/index.html#video-part-1-jupyterlab-basics",
    "href": "edu/2021-10-19-Jupyter-Notebooks/index.html#video-part-1-jupyterlab-basics",
    "title": "UK Biobank Webinar: Using JupyterLab",
    "section": "Video Part 1: JupyterLab Basics",
    "text": "Video Part 1: JupyterLab Basics"
  },
  {
    "objectID": "edu/2021-10-19-Jupyter-Notebooks/index.html#learning-objectives-part-1-1",
    "href": "edu/2021-10-19-Jupyter-Notebooks/index.html#learning-objectives-part-1-1",
    "title": "UK Biobank Webinar: Using JupyterLab",
    "section": "Learning Objectives (Part 1)",
    "text": "Learning Objectives (Part 1)\n\nExplain the difference between a single JupyterLab instance and a Spark JupyterLab cluster\nExplain the basic structure of the RAP Dataset\nAccess and manipulate RAP Data and cohorts using the dxdata package on Spark JupyterLab\nArticulate possible further Spark enabled analyses"
  },
  {
    "objectID": "edu/2021-10-19-Jupyter-Notebooks/index.html#part-2-using-spark-jupyterlab-to-access-phenotype-data",
    "href": "edu/2021-10-19-Jupyter-Notebooks/index.html#part-2-using-spark-jupyterlab-to-access-phenotype-data",
    "title": "UK Biobank Webinar: Using JupyterLab",
    "section": "Part 2: Using Spark JupyterLab to access phenotype data",
    "text": "Part 2: Using Spark JupyterLab to access phenotype data"
  },
  {
    "objectID": "edu/2021-11-03-GWAS-on-UKB-RAP/index.html",
    "href": "edu/2021-11-03-GWAS-on-UKB-RAP/index.html",
    "title": "GWAS on UKB RAP",
    "section": "",
    "text": "Webinar about how to run Regenie (GWAS executable) on the Research Analysis Platform."
  },
  {
    "objectID": "edu/2021-11-03-GWAS-on-UKB-RAP/index.html#learning-objectives",
    "href": "edu/2021-11-03-GWAS-on-UKB-RAP/index.html#learning-objectives",
    "title": "GWAS on UKB RAP",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDescribe how regenie utilizes genotype and phenotype information for GWAS\nPrepare input files of merged assay genotypes\nPrepare phenotype information for use in regenie\nPerform single variant association tests on human genomic data using regenie\nExplain next steps for GWAS analysis on the RAP platform"
  },
  {
    "objectID": "edu/2021-11-03-GWAS-on-UKB-RAP/index.html#video",
    "href": "edu/2021-11-03-GWAS-on-UKB-RAP/index.html#video",
    "title": "GWAS on UKB RAP",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "edu/2021-03-20-tidyowl/index.html",
    "href": "edu/2021-03-20-tidyowl/index.html",
    "title": "{tidyowl}",
    "section": "",
    "text": "{tidyowl} is my LearnR Package for going further in the tidyverse.\nSo far, it covers the following:\n\nLearning tidyselect (verbs and methods for selecting and processing columns of data)\nLearning rowwise (verbs and methods for processing data row-by-row)\n\n\n\nLearnR Repository\nInstall on your own machine with:\ninstall.packages(\"remotes\")\nremotes::install_github(\"laderast/tidyowl\")"
  },
  {
    "objectID": "edu/2020-04-01-neus-643-stats-for-neuroscientists/index.html",
    "href": "edu/2020-04-01-neus-643-stats-for-neuroscientists/index.html",
    "title": "NEUS 643: Stats for Neuroscientists",
    "section": "",
    "text": "Description\nThis is a full quarter course that teaches the basics of image processing, statistics, and machine learning using Bioconductor. It consists of an RStudio.cloud workspace and online lectures.\n\n\nLearning Objectives\n\nLearn and execute the basic steps of image analysis in R/Bioconductor.\nAssess whether image processing algorithms are working correctly.\nUnderstand basic spatial statistics and how to assess null models of spatial data.\nExecute and Interpret basic spatial statistics on data that are involved with fluoresence microscopy analysis.\n\n\n\nLinks\nCourse Webiste Syllabus\n\n\n\n\nCitationBibTeX citation:@online{laderas2020,\n  author = {Ted Laderas and Ted Laderas},\n  title = {NEUS 643: {Stats} for {Neuroscientists}},\n  date = {2020-04-01},\n  url = {https://laderast.github.io//edu/2020-04-01-neus-643-stats-for-neuroscientists},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, and Ted Laderas. 2020. “NEUS 643: Stats for\nNeuroscientists.” April 1, 2020. https://laderast.github.io//edu/2020-04-01-neus-643-stats-for-neuroscientists."
  },
  {
    "objectID": "edu/2021-03-20-bsta-504-r-programming/index.html",
    "href": "edu/2021-03-20-bsta-504-r-programming/index.html",
    "title": "BSTA 504: R Programming",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand and utilize R/RStudio.\nUnderstand basic data types and data structures in R.\nFamiliarize and load data files (Excel, Comma Separated Value files) into R/Rstudio, with tips on formatting.\nVisualize datasets using ggplot2 and understand how to build basic plots using {ggplot2} grammar.\nFilter and format data in R for use with various routines using {dplyr}.\nExecute and Interpret some basic statistics in R using {broom}.\nAutomate repetitive tasks in R, such as loading a folder of files using {purrr}.\nExecute basic machine learning workflows using {tidymodels}.\n\n\n\nCourse Links\nCourse Website Course Syllabus\n\n\n\n\nCitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {BSTA 504: {R} {Programming}},\n  date = {2021-03-20},\n  url = {https://laderast.github.io//edu/2021-03-20-bsta-504-r-programming},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “BSTA 504: R Programming.” March 20,\n2021. https://laderast.github.io//edu/2021-03-20-bsta-504-r-programming."
  },
  {
    "objectID": "edu/2021-03-20-a-practical-guide-to-reproducible-papers/index.html",
    "href": "edu/2021-03-20-a-practical-guide-to-reproducible-papers/index.html",
    "title": "A Practical Guide to Reproducible Papers",
    "section": "",
    "text": "Links\nPresentation Code\n\n\n\n\nCitationBibTeX citation:@online{laderas2020,\n  author = {Ted Laderas and Aurora Blucher and Ted Laderas},\n  title = {A {Practical} {Guide} to {Reproducible} {Papers}},\n  date = {2020-02-20},\n  url = {https://laderast.github.io//edu/2021-03-20-a-practical-guide-to-reproducible-papers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, Aurora Blucher, and Ted Laderas. 2020. “A Practical\nGuide to Reproducible Papers.” February 20, 2020. https://laderast.github.io//edu/2021-03-20-a-practical-guide-to-reproducible-papers."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "This is a continually updated list of my teaching materials. Unless indicated, all material is released under CC-NonCommercial license. For other uses, please contact me.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBash for Bioinformatics\n\n\n\n\n\n\n\nukbiobank\n\n\ndnanexus\n\n\nbash\n\n\nwebinar\n\n\nfree\n\n\n\n\n\n\n\n\n\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCloud Computing for HPC Users\n\n\n\n\n\n\n\nukbiobank\n\n\nwebinar\n\n\nfree\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\nTed Laderas, Anastazie Sedlakova\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow Description Language (WDL) on RAP\n\n\n\n\n\n\n\nukbiobank\n\n\nwebinar\n\n\nfree\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2022\n\n\nTed Laderas, Anastazie Sedlakova\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGWAS on UKB RAP\n\n\n\n\n\n\n\nukbiobank\n\n\nwebinar\n\n\nfree\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2021\n\n\nTed Laderas, Anastazie Sedlakova\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Biobank Webinar: Using JupyterLab\n\n\n\n\n\n\n\nukbiobank\n\n\nwebinar\n\n\nfree\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2021\n\n\nTed Laderas, Anastazie Sedlakova\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Biobank Overview Webinar\n\n\n\n\n\n\n\nukbiobank\n\n\nwebinar\n\n\nfree\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2021\n\n\nTed Laderas, Anastazie Sedlakova\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{tidyowl}\n\n\n\n\n\n\n\nself-learning\n\n\nfree\n\n\ntidyverse\n\n\n\n\nIntermediate lessons in R\n\n\n\n\n\n\nMar 20, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBSTA 504: R Programming\n\n\n\n\n\n\n\ncourse\n\n\ntidyverse\n\n\nR\n\n\n\n\nOur course for OHSU/PSU School of Public Health teaching the basics of R-Programming\n\n\n\n\n\n\nMar 20, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to {distill} for websites\n\n\n\n\n\n\n\nworkshop\n\n\nfree\n\n\n\n\nHow you too can learn to {distill}.\n\n\n\n\n\n\nMar 19, 2021\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA gRadual intRoduction to Shiny\n\n\n\n\n\n\n\nworkshop\n\n\nfree\n\n\nR\n\n\nshiny\n\n\n\n\nA Workshop that covers the basics behind learning Shiny.\n\n\n\n\n\n\nJan 28, 2021\n\n\nTed Laderas, Ted Laderas, Jessica Minnier, Dar’ya Pozhidayeva, Pierrette Lo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI 569: Data Analytics\n\n\n\n\n\n\n\ncourse\n\n\ndatascience\n\n\nR\n\n\n\n\nPractical coursework in R/SQL for our hybrid Data Analytics Course taught with Kaiser Permanente.\n\n\n\n\n\n\nApr 20, 2020\n\n\nTed Laderas, Ted Laderas, Mark Klick, Aaron Coyner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Storytelling\n\n\n\n\n\n\n\nfree\n\n\nworkshop\n\n\nbiodata-club\n\n\ntidyverse\n\n\nR\n\n\n\n\nLearn how to make effective visualizations using insights from cognitive psychology.\n\n\n\n\n\n\nApr 1, 2020\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n  \n\n\n\n\nNEUS 643: Stats for Neuroscientists\n\n\n\n\n\n\n\ncourse\n\n\npython\n\n\n\n\nCourse that applies statistics and machine learning using image processing.\n\n\n\n\n\n\nApr 1, 2020\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReady for R\n\n\n\n\n\n\n\ncourse\n\n\nfree\n\n\nR\n\n\n\n\nGet started with R/RStudio and the tidyverse\n\n\n\n\n\n\nMar 20, 2020\n\n\nTed Laderas, Ted Laderas, Aaron Coyner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical Guide to Reproducible Papers\n\n\n\n\n\n\n\nworkshop\n\n\nbiodata-club\n\n\nfree\n\n\nreproducibility\n\n\n\n\nGuidelines on making your project reproducible by others. Includes a short tour of mybinder.org.\n\n\n\n\n\n\nFeb 20, 2020\n\n\nTed Laderas, Aurora Blucher, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Scavenger Hunt: Exploring NHANES\n\n\n\n\n\n\n\nworkshop\n\n\nfree\n\n\nR\n\n\nshiny\n\n\n\n\nA short workshop teaching exploratory data analysis using the NHANES dataset.\n\n\n\n\n\n\nJan 25, 2019\n\n\nTed Laderas, Ted Laderas, Jessica Minnier, Thomas Frohwein\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Bootcamp\n\n\n\n\n\n\n\nself-learning\n\n\nfree\n\n\ntidyverse\n\n\n\n\nOur free online interactive course teaching the tidyverse.\n\n\n\n\n\n\nJan 20, 2019\n\n\nTed Laderas, Ted Laderas, Jessica Minnier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHPC Tutorial\n\n\n\n\n\n\n\nfree\n\n\nworkshop\n\n\nhpc\n\n\n\n\nSelf paced tutorial for learning about cluster computing and SLURM.\n\n\n\n\n\n\nJun 22, 2017\n\n\nTed Laderas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-01-15-if-you-want-to-talk-with-me-for-an-informational-interview/index.html",
    "href": "posts/2018-01-15-if-you-want-to-talk-with-me-for-an-informational-interview/index.html",
    "title": "If you want to talk with me for an informational interview",
    "section": "",
    "text": "Note: I have come to believe that this post is way too harsh and unwelcoming to people interested in data science. I will post an update to this, and a FAQ for those who want informational interviews. But in the spirit of showing my mistakes, I’ll leave this up here.\n\n\nI have had many people who have asked me for informational interviews. They tell me that they are interested in Data Science and want to hear about what I do on a day to day basis. To be honest, I’ve begun to dread these kinds of interviews.\nInevitably, I spend a lot of energy explaining what I do to someone who rarely follows up. Consequently, I don’t find these interviews rewarding at all. So I’ve written this post so that I have a better time doing these kinds of interviews.\nI reserve the right to refuse interviews from people who do not read this.\n\nDo your homework. Please don’t expect me to give the five minute spiel about my research, hoping to look for an “in”. Please do some research and try to ask interesting questions about my work. I’ve given you plenty of resources on this website to know more about me. Ask me about my software, ask me about teaching, ask me carefully thought questions about my research.\nDon’t offer to buy me coffee. If I talk with you for 30 minutes, know that my time is worth far more than a cup of coffee. Instead, offer to pay it forwards. Volunteer with a group that does scientific communication or education; I don’t work with people who aren’t willing to teach others. I don’t work or talk with selfish people, having been burned many times by such people.\nBe specific in your ask. Asking about what next steps to take in learning data science and where to get a job is not specific enough. Again, do your research. What kinds of Data Science are you interested in? Are you interested in predictive analytics in healthcare? Or are you interested in systems modeling of disease? Be specific, and if you ask for something, make sure I can achieve it in five minutes or less.\nDon’t ask for a job, ask for connections. I know a decent number of people around Oregon and OHSU, so if you want me to introduce you to one of my connections, I’m happy to. If I know someone, I’m happy to do this, since it’s usually a five-minute ask.\nFollow up, and be willing to return the favor. Even a nice thank-you email is good. I’m happy to make you a Linkedin connection, if it means I can help someone else further down the line. If I expend energy on you, I’d like to see my impact. If my efforts meant that you managed to find a job, please let me know!\nRead Give and Take. This book by Adam Grant really struck me as the way to actually network. In short, I learned that if I want a return on my energy expenditure, I have to spend my energy on Givers, or people who help others. I am a Giver, but I now tend only to give informational interviewers to other Givers. The rest are too draining for me.\n\nI have given so many of these interviews and have gotten nothing from them. Not even a follow-up, which really is bad practice. So, if I’m considered difficult to approach about these, know it is because your previous askers have really been an energy drain and have been inconsiderate Takers. Make it interesting for me and be considerate. I’m much more likely to help you.\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {If You Want to Talk with Me for an Informational Interview},\n  date = {2018-01-15},\n  url = {https://laderast.github.io//posts/2018-01-15-if-you-want-to-talk-with-me-for-an-informational-interview},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “If You Want to Talk with Me for an\nInformational Interview.” January 15, 2018. https://laderast.github.io//posts/2018-01-15-if-you-want-to-talk-with-me-for-an-informational-interview."
  },
  {
    "objectID": "posts/2017-06-28-HowToNotBeAfraid/index.html",
    "href": "posts/2017-06-28-HowToNotBeAfraid/index.html",
    "title": "How to Not Be Afraid of Your Data",
    "section": "",
    "text": "I’m going to be giving a talk for the PDX RLang Meetup on July 11 called “How to Not Be Afraid of Your Data: Teaching EDA using Shiny”. Abstract below.\nMany graduate students in the basic sciences are afraid of data exploration and cleaning, which can greatly impact their downstream analysis results. By using a synthetic dataset, some simple dplyr commands, and a shiny dashboard, we teach graduate students how to explore their data and how to handle issues that can arise (missing values, differences in units). For this talk, we’ll run through a simple EDA example (combining two weight loss datasets) with a general data explorer in shiny that can be easily customized to teach specific EDA concepts.\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {How to {Not} {Be} {Afraid} of {Your} {Data}},\n  date = {2017-06-28},\n  url = {https://laderast.github.io//posts/2017-06-28-HowToNotBeAfraid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “How to Not Be Afraid of Your Data.”\nJune 28, 2017. https://laderast.github.io//posts/2017-06-28-HowToNotBeAfraid."
  },
  {
    "objectID": "posts/2017-04-19-Breadth-and-depth/index.html",
    "href": "posts/2017-04-19-Breadth-and-depth/index.html",
    "title": "On Breadth and Depth in Your Academic Career",
    "section": "",
    "text": "I was talking with a student and they were complaining that when at conferences, they would try to inject other topics of interest (such as cooking) into discussions with colleagues. Unfortunately, one of the after effects of this was that they were looked at as “not a serious scientist”. There’s an expectation that a scientist must be all depth, only talking and thinking about their sub-field.\nAs a cross disciplinarian, I have to say that is hogwash. The genesis of so many creative ideas in science has happened because of cross-pollination across disciplines. For example, microwave technology might never have been invented without the intersection of disciplines. We know that the Arts Foster Scientific Success - a large number of Nobel and National Academy members do art in some form or other. Bernstein et al theorize that\n\n“there exist functional connections between scientific talent and arts, crafts, and communications talents so that inheriting or developing one fosters the other.”\n\nHaving breadth and depth enables you to make connections that no one else has. It is the hallmark of a curious and creative person. These kinds of people are desparately needed to push science in new directions.\nI have a parallel career in performance and improvisational music. Music, for me, is endlessly inspiring and has forced me out of my introverted shell. One of the reasons I took up cello is that I can play many roles; accompanist, rhythm, solo. This flexibility in playing music has translated to my flexibility in collaboration. Being able to adjust to new circumstances and improvise new ideas to explore is a critical component of being a responsible scientist. My background improvisation has helped me pivot ideas. I have become less attached to dogmatic ideas. Many of my good ideas come from idle wondering about data that has captured my imagination. This is part of the reason why I teach students how to explore their data.\nSo, the next time another scientist looks down at you for being a polymath, pity them. Their world and their ideas are not as rich as yours.\n\nFurther Reading\n\nThe Correlation Between Arts and Crafts and a Nobel Prize\nArts Foster Scientific Success\nDual Thinking for Scientists\n\n\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {On {Breadth} and {Depth} in {Your} {Academic} {Career}},\n  date = {2017-04-19},\n  url = {https://laderast.github.io//posts/2017-04-19-Breadth-and-depth},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “On Breadth and Depth in Your Academic\nCareer.” April 19, 2017. https://laderast.github.io//posts/2017-04-19-Breadth-and-depth."
  },
  {
    "objectID": "posts/2017-06-07-CascadiaRNotes/index.html",
    "href": "posts/2017-06-07-CascadiaRNotes/index.html",
    "title": "Some Lessons We Learned Running Cascadia-R 2017",
    "section": "",
    "text": "Well, the first Cascadia R Conference has come and gone. I have to say that it was super fun, and well attended (over 190 people!). I had a blast meeting and chatting with everyone. Hopefully, we showed newbies that R is learnable and others that there are lots more things to learn about R.\nThe following is my attempt to document what we learned from organizing Cascadia-R. It’s not complete; I may add and subtract from it as I think of more things to say about the planning process.\nDecide the tone. Our goals with Cascadia-R were modest. We wanted to get a diverse group of R users together in a safe and encouraging environment. We wanted our workshops to be accessible to even beginners, and encourage them in the use of R.\nPart of meeting these goals of this is setting the tone. We really wanted to encourage all levels of R users to attend. All of our flyers, emails and promotional tweets encouraged beginners to come. We got help with making a Code of Conduct for the conference. Part of creating a supportive environment is encouraging diversity in both speakers and attendees. We did our best to reach out to current groups that encourage diversity, such as Women in Science Portland, and R-Ladies Global.\nWe also offered diversity scholarships to encourage people from diverse backgrounds to attend, and made diversity part of our criteria for selecting talks.\nStart planning early. As junior faculty at OHSU, I’m lucky enough to be able to book facilities here, including the large learning studios where we held the conference. Having the venue secured early on made the remaining logistics of the conference much easier.\nMuch like wedding planning, there are plenty of conference planning services out there who would be happy to take over aspects of your conference, for a fee. You can spend however much you want to on these things. However, I believe that such a approach is not financially responsible. I also feel that taking a more DIY/bespoke approach can make a conference most engaging (see csvconf). We tried to do most things ourselves (including design, promotion, talk submission, workshops, and registration/logistics).\nIterate your budget. Think of a conference as a project with lots of linked dependencies. Your first plan is probably not going to be your final plan. Start a plan, iterate, realize that things are going to shift, have a backup plan. What if registration is not going to pay for the venue rental fee? Talking to simpatico sponsors can take much of the financial stress. In our case, the Rstudio foundation and ROpenSci stepped up to contribute some money as a cushion.\nRemember, there are fixed costs (such as venue rental, and recording/streaming costs) and variable costs that scale with the number of attendees (food, badges, alcohol). Separate these out. When possible, pay off the fixed costs first, so that it’s easier to manage the variable costs.\nAgain, who is your desired audience and can they afford your conference? We decided to make our conference as affordable as possible to encourage as many different kinds of people to attend. We initially wanted to make attendance free for students. The problem with free is that literally it’s free. It has no value in the mind of a person who accepts free admission. So we decided to charge students a small fee just to emphasize that the conference has value.\nTalk with others who have done it. We were very clueless about much of the logistics side at OHSU. I managed to get through by talking with a number of people here (including Robin Champieux and Shannon McWeeney) who have done conferences here at OHSU. Thank you so much for your invaluable advice.\nEncourage each other and delegate. No one of us could have done all of the conference planning alone. Each of us took on various aspects of conference organization and brought in the others as support as needed. Some of us selected talks, some of us did design, and we all pitched in to get registration working as efficiently and quickly as possible.\nOur slack channel on pdxdata.slack.com is full of our decisions. Slack was so useful as a planning mechanism that we only met online via Google Hangouts a few times, and only had two in-person planning sessions.\nBe Willing to Make Mistakes. Lord knows I made a bunch of mistakes when I made announcements and hosted the lightning sessions. However, I owned up to these mistakes, shrugged, and moved on. Improvising in the moment can be just as important as planning.\nThink about the future. What should the next Cascadia-R look like? I know it just happened, but we’re trying to envision what it would look like. Based on the feedback we’ve gotten so far, people really want more workshops!\nIn a following post, I’m also going to talk about lessons I learned when Chester and I put on our tidyverse workshop.\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {Some {Lessons} {We} {Learned} {Running} {Cascadia-R} 2017},\n  date = {2017-06-07},\n  url = {https://laderast.github.io//posts/2017-06-07-CascadiaRNotes},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “Some Lessons We Learned Running Cascadia-R\n2017.” June 7, 2017. https://laderast.github.io//posts/2017-06-07-CascadiaRNotes."
  },
  {
    "objectID": "posts/2015-12-23-Surrogate-Oncogene-Paper/index.html",
    "href": "posts/2015-12-23-Surrogate-Oncogene-Paper/index.html",
    "title": "Surrogate Oncogene Paper is Published",
    "section": "",
    "text": "My dissertation paper, A Network-Based Model of Oncogenic Collaboration for Prediction of Drug Sensitivity is now published! Here’s a lay summary:\n\nOne outstanding issue in analyzing genomics in the context of personalized medicine is the incorporation of rare or infrequent genetic alterations (copy number alterations and somatic mutations) that are observed in individual patients. We hypothesize that these mutations may actually ‘collaborate’ with known oncogenes in the genesis of tumors through their interactions. In order to show this effect, we assess whether these interacting rare mutations cluster around known oncogenes and assess these mutational clusters, which we term surrogate oncogenes. We assess their statistical significance using a simple model of mutation. We show that surrogate oncogenes are predictive of drug sensitivity in breast cancer cell lines. Additionally, they are prevalent in three different cancer cohorts (Breast, Glioblastoma, and Bladder Cancer) from The Cancer Genome Atlas. Within the Breast Cancer and Bladder Cancer populations, surrogate oncogenes are predictive of overall patient survival. The chief strength of the surrogate oncogene approach is that it can be run at a single-patient level in comparison to other methods of assessing mutational significance.\n\nIf you’re interested in learning more, you can check out the Surrogate Oncogene Explorer in order to understand the nature of surrogate oncogenes, and my R/Bioconductor Package on GitHub if you’d like to try out the analysis.\nThere’s a follow-up paper that I’m working on that I’m very excited about. More news soon.\n\n\n\nCitationBibTeX citation:@online{laderas2015,\n  author = {Ted Laderas},\n  title = {Surrogate {Oncogene} {Paper} Is {Published}},\n  date = {2015-12-23},\n  url = {https://laderast.github.io//posts/2015-12-23-Surrogate-Oncogene-Paper},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2015. “Surrogate Oncogene Paper Is Published.”\nDecember 23, 2015. https://laderast.github.io//posts/2015-12-23-Surrogate-Oncogene-Paper."
  },
  {
    "objectID": "posts/2021-03-18-updated-blog/index.html",
    "href": "posts/2021-03-18-updated-blog/index.html",
    "title": "Updated Blog",
    "section": "",
    "text": "I’ve imported most of my blog posts over from my Hugo/Blogdown blog over to this new {distill} site.\nWhy? {distill} is much more lightweight and easier to maintain. If I want, I can drag over articles from tidytuesday explorations or such and easily link to them. The {blogdown} workflow was really becoming a pain, especially in maintaining different Hugo versions, and honestly, the page was getting much too busy.\nSo, really, theoretically, you should see more content from me soon.\n\n\n\nCitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Updated {Blog}},\n  date = {2021-03-18},\n  url = {https://laderast.github.io//posts/2021-03-18-updated-blog},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Updated Blog.” March 18, 2021. https://laderast.github.io//posts/2021-03-18-updated-blog."
  },
  {
    "objectID": "posts/2017-04-17-Building-A-Peer-Mentoring/index.html",
    "href": "posts/2017-04-17-Building-A-Peer-Mentoring/index.html",
    "title": "Fostering a Peer Mentoring Culture",
    "section": "",
    "text": "I realize that it has been an embarrasingly long time since I updated this blog. I had all sorts of grandiose plans for it, and I think my problem was that I was thinking too broad, too pie-in-the-sky. I’m going to try to focus on short and informative blog posts.\nOne of the things that I have been thinking about graduate school is the idea of building a Peer Mentoring culture in our department. I believe that students should help and support each other, and we need to provide a forum to do that. Not just assign mentors, but provide a time and a place to do that.\nWe try to foster a mentoring culture within our student group, BioData-Club. Students are free to talk about issues that concern them, especially about datasets, and are encouraged to share their experiences of software that they’ve used. I believe that we try to give students a psychologically safe place to talk about their issues with data. We try to make people feel like they’re not alone, and coach beginners so they can get over the hump.\nWe’re now embarking on an experiment to reach even more people at OHSU, because we know there are lots of students who struggle with practical skills in data analysis. Our group is growing, and that’s exciting.\nI’m going to try and get everyone in our group to write a paper about Peer Mentoring Culture and how to encourage it in other schools.\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {Fostering a {Peer} {Mentoring} {Culture}},\n  date = {2017-04-17},\n  url = {https://laderast.github.io//posts/2017-04-17-Building-A-Peer-Mentoring},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “Fostering a Peer Mentoring Culture.”\nApril 17, 2017. https://laderast.github.io//posts/2017-04-17-Building-A-Peer-Mentoring."
  },
  {
    "objectID": "posts/2019-09-23-kindness-at-the-cost-of-yourself/index.html",
    "href": "posts/2019-09-23-kindness-at-the-cost-of-yourself/index.html",
    "title": "Kindness at the Cost of Yourself",
    "section": "",
    "text": "In healthy cultures, people rise by elevating others and fall by undermining others. In toxic cultures, people are forced to choose between helping others and achieving success. Choose the workplace where success comes from making others successful. - Adam Grant\nWhen I read Adam Grant’s Give and Take, it was like a light went off. In this book, Grant gives profiles of highly successful people who are givers - people who have changed their field by making things better for others. I immediately connected to this idea of giving. Here was a way to network with people that really worked for me. Giving and connecting people are what I do best. I like to help people achieve their goals if I know someone else who can help them.\nAnd no doubt, embracing my identity as a giver has helped change my career trajectory - in the last few years, I co-founded Cascadia R, produced the R-Bootcamp with Jessica Minnier, and have met so many people in the field of education and open science that I want to work with and who have inspired me.\nHowever, as I’ve noted before, I tend to be giving to the point of being self sacrificing, which is not good. I’m reminded of the end of Kafka’s Metamorphosis when (spoilers) as Gregor Samsa dies in his bug form. As he dies, he worries about the agency of his family. Will they be able to support themselves? In the end, the family that he was worried about finds jobs, support themselves and thrive in the end. Without him. One interpretation of this story: no one is so irreplaceable in this world.\nThis Spring, I probably gave a little too much to my Health Analytics class, to the point of being self-sacrificing. This shouldn’t be a reflection of the class - they were great and really whip smart - I just tend to give a little too much. As a result, I was pretty drained by the time I got to Summer quarter. I’d definitely overdrawn on giving - I was pretty wiped out through the summer. Being overdrawn actually meant I didn’t have enough energy to teach as well as I would have liked - and I felt worse for this.\nThe lesson I’ve learned this summer: If you find yourself giving to the point of self-sacrifice, you need to ask yourself why you are sacrificing so much of yourself. Excessive self-sacrifice seems to occur when your self-esteem is tied to the perception of yourself by other people. Meaning, you should question why you are self-sacrificing. Is it because you feel like you need the approval of others? No one’s approval is worth overworking yourself over.\nSo, self-sacrifice is ultimately bullshit. Ironically, being self-sacrificing can lead to feelings of helplessness and resentment from the people you are sacrificing yourself for. Additionally, being highly proactive and championing change can actually have negative effects if your actions are perceived as a threat to your organizational culture. There’s a reason why change management is important. No one person can be a revolution - it takes others to do so. It shouldn’t fall to just being on you.\nYou need to take care of yourself and protect yourself from the takers. As Faith Harper writes in Unf#ck Your Adulting, “taking care of yourself does the world a favor”. Taking care of yourself means that you can do good in the world sustainably, and continue to do it, rather than burning out.\nUnderstand that the notion of true giving is not rooted in self-sacrifice. Giving is in the spirit of generosity; in contrast, self-sacrifice is more akin depleting one’s own resources. As a giver, your generosity should be recognized and not exploited.\nOne thing to be aware of as a giver is passion exploitation, where people and organizations will take advantage of you because of your passion for a subject as an opportunity to pay you much less than you’re worth. Such an attitude is prevalent in creative industries, where one is encouraged to work for free or “exposure” (I don’t really do soundtrack work any more because of this). As if being a poor artist or creative was a privilege. Such situations will drain your passion and your energy and burn you out with nothing to show for it. Again, you have to keep something for yourself.\nIf you want to continue to do good in the world, you must take care of yourself. This is especially the case in a toxic work culture. You have to build reserves - never give more of yourself than you can. Especially do not do this in a work setting. It is amazing how self-sacrifice can become the status-quo and what is expected of workers. No one is well off because of that. Toxic work cultures encourage this: crunch time as the norm at game studios, overwork at Japanese Black Companies, even in the non-profit sector.\nIn short, I believe in kindness and giving. But the kindness and giving must be sustainable. You must defend yourself from those who would take more than they give, and avoid being exploited by those who see your passion as something exploitable. Identifying these people and organizations and saying no is key to taking care of yourself while making things better for others. Don’t feel guilty about this. Don’t make self-sacrifice part of your identity or job description."
  },
  {
    "objectID": "posts/2019-09-23-kindness-at-the-cost-of-yourself/index.html#resources",
    "href": "posts/2019-09-23-kindness-at-the-cost-of-yourself/index.html#resources",
    "title": "Kindness at the Cost of Yourself",
    "section": "Resources",
    "text": "Resources\n\nGive and Take - this is the book that opened my eyes to the power of being a social giver.\nUnfuck Your Adulting - this book by Faith Harper is full of good advice about navigating situations, emotions, and taking responsibility for what you can take responsibility for.\nHow to be a giver, not a self-sacrificer - talks about giving in the spirit of generosity, and not self-sacrifice. Again, keep something for yourself.\nWhen to take initiative at work, and when not to\nAvoiding Burnout: Tips for self-care - again, very good tips on taking care of yourself, because no one else will."
  },
  {
    "objectID": "posts/2018-04-04-a-simple-intro-to-genome-wide-association/2018-04-04-a-simple-intro-to-genome-wide-association.html",
    "href": "posts/2018-04-04-a-simple-intro-to-genome-wide-association/2018-04-04-a-simple-intro-to-genome-wide-association.html",
    "title": "A Simple Intro to Genome Wide Association",
    "section": "",
    "text": "This term, I’m co-teaching an undergraduate course for the PSU/OHSU School of Public Health called Health Informatics with a number of my collegues in my department, including Bill Hersh, Eilis Boudreau, Karen Eden, and Virginia Lankes. We’re trying to give students a feel for what informatics is about in an accessible way.\nI’m trying to make the lectures as understandable as I can. This week we tackled Genome Wide Association Studies and discussed the strength of evidence behind SNP variants associated with phenotypes. We ended with an activity where they queried the GWAS catalog for information on particular SNP variants.\nThe repo for my lectures is here and will be updated as the course goes along: https://github.com/laderast/HSMP410 The lecture is here: An Introduction to Bioinformatics: Genome Wide Association and the activity is here: SNP In class assignment. All my material is free to use and released under Apache 2.0, so if this is interesting to you, please feel free to fork it and remix it!\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {A {Simple} {Intro} to {Genome} {Wide} {Association}},\n  date = {2018-04-04},\n  url = {https://laderast.github.io//2018-04-04-a-simple-intro-to-genome-wide-association.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “A Simple Intro to Genome Wide\nAssociation.” April 4, 2018. https://laderast.github.io//2018-04-04-a-simple-intro-to-genome-wide-association.html."
  },
  {
    "objectID": "posts/2022-08-04-RStudio-Conf/index.html",
    "href": "posts/2022-08-04-RStudio-Conf/index.html",
    "title": "RStudio Conf 2022 Highlights: Days 1 and 2",
    "section": "",
    "text": "Well, RStudio Conference 2022 has come and gone, and I thought I’d just write a couple blog posts of my experience attending it.\nBefore I start, I just want to mention the big news: RStudio (the company) is becoming Posit. This new name reflects the fact that a lot of their new products, including Quarto, Shiny for Python, and Tidymodels/Vetiver are cross-language.\nJust to be 100% crystal clear: the products, such as RStudio, are not undergoing a name change. Posit is the new name of the company, not necessarily its products."
  },
  {
    "objectID": "posts/2022-08-04-RStudio-Conf/index.html#day-2",
    "href": "posts/2022-08-04-RStudio-Conf/index.html#day-2",
    "title": "RStudio Conf 2022 Highlights: Days 1 and 2",
    "section": "Day 2",
    "text": "Day 2\nDay 2 started with a more philosophical bent; that of function design. Specifically, the difference between pure functions (that only process what you give them), and functions with side effects (which alter the outside universe3). One of the key insights is that it is way easier to write unit tests for pure functions, and it is harder to write tests for functions with side effects.\nWe talked more about testing functions with side effects using snapshot testing.\nIan is way better at grokking the tidyeval documentation than me, so it was a pleasure watching him teach the two main uses of tidyeval: 1) data-masking and 2) tidyselect. Specifically, he talked about different ways to implement\nI think there is a special bond that’s formed when you teach together, and it was great to work with the entire team."
  },
  {
    "objectID": "posts/2018-03-19-some-notes-from-the-evidence-to-scholarship-conference/index.html",
    "href": "posts/2018-03-19-some-notes-from-the-evidence-to-scholarship-conference/index.html",
    "title": "Some Notes from the Evidence to Scholarship Conference",
    "section": "",
    "text": "Last week I had the good fortune to attend the From Evidence to Scholarship Conference at my alma mater, Reed College. The focus of the conference was improving the research process for undergraduates using digital scholarship. I came away from it excited about the work other people are doing in this realm and thinking about ways we could adapt these approaches.\nNicole Vasilevsky and I (both former Reedies) each gave talks, about our experience developing materials for Data Science and giving data science workshops to undergraduates.\nI found the conference to be very exciting and friendly overall. Laurie Allen kicked off the conference with a plenary session talking about the role digital scholarship librarians can have in the community surrounding them, talking about her work with Monument Lab and Data Refuge. Key takeaway point: it’s ok to do something badly if it gets the conversation started with the community.\nOne of the key takeaway points from the conference is the role that digital scholarship librarians have in revamping current curricula at the liberal arts colleges. They often do short-form training (in digital scholarship tools, such as webpage building and network analysis), which when integrated into a current course, can revitalize the course. Laura O’Brien and Hélène Bilis talked about integrating digital scholarship tools into a french literature course, talking about modeling the social networks of characters and how they evolved over time.\nAn exciting talk was about the ILiADS program, which is a multi-disciplinary digital scholarship program. It’s run as a summer institute which starts as teams of students, faculty and staff. The projects are student driven and they try to incorporate both students with the technical skills and those who are non-technical. I really liked this idea of teaming up students from different fields and having faculty to advise them as needed. I’d like to try something similar at OHSU.\nCollaboration with allied departments led to enhanced interdisciplinary courses. Jon Caris’ talk about his Spatial Analysis Lab at Smith College and the unexpected collaborations they had was a really great one, talking about using spatial analysis to analyze painting styles in the Painting III course, and using GIS to map out invasive species at a pond at Smith College for an intro Biology class. One of the best quotes from his talk was talking about emerging technology: “Emerging technology means that you’re in a perpetual state of incompetence. And that’s a fun place to be.”\nI’d be remiss if I didn’t mention Parveneh Abbaspour and Jeremy McWilliams’ talk about LCPhysX, which is an online video archive showcasing student projects in the physics department. The videos are really great, especially the one about the Golf Robot. The engagement of the students actually seems to have had an effect on admissions; a few of the new physics students actually noted that the LCPhysX site made them decide to go to Lewis and Clark.\nFinally, I really enjoyed Andrew Bray’s talk about his collaborative course, “Case Studies in Data Science: Elections”, where he solicited political scientists about important problems in voting for the students to tackle. By giving them some basic analyses in R, the students managed to complete a number of projects, including a package for visualizing ranked choice voting, which is actually being utilized for visualizing these results.\nI emerged from this conference excited about the passion that undergraduates have and excited about how digital scholarship librarians can change curricula for the better.\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Some {Notes} from the {Evidence} to {Scholarship}\n    {Conference}},\n  date = {2018-03-19},\n  url = {https://laderast.github.io//posts/2018-03-19-some-notes-from-the-evidence-to-scholarship-conference},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Some Notes from the Evidence to Scholarship\nConference.” March 19, 2018. https://laderast.github.io//posts/2018-03-19-some-notes-from-the-evidence-to-scholarship-conference."
  },
  {
    "objectID": "posts/2021-04-29-moving-on-and-upwards/index.html",
    "href": "posts/2021-04-29-moving-on-and-upwards/index.html",
    "title": "Moving On and Upwards",
    "section": "",
    "text": "I’m leaving OHSU. I have accepted a position to become a bioinformatics trainer for DNANexus. I am passionate about making sure that people do good analysis, and I’m convinced that DNANexus is the best place for me to make an impact and move away from academia.\nAs an bioinformaticist/collaborator who is focused on doing reproducible analysis, the level of financial support I received in academia was difficult to sustain. I didn’t have an R01 of my own; I was often divided between multiple projects where I was faced with the decision of having to triage work and do work I was less proud of. I was spending less time on things I’m passionate about, and more on administration, which was much less important to me. I was also split between multiple departments and meeting the expectations of everyone was way too difficult for me.\nHaving multiple bosses, I was also forced to adopt the survival strategy of not disappointing the same person in a row. I never felt good about that, and I’m tired of feeling that way. I could not see my career as being sustainable, nor did I ever consider myself someone who would be promoted in the current environment. The university did not recognize the level of collaboration that I dedicate myself to, nor does it priortize this level in terms of promotion and tenure criteria. So I never felt sure whether I was on the path to promotion.\nA lot of what I’ve tried to accomplish has been fueled by a deep anger by what happened to me during my dissertation process. My dissertation advisor was unsupportive and I had a toxic dissertation committee. But I can’t make these feelings the cornerstone of my future life.\nI’ve tried to make academia less painful through efforts such as Cascadia-R, BioData Club, and my workshops and courses. I still believe that learning should be less lonely and that fighting impostor syndrome is something that should be done in a supportive community. I will continue to be involved with both in a limited capacity.\nI will always be grateful to my students and TAs for their fresh perspectives and energy. I hope you will continue on with that enthusiasm. I hope that I have inspired you, at least in a small way. I am also grateful for all the faculty, staff, postdocs, and everyone in the open science, data science and R communities who have been supportive of me in my journey, and who have helped me arrive at this decision.\nI don’t want my decision to be an example for others. I don’t want to discourage others in their pursuit of an academic career, if that is what their heart is set on. But please be aware of the potential downfalls of such a path. The high rejection rates for funding. The low percentage of us who are destined for stable careers in academia. You need to find your own way, and you will probably do a better job of finding funding than I did. Get to independence through securing your own funding as fast as possible, or start building those marketable skills.\nIn the coming months, I will probably release less R material, due to my focus on bioinformatics training. I love R, but I have given more than I have recieved back and I need to focus on myself for the time being. I’ve depleted myself over the last year and I need to recharge. Hopefully you will respect that.\n\n\n\nCitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Moving {On} and {Upwards}},\n  date = {2021-04-29},\n  url = {https://laderast.github.io//posts/2021-04-29-moving-on-and-upwards},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Moving On and Upwards.” April 29, 2021.\nhttps://laderast.github.io//posts/2021-04-29-moving-on-and-upwards."
  },
  {
    "objectID": "posts/2015-05-26-mutational-burden-in-skin/index.html",
    "href": "posts/2015-05-26-mutational-burden-in-skin/index.html",
    "title": "Somatic Mutations in Skin Paper",
    "section": "",
    "text": "This paper, High burden and pervasive positive selection of somatic mutations in normal human skin is fascinating. It suggests that the mutational burden is much higher than we expected in skin cells due to UV exposure. In addition, subclones exist in the skin that are positively selected for oncogenes.\nIt also makes me want to stock up on sunscreen.\n\n\n\nCitationBibTeX citation:@online{laderas2015,\n  author = {Ted Laderas},\n  title = {Somatic {Mutations} in {Skin} {Paper}},\n  date = {2015-05-26},\n  url = {https://laderast.github.io//posts/2015-05-26-mutational-burden-in-skin},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2015. “Somatic Mutations in Skin Paper.” May\n26, 2015. https://laderast.github.io//posts/2015-05-26-mutational-burden-in-skin."
  },
  {
    "objectID": "posts/2018-06-09-cascadia-r-2018-how-we-planned-it/index.html",
    "href": "posts/2018-06-09-cascadia-r-2018-how-we-planned-it/index.html",
    "title": "Cascadia-R 2018: How we planned it and the reasons why",
    "section": "",
    "text": "Well, Cascadia-R 2018 has come and gone. This year we tried our best to make it as inclusive, welcoming, and friendly as we could. Considering we had 224 participants this time around, I’d say it was a success.\nI just thought I would do a little write up of some of the things we did and why we did them in our conference. I’m hoping it will be useful for other conference planners to create a welcoming environment.\n\nWe created a pull plan based on what we learned last year. Because last year was pretty chaotic in terms of planning, we decided to try to make a pull plan this year. Pull planning is a project management technique in which you work backwards from absolute deadlines (from the day of the conference). Doing so spaces out a lot of the work and makes things potentially more dividable across a group. I’ve made our pull plan public so other conference planners can see what we did and the timing it took to organize the conference.\nFind sponsorship money. We ran last year on a shoestring, using only about $3500. We realized that to make the conference more inclusive this year, we had to get sponsorship money. Childcare and diversity scholarships cost money. We wanted sponsors who had values that aligned with our inclusion goals. Lilly Winfree was especially great at finding sponsors who were in tune with what we wanted to accomplish with our conference. Sponsorship also allowed us to pay for lunch for everyone, which we couldn’t last year.\nProvide childcare. We wanted parents to be able to attend, so we got DivCare to provide childcare for them in our building. DivCare provided childcare for 8 children, and we had some grateful parents who were happy they could attend.\nDiversity Committee. We did this last year, but we had more money this year for diversity scholarships for those who might not be able to attend. I think next year we might think about reaching out to more STEM groups, but I think the real challenge is finding how to get those diverse students who might not even know about data science in and how to properly host and support them.\nGet rid of longer form talks. Keynotes and Lightning talks only. This is probably the most controversial choice that we made. However, we realized that the 10-15 minute talks we had in 2017 were mostly academic ones. Honestly, there are already too many academic conferences out there. We wanted a format that was more accessible and encouraged discussion afterwards, so we stayed with the lightning talks.\nHave keynote speakers show newcomers how to join the R Community. We chose Kara Woo and Alison Hill as our keynote speakers for a very specific reason: we wanted to encourage people new to R that they could learn things. Alison gave a wonderful talk about ways to approach learning R; Kara gave a great talk talking about how to contribute to various R-projects on GitHub, especially the tidyverse suite of packages. What I really liked about both their talks was that they emphasized learning by doing.\nMoar workshops. We wanted beginners and intermediate people feel like they were learning something, and in a safe learning environment. One piece of feedback from last year was that people wanted more workshops. We created a beginner track and an intermediate track for workshops. I would just like to say that none of the workshop people got paid for their work, and I do wish that some of the attendees realized that before they provided feedback such as “BORING” (seriously? please realize that there are people who develop these, and constructive feedback is always better). All of these workshops were done for free as labors of love, unlike conferences like ODSC.\nOrganize Volunteers. We had lots of volunteers this year and we finally were able to figure out roles for everyone to do (registration, nametags, TAs, etc).\nDo a visualization fest that wasn’t competitive. I know that everyone loves bake-offs like the DREAM Challenges and Kaggle, but we felt that having a competition was in conflict with the community building we wanted to do with the conference.That, and Tidy Tuesday, were the inspirations for the cRaggy. We wanted people to share ideas with each other and to be constructive with each other. We had three great short talks talking about their approach to the visualization and we shared the data with Tidy Tuesday.\nTalk honestly about impostor syndrome. We had a panel with three people (the wonderful Paige Bailey, Kevin Watanabe-Smith and Lilly Winfree) and they all talked about how they deal with impostor syndrome. Some of the lessons are: you feel like less of an impostor the more you do and the more you participate.\nHave more downtime. We had feedback last year that there wasn’t enough social time. We built more breaks into the schedule and had a third room (the hack room) dedicated to the “hallway track”: often the best times we’ve had conferences is when we play hooky from conference activities. Some people suggested this year that there maybe was too much downtime. I would say to that, there’s always more opportunity to meet people.\n\nSo that’s why Cascadia R was the way it was this year. In another blog post, I’ll talk more about the fun things that happened this year and our hopes for the next year.\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Cascadia-R 2018: {How} We Planned It and the Reasons Why},\n  date = {2018-06-09},\n  url = {https://laderast.github.io//posts/2018-06-09-cascadia-r-2018-how-we-planned-it},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Cascadia-R 2018: How We Planned It and the\nReasons Why.” June 9, 2018. https://laderast.github.io//posts/2018-06-09-cascadia-r-2018-how-we-planned-it."
  },
  {
    "objectID": "posts/2022-02-27-yes-and-is-the-foundation-of-collaboration/index.html",
    "href": "posts/2022-02-27-yes-and-is-the-foundation-of-collaboration/index.html",
    "title": "‘Yes, and…’ is the foundation of collaboration",
    "section": "",
    "text": "Collaboration as Improvisation\nCollaboration is a delicate thing. It needs to be nurtured and encouraged, especially in the early stages. It is easy to destroy it if you go in with the wrong expectations and attitude.\nA lot of what I have learned about collaboration can be summed in the “Yes, and…” rule of comic improvisation:\n\nThe basic concept of these two words is that you are up for anything, and will go along with whatever gets thrown your way. Essentially, you don’t use the word “No” in improv very often! The “And” part comes in when you are in a scene and can add to what your partner started rather than detract from it. - Second City\n\nIn collaboration, you need to build on what your partner is doing. Doing so requires openness and lack of ego. You are extending and supporting each other. You are creating something new in your journey together, and it will not be like what you do on your own. That is a wonderful thing.\nThis is obviously the opposite of what we’re taught in academia. In academia, we’re taught that we need to analyze things and take them apart.\n\n\n\n\n\n\n\n\n\nUsing the “Yes, and…” rule, we are synthesizing instead of analyzing. We are putting things together, taking risks, and hopefully having fun and being energized on the way.\n\n\n“Yes, and…” IRL Example\nWhat does the “Yes, and…” rule look like in real life?\nAs an example, I worked with other members of the OHSU’s Educator’s Collaborative. We were having a discussion during our virtual coffee break about how we can best support online students during COVID.\nEventually, we realized that we wanted to work on something together. We wanted to show fellow educators concrete advice on how to support students.\nThrough talking some more, we identified ways of doing this: encouraging psychological safety through a code of conduct, providing a forum and space for student support in courses, and identifying open educational resources to utilize in courses rather than making students purchase expensive textbooks.\nThe result was an hour-long presentation that came together on these themes. It was a really positive experience that leveraged each of our strengths and demonstrated concrete and evidence-based steps to support students.\n\n\nRespect and Sharing Values is Key to Taking Risks Together\nAt the heart of this collaboration was our respect for each other’s knowledge and values. We knew we could share our experiences and our desire to help students.\nImagine if one of us proposed something, and another of us just shot it down. Where can we go from there? Not many places.\nBut if we build and encourage each other, we can find new paths by taking risks together. Tina Fey had a great perspective on this:\n\nAdd to the discussion. In the example above, the second person did not just say, “Yes we’re at the beach,” she said, “I can’t wait to get in the water, I hope it’s not too cold.” This statement adds value to the scene. Now the audience knows it’s cold and that we plan to enjoy the ocean as opposed to looking for gold or taking a yoga class.\n\n\nTo me, this rule challenges you to contribute. Whether you are developing an ad campaign or deciding where to eat dinner, put your neck out there, give your thoughts and have a say. Two minds are always better than one. - Tina Fey, Bossypants\n\nIn collaboration, you need to focus on adding value, not subtracting from other’s contributions. Don’t judge your collaborator’s contributions. Nothing is more demotivating nor more destructive of a fledgling collaboration.\nEncourage each other, make your own psychologically safe space together. This space can be difficult to establish, but if you respect each other, it will be ok to take risks together.\n\n\nTJ and Dave: Tapping Into the Flow\n\n\n\n\n\n\n\n\n\nAs an extreme example of the “Yes, and…” rule, look at TJ and Dave, two masters of long-form improvisation. Their shows are completely improvised from scratch each time. Somehow, for every performance, they manage to fill an hour and make people collapse with laughter. It is creation on the edge of chaos, and they always manage to make something.\nTJ and Dave talk about their improv as tapping into a flow of events that’s external to them, and that the improv is just tapping into that flow. Ego or who authored what doesn’t come into play - they are just vessels for this flow. They seem incredibly humbled and grateful to be able to tap into this, and this keeps their collaboration fresh.\nWhen you find a good collaborative partner, you can channel the flow together. It is a form of play, and it is energizing and fun.\n\n\nAsking Great Questions for the Setup: The 2000 year old man\nThe 2000 Year Old Man was a comedy routine that Carl Reiner and Mel Brooks came up with together while working together in a writer’s room. Carl played an interviewer asking a 2000 year old man (played by Brooks) questions about daily life as 2000 year old man, did he meet certain historical figures, etc. It was completely improvised and Mel’s responses were usually pretty darn funny.\n\n\n\n\n\n\n\n\n\nMel’s responses always got the laughs, but Carl’s role was especially important. He asked the right questions that Mel could build his answers on. His questions set up Mel for success:\n\nCarl Reiner was a master of the underrated art of the setup. Most “straight men” are known for their responses that release the laugh. Carl did that too, but even more brilliantly, he subtly puts all of the pieces into play for Mel Brooks to push off of into the comedy stratosphere. - Anne Librera\n\nWhat lessons can we apply from Carl Reiner? When collaborating, ask yourself: how can I ask these kinds of questions and set up my collaborators to push off into that stratosphere? How can I set ourselves up to capture lightning in a bottle?\n\n\nHow to Find IRL Collaborators: Share Values\nThe best way to find collaborators is to find people who share your values.\nThere are a lot of opportunties for this: help organize a conference, join a community of practice, find an open-source project that encourages first-time contributors. Work on putting together a workshop with someone - this is a great way to make connections quickly. Be curious about other people’s work; that curiosity can be the start of a great relationship. Again, early on, focus on the relationship, not the output product.\nThe majority of my educational collaborations have come about because I share values with my collaborators. Wanting to make education more inclusive and accessible has been a great value to unify on.\nIf you meet someone who shares your values and could be fun, try and find a way to work together.\n\n\nIt’s All About Fun, Energy, and Connection\nCollaboration is an antidote to the loneliness we all feel nowadays. Science can be a very lonely place, and it sometimes seems that when you present ideas to others, there’s always someone who wants to shoot it down. (I’ve often felt like a grant could best be defined as a dream a reviewer stomps to death.) This is very draining.\nIn contrast, a good collaboration can be energizing and fun. We may downplay fun, but it is an important part of any collaboration.\nWe need to know when we need to re-evaluate our collaborations. The canary in the coal mine for any collaboration is the question: am I having fun in this collaboration?\nIf this collaboration is something you volunteered to do (that is, you’re not being paid), and it’s no longer fun, you have my permission to drop the collaboration. Life is too short to spend on voluntary activities that are not fun.\n\n\nTL; DR\nHere’s a quick list of ground rules that synthesizes the above.\n\nWork with those who respect you and share your values.\nValue the collaborative relationship (which is long-term) over any short-term products.\nUse the “Yes, and…” rule. Add to your collaborator’s contributions, don’t detract from them.\nTake risks and make something cool together.\nSet each other up for success.\nHave fun together. Stop if it’s no longer fun.\n\n\n\nThank You, Open Science Communities\nFinally, I want to end on a personal note and express gratitude for everyone who has helped me on my journey.\nAt an especially low time in my life, the communities of Open Science saved me, showing me a positive way to do science. This started with Mozilla Open Leaders, BioData Club and Code for Science and Society. There are countless others, including rOpenSci, csvconf, RStudio Certified Educators, RStudioConf, R/Medicine, PDX-R, Cascadia-R Conf, R-Ladies, SDSS, and The Carpentries. Grateful for everyone I’ve met through participating that showed me a way forward.\nThis post is my love letter to them. Thank you, everyone.\n\n\nFor More Info\n\nBossypants by Tina Fey has a great summary of why “Yes, And…” can change your life.\nRead the oral history of the Upright Citizen’s Brigade to learn more about “Yes, and…” and Del Close, whose philosophy of performance is the foundation of theatre sports.\nMel Brooks’ Autobiography All about me! is full of his kudos for his collaborators, and awe at his education in comedy. Highly recommended.\nThe Carpentries material has much more information about teaching together (search for “Never Teach Alone” on this page).\nGreg Wilson’s book Teaching Tech Together has even more information about this.\n\n\n\n\n\nCitationBibTeX citation:@online{laderas2022,\n  author = {Ted Laderas},\n  title = {“{Yes,} and...” Is the Foundation of Collaboration},\n  date = {2022-02-27},\n  url = {https://laderast.github.io//posts/2022-02-27-yes-and-is-the-foundation-of-collaboration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2022. “‘Yes, and...’ Is the Foundation\nof Collaboration.” February 27, 2022. https://laderast.github.io//posts/2022-02-27-yes-and-is-the-foundation-of-collaboration."
  },
  {
    "objectID": "posts/2015-11-19-statcheck-package/index.html",
    "href": "posts/2015-11-19-statcheck-package/index.html",
    "title": "Statcheck Interview",
    "section": "",
    "text": "Due to the usual postdoc busy-ness, I haven’t had the energy to update this blog as much as I would like, but I thought this interview on Retraction Watch from Michèle B. Nuijten, the developer of the R-package statcheck to be fascinating. Her package essentially automates the checking of p-values given published data in papers, from converting the papers from pdf to text, and sees if the calculated p-values are correct. There was a lot of trial and error in parsing known formats for p-values, but now the package is available.\nI see an potentially really interesting master’s thesis in forensic bioinformatics in using the package to assess reproducibility of results in a field. Note that the student probably wouldn’t make any friends in high places, but it would be a potentially high impact thesis.\n\n\n\nCitationBibTeX citation:@online{laderas2015,\n  author = {Ted Laderas},\n  title = {Statcheck {Interview}},\n  date = {2015-11-19},\n  url = {https://laderast.github.io//posts/2015-11-19-statcheck-package},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2015. “Statcheck Interview.” November 19,\n2015. https://laderast.github.io//posts/2015-11-19-statcheck-package."
  },
  {
    "objectID": "posts/2018-06-25-asking-for-help-to-get-better/index.html",
    "href": "posts/2018-06-25-asking-for-help-to-get-better/index.html",
    "title": "Asking for help to get better",
    "section": "",
    "text": "I want to thank everyone who has reached out to me after I wrote my post on struggling with my depression and self-care. I am incredibly grateful for everyone’s concern about me. I wrote that at a low point in my life because I had to. I was suffering too long in silence, and I needed to do something. Writing that post was incredibly scary. I am still worried that it may be used against me somehow down the line when I am reviewed for tenure. But the truth is that I’m not a productivity machine; I can be extremely dependable in very uncertain situations, but to do that I need support.\nI wanted to start the conversation because mental health is not something we talk about much in academia. There is still a lot of bias and stigma against those with mental illness, that they are “weak”, or that they should “get out of academia”. This stigma prevents a lot of students and postdocs from seeking the treatment they need. I just want to say this to everyone who is struggles with mental illness: you aren’t weak, and you aren’t alone. As far as I am concerned, each of you that manages to get through your day despite of your illness should be considered a hero. Because I know how hard it can be to even get out of bed some days. And I would like for PIs to consider that your overall neglect and/or bad advising can contribute to your student’s decline in mental health.\nThank you to all my students, my colleagues, alumni, and everyone in my department (including my department head and division head) who are supporting me. Your kindness and sharing your stories about your struggles really has meant a lot to me. I’ve come to realize that I have an amazing support network and that I just need to ask for help. I am slowly figuring out what that help is. I’m going to 0.8 time for a while to give me more breathing room. I’m exhausted and I just need some time to replenish my energy and reflect where I want to go next.\nI have come to realize that saying yes when I shouldn’t comes at a cost to myself and my mental health. Everything I do comes at a cost; I need to have a better gauge of when that cost is worth it, and how much mental reserve I have in my tanks. When my collaborators don’t understand the amount of work I do cleaning their data, when they don’t respect the work I have done managing their data and reconciling the issues with data collection, that’s when I feel like working in research is not worth it.\nOne of my industry collaborations was like that, and it pretty much drained nearly all of my energy. I don’t like working with people who expect me to give them an answer and aren’t willing to educate me about what I’m supposed to be looking for. I also don’t like working with people who take advantage of my work in open science to further their own work agenda. I need less of these kind of collaborations, and I need the gumption to walk away from them.\nI just want to say this: I am not a rock. I am not strong. But I am a good advocate for others, especially those I feel who have been dealt an unfair hand. I realize that I need to advocate for myself right now. Thank you to other academics such as Dr. Emily Hencken Ritter, who wrote about her struggles with depression and what help she asked for. I’m going to highlight posts about this using the hashtag #DepressionInAcademia on Twitter.\nIn short: I’m not going to get better overnight, but I’m trying to take steps to get better. I’ll be taking a little time off this summer as well to recharge and figure out what’s next. I’ll try to chronicle what works for me and to highlight other people’s writings about this.\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Asking for Help to Get Better},\n  date = {2018-06-25},\n  url = {https://laderast.github.io//posts/2018-06-25-asking-for-help-to-get-better},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Asking for Help to Get Better.” June\n25, 2018. https://laderast.github.io//posts/2018-06-25-asking-for-help-to-get-better."
  },
  {
    "objectID": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html",
    "href": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html",
    "title": "Burning Out and Recovering (slowly)",
    "section": "",
    "text": "Last year about this time, I had a meltdown, which is a symptom of burnout. I’d like to explain a little bit about my burnout in the hopes that other people can avoid it.\nI want to help people, and I’m a people pleaser. I’m a giver, and sometimes I give too much, to the point I have nothing left. I have to be aware when people exploit this. One collaboration, unfortunately took advantage of my giving nature.\nI was on a contracted project where I used some of the time to build an open source project to enable our research. With our collaborators, I showed them my work and they seemed excited. Unfortunately, this “excitement” turned into unreasonable demands. I was literally trying to do a full time development position while balancing the demands of other projects, including teaching. I started feeling like a machine, where each of my collaborators just wanted me to output figures and analyses.\nThe worst I’ve ever felt is when I have been busting my ass for things I don’t believe in, or have been stretched across too many projects. Unlike a lot of faculty at OHSU, I don’t have my own R01. I am a collaborator, and I work hard at it.\nThe wake-up call for me was persistent thoughts of suicide. I realized that I didn’t care that much about anything and I knew that things had to change."
  },
  {
    "objectID": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html#regaining-balance",
    "href": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html#regaining-balance",
    "title": "Burning Out and Recovering (slowly)",
    "section": "Regaining Balance",
    "text": "Regaining Balance\nThe past year has been about finding my way back. I’ve been trying to do the following to regain my balance:\n\nWorking 4 days/week instead of the usual 5. On my “day off”, I try my best to contribute to open source projects, work on music, and do other things that recharge me. This has enabled me to say no more often.\nUnderstanding my values and restructuring my work to fit it. I’m much more focused on education, outreach, and mentoring these days. If a project comes up, I have to ask myself whether it is in line with my values. I’d like my research to include educational and data exploration components, such as Shiny.\nNot putting all my eggs in the work basket. I’m an artist in addition to being a data scientist. I play the cello, compose, and do photography. I make more time for these.\nRealizing that excellence does not require self sacrifice. I try to stop working on things obsessively, and only within the time I’m paid.\nMindfulness work. I’ve been using the Calm app to help me reorient me and not immediately react to adversities I encounter.\nGiving up on perfection. I’m learning to slowly abandon perfection on projects I don’t care that much about (more on this later), and instead turn my alerts off, or only respond to them within sane business hours."
  },
  {
    "objectID": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html#am-i-recovered",
    "href": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html#am-i-recovered",
    "title": "Burning Out and Recovering (slowly)",
    "section": "Am I Recovered?",
    "text": "Am I Recovered?\nI’d like to say I’m fully recovered, but I’m not. I’m not sure I’ll ever be 100% recovered, if recovered means the insane productivity levels I was showing before. I have been slowly asking myself whether projects expect insane levels of work for the amount of time I can dedicate to them.\nThankfully, I have had really great support from my department and from the university overall throughout. I have had support from the communities I help manage (BioData Club and Portland R User Group), and all the great people involved in open science that I have met (rOpenSci, Mozilla, The Carpentries).\nI would say that this support has been wonderful, and I feel like the journey back from research burnout has been ongoing. I wish there was a magic bullet for burnout, but working one’s way out of mental and emotional exhaustion ultimately takes a lot of time."
  },
  {
    "objectID": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html#resources",
    "href": "posts/2019-07-03-burning-out-and-recovering-slowly/index.html#resources",
    "title": "Burning Out and Recovering (slowly)",
    "section": "Resources",
    "text": "Resources\nIf you are burned out, please seek mental health assistance if you need help.\n\nThe Telltale Signs of Burnout - How to recognize when you’re burned out.\nMayo Clinic on Burnout - More information on Burnout.\nResponding to Burnout. How can we recognize burnout in others?\nWorkLife Podcast - This podcast by Adam Grant (organizational psychologist) has been full of ideas on how to make what I work on more meaningful. Highly recommended."
  },
  {
    "objectID": "posts/2018-02-28-data-science-and-systems-science/index.html",
    "href": "posts/2018-02-28-data-science-and-systems-science/index.html",
    "title": "Data Science and Systems Science",
    "section": "",
    "text": "I gave a talk for the Portland State University Systems Science seminar called How are Data Science and Systems Science Connected?. In this talk, I was highlighting current blind spots in Data Science that I think Systems Science approaches can address, especially that of interactions between features. I talked a little about my dissertation research (surrogate oncogenes), and the problem of black-box interpretability of predictive models.\nIf you’re interested in listening to the recording, the playback is available here: https://us.bbcollab.com/recording/059acf5cfed64f9abbf7cf432fdfb566\nSlides are here: https://laderast.github.io/sysc_data_sci\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Data {Science} and {Systems} {Science}},\n  date = {2018-02-28},\n  url = {https://laderast.github.io//posts/2018-02-28-data-science-and-systems-science},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Data Science and Systems Science.”\nFebruary 28, 2018. https://laderast.github.io//posts/2018-02-28-data-science-and-systems-science."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html",
    "title": "Setting Boundaries and Saying No",
    "section": "",
    "text": "I hate saying no. But as I go on in life, I realize that I can’t be all things to all people. I want to help people, but I also have to have enough energy to get through the day.\nIt’s easy to go through life doing what everyone expects of you and have nothing to show for it. In fact, this is often the curse of hyper-competent people. People tend to see you as a gear that fits within their own vision and expect you to do things for them.\nI want to outline an approach that will make it easier to say no. It starts with understanding yourself."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#what-are-your-values-who-are-you-fighting-for",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#what-are-your-values-who-are-you-fighting-for",
    "title": "Setting Boundaries and Saying No",
    "section": "What are your values? Who are you fighting for?",
    "text": "What are your values? Who are you fighting for?\nIn order to say no, you have to have clarity on your values and goals. What do you see your life as?\nHere are some of my values:\n\nMake doing data science inclusive, accessible, and fun\nIntroduce people to data science who hadn’t seen it before\nIntroduce useful and applicable data science skills that learners can provide\nTeach others about lifelong learning and participate and foster learning communities in Data Science\n\nAn equally useful question to ask yourself is who are you fighting for? In my case:\n\nUnderrepresented Minorities\nLGBTQ folks\nMyself and my husband\n\nI will say that as I get older, I have achieved greater clarity about what matters to me, and what doesn’t matter to me. This clarity helps me understand what is and what isn’t important to me."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#dont-respond-to-requests-right-away",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#dont-respond-to-requests-right-away",
    "title": "Setting Boundaries and Saying No",
    "section": "Don’t respond to requests right away",
    "text": "Don’t respond to requests right away\nIf you don’t want to add to your workload, I beg you to not give an answer right away. You need to check in with your feelings.\nHere is a boilerplate response if you need something to work off of:\n\n“Thanks for thinking of me with this opportunity. However, I am extremely busy at the moment. I need to evaluate my current workload in order to tell you yes or no.”\n\nThis buys you time to think and understand about how you feel about the request. Also, it gives you time to compose a thoughtful reply."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#ask-yourself-is-this-in-line-with-my-valuesgoals",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#ask-yourself-is-this-in-line-with-my-valuesgoals",
    "title": "Setting Boundaries and Saying No",
    "section": "Ask Yourself: Is this in line with my values/goals?",
    "text": "Ask Yourself: Is this in line with my values/goals?\nTake a deep breath, and ask yourself how you feel about the request.\nDoes it help further your goals or the people you’re fighting for? Will it upset your current work/life balance? Are you willing to pay that price?\nIf the answer is no to all of these, then the answer is no. No matter how noble their goals, if they don’t fit with yours and your current workload, you should probably say no. You can leave the door open for future collaborations, though.\nThe first few times, it may be agonizing to do this. But consulting your feelings about something will become second nature to you."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#gently-say-no.-dont-feel-sorry.",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#gently-say-no.-dont-feel-sorry.",
    "title": "Setting Boundaries and Saying No",
    "section": "Gently Say No. Don’t Feel Sorry.",
    "text": "Gently Say No. Don’t Feel Sorry.\nHere’s the thing: it is way easier to say no when the request doesn’t align with your values or it doesn’t benefit who you’re fighting for. It gives your response much more force.\nYou may need to do a little verbal/email jujitsu here, because some people don’t hear no. Use absolutes for right now, but keep the door open if you see yourself possibly working together in the future.\nMore boilerplate:\n\n“I really appreciate you thinking of me. However, I cannot help with your effort at the moment. I am trying to maintain work/life balance, and this would upset that. [I hope you’ll consider me for future efforts.]”\n\nYou don’t need to provide more personal reasons than the above, especially if the requestor is a stranger."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#boundaries-keep-you-sane",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#boundaries-keep-you-sane",
    "title": "Setting Boundaries and Saying No",
    "section": "Boundaries Keep You Sane",
    "text": "Boundaries Keep You Sane\nSaying no doesn’t feel good. But the more you do it, the more you can achieve work/life balance.\nYou can’t be everything to everyone, and you will end up dissappointing people. But you should feel okay about that, especially if their efforts are outside your own goals.\nI have to admit that I’m still learning how to do this. I’ve written this as much for myself as for others."
  },
  {
    "objectID": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#further-reading",
    "href": "posts/2020-10-14-setting-boundaries-and-saying-no/index.html#further-reading",
    "title": "Setting Boundaries and Saying No",
    "section": "Further Reading",
    "text": "Further Reading\n\nParts of this article are adapted from Unfuck Your Boundaries by Faith Harper. Her advice is no-nonsense, but it does require some soul searching to really utilize her advice."
  },
  {
    "objectID": "posts/2017-11-07-ODSC-Notes/index.html",
    "href": "posts/2017-11-07-ODSC-Notes/index.html",
    "title": "Notes on Open Data Science Conference West 2017",
    "section": "",
    "text": "I just came back from the Open Data Science Conference (ODSC) in San Francisco and I found it really stimulating and interesting. I learned a ton, met some great people working in very different fields, and overall found it quite worthwhile.\nHere are some of the highlights from my notes:"
  },
  {
    "objectID": "posts/2017-11-07-ODSC-Notes/index.html#workshops",
    "href": "posts/2017-11-07-ODSC-Notes/index.html#workshops",
    "title": "Notes on Open Data Science Conference West 2017",
    "section": "Workshops",
    "text": "Workshops\nscikit-learn intro Workshop and Advanced\nI admit that I am not really a Python person. But I am helping to develop some materials for an introductory workshop and I found this workshop and its materials to be a very beginner-friendly to scikit-learn and machine learning concepts, much like caret for R. All the slides and workshop materials are available at the above links.\nSparklyR Workshop\nI liked this workshop from John Mount of Win-Vector. It started out with a dplyr intro, and introduced us to the basics of Apache Spark, which is a cluster-computing based machine learning framework, which is designed to do very large queries and machine learning. RStudio’s Edgar Ruiz managed to get us each an RStudio Pro Instance running on AWS with all the required packages installed so we could test out the SparklyR package, which uses dplyr’s commands to run Spark jobs.\nIn-Memory Computing Essentials for Data Scientists\nThis was an introduction to Apache Ignite, which is a distributed, in-memory database that can be leveraged by different languages. The really interesting thing about Ignite is that it will colocate related data on the same cluster node, resulting in rapid queries within each node. I think this technology will become very important as we need more datasets to be openly accessible to compute on."
  },
  {
    "objectID": "posts/2017-11-07-ODSC-Notes/index.html#talks",
    "href": "posts/2017-11-07-ODSC-Notes/index.html#talks",
    "title": "Notes on Open Data Science Conference West 2017",
    "section": "Talks",
    "text": "Talks\nThese were the most interesting talks that I attended.\nVisually Explaining Statistical and Machine Learning Concepts\nThis was a great talk by Mike Freedman about his process of how he put together D3.js based visualizations to explain some statistical concepts. I thought the explanation of his process (isolate specific ideas, identify data structures, leverage visualization algorithms). Check out the slides above. They’re very cool.\nThe Wonder Twins: Data Science and Human Centered Design\nThis was a really interesting talk about the interplay between data science and design in helping encourage a mobile money system in Tanzania. It was inspiring to see how they had both designers and data scientists embedded and looking at how the mobile payment system worked. One interesting example was doing network analysis of the Mobile Money Agents, who distribute cash. They targeted a highly influential group of these agents based on this analysis. Very cool.\nThe People’s Data and The Deontology of Data Science\nI thought these were really interesting sides about the human side of data science. DJ Patil, who was chief data scientist under the Obama administration, talked about citizen-driven data projects and how it enabled a number of advances. The most interesting case was basically a parent built an online community of people who had a very rare disease condition so he could help his son with the condition.\nIgor Perisic (of LinkedIn) followed this with a talk about ethical issues in data science. In particular, he identified three different areas to concentrate on: 1) The Ethics of Data, 2) The Ethics of Algorithms, and 3) The Ethics of practice. He concentrated on the recent New York Times article about using machine learning to identify potential re-offenders in the prison system. The lack of transparency in how the algorithm identifies potential reoffenders is a huge ethical problem.\nIn all, I had an interesting time and I met lots of people in industry, which was a nice contrast to the academic side of things."
  },
  {
    "objectID": "posts/2018-06-06-self-care-and-self-compassion-in-academia/index.html",
    "href": "posts/2018-06-06-self-care-and-self-compassion-in-academia/index.html",
    "title": "Self Care and Self Compassion in Academia",
    "section": "",
    "text": "Note: I am not writing the following to complain, or excuse any past behavior. I am writing this just to be honest and transparent about my current struggles in academia. I hope it helps someone, or encourages other to seek help.\n\nI have to confess that I haven’t really been feeling all that well the past few months. Right now I am plagued with feelings that I am doing my work as an Assistant Professor wrong. I struggle with this persistent voice in my head that even when I am working at full tilt and beyond, that it’s not enough.\nFor most of my life, I have had chronic depression. I am a high functioning depressive; I have managed to get things done when I am depressed. However, this effort comes at the cost of self-care. I really think that some past posts have really come from a place of high stress and high anxiety. I apologize for the harsh tone of this post. I am also struggling with burnout at this point. I am beginning to feel like a lot of my efforts to encourage interdisciplinary collaborations are not feeling very productive. Part of this has to do with internal politics.\nI don’t think my depression is an excuse for my past behavior. The lack of self-care, however, is a major cause. When I don’t take care of myself, I reach a place of high stress and high anxiety and I can’t think straight. If I have been hurtful or bullying to others in any way because of this, I apologize.\nI still struggle with impostor syndrome. I do think that in many ways, this struggle has helped me become more compassionate as a teacher, remembering what concepts I struggled with. I have been trying my best to keep up with the added responsibilities of a faculty member: mentoring students, writing new course material, doing outreach, and also trying to keep the lights on by doing research (I won’t get into grant writing here; that’s another major depressive kettle of fish). I feel like I’ve currently been a disappointment as a faculty member so far.\nI am currently in therapy, which has been very helpful in understanding what problems I can address and what problems that are currently endemic to the academic system. I don’t know of any other field where one can accomplish a lot yet still feel like a failure.\nThe hardest thing for me is saying no, especially to students. But I realize that I can’t really be my best for students without taking care of myself. That balance between being considerate of others and also being considerate of my self is something that I am struggling with as a faculty member.\nI guess my point being is that I need to reframe saying no as being protective of myself and my current students rather than registering the disappointment when I say no. I’d like to help people, but I can help people better when I’m feeling well and better. Part of this may be coming to grips that academia may not be my place in the world if it demands that I sacrifice my mental health just to stay where I am.\nSo for the time being, I’d like to focus on getting better and feeling better. For your sake and mine. I will try to welcome newcomers to data science the best I can - just realize that what I may be able to do for you is a little limited right now.\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Self {Care} and {Self} {Compassion} in {Academia}},\n  date = {2018-06-05},\n  url = {https://laderast.github.io//posts/2018-06-06-self-care-and-self-compassion-in-academia},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Self Care and Self Compassion in\nAcademia.” June 5, 2018. https://laderast.github.io//posts/2018-06-06-self-care-and-self-compassion-in-academia."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "",
    "text": "knitr::include_graphics(\"poster_laderast.png\")\nWell, RStudio Conf 2019 has come and gone. I attended the main conference, starting with the poster session on wednesday and stayed through the tidyverse developer day on Saturday.\nTo say that the conference was inspiring was an understatement. So many talented people working on such interesting and inspiring packages! It made me excited again about doing data science and teaching data science.\nThis post is going to highlight the interesting talks about education and organizational management at the conference. There were lots of thought provoking talks and great resources about education. Here’s a link to the other resources, posters, and talks (thanks, Karl Broman!). There are a lot of new neat whiz bang features in RStudio, which I won’t cover, but are covered very well by others at Karl’s link above.\nUPDATE: I’m adding links to the videos as subheadings that you can click on. I won’t embed these directly because there is often related material with the video."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#available-teaching-resources",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#available-teaching-resources",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "Available Teaching Resources",
    "text": "Available Teaching Resources\nThere was much sharing of educational resources. I’ll talk about a few.\n\nCarl Howe’s Talk: The Next Million R Users\n\n\nMel Gregory: RStudio for Education\nCarl Howe (Director of Education for RStudio) presented a number of resources that RStudio is providing for educational purposes. Their goal is to train the next million R users. Here are some of the RStudio based educational resources that might be useful for everyone.\n\nRStudio Cloud is now available free for people who teach courses, who can send them a course syllabus to gain access. RStudio Cloud includes resources to make coursework publically or privately available, allows instructors to install default packages, and gives students an immediate way to start playing with R/RStudio. We are definitely going to use this in teaching our BMI 569/669 Data Analytics course. Mel Gregory’s talk was about the nuts and bolts of using RStudio Cloud in a classroom situation. Highly recommended to get familiar with the basics.\nMine Çetinkaya-Rundel also has a Data Science In a Box course available that can be forked and used by anyone. There is a great discussion of how to use the course and the tech stack (RStudio Cloud, GitHub, and Slack) needed to make the course runnable.\nThere are also a number of LearnR Tutorials built into RStudio Cloud called RStudio Primers that cover a lot of basic RStudio operations for the self-directed learners.\n\nJessica Minnier and I presented at the poster session about our LearnR tutorials, DSIExplore and dataLiteracyTutorial and our burro package, which lets users explore a new dataset with a simplified Shiny App. There’s even a function that lets you build a shiny app from a dataset that you can publish to a Shiny server such as shinyapps.io or RStudio Connect for sharing and having a data scavenger hunt together. Or as Angela Bassa calls it, an EDA Party. Whoo!\n\n\nIrene Stevens’ Talk: Teaching Data Science With Puzzles\nI really loved Irene Stevens’ talk about Teaching Data Science Using Puzzles. Irene and Jenny Bryan put together a simple project-based framework that lets students download simple data wrangling puzzles per week, lets them submit their answer to an answer server and get feedback, and when they solve it, it creates a minimal reproducible example (reprex) to share with their puzzle community. Very Slick!\n\n\nKelly Bodwin: Introductory Statistics with R\nKelly Bodwin gave a presentation of how she used Shiny apps as an intermediate step as part of an approach to teaching introductory statistics. Kelly is of the opinion that coding is good even for non majors, and her Shiny apps provide an intermediate step between a more click-based app and coding, asking the students to input their variables, their hypothesis, and highlights the output that results. I really liked this approach. I also really loved this slide:"
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#diversity-and-inclusiveness-make-for-better-data-science",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#diversity-and-inclusiveness-make-for-better-data-science",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "Diversity and Inclusiveness Make for Better Data Science",
    "text": "Diversity and Inclusiveness Make for Better Data Science\n\nJesse Mostipak: R4DS online learning community: Improvements to self-taught data science & the critical need for diversity, equity, and inclusion in data science education\nI really enjoyed Jesse Mostipak’s talk about her data science journey starting the R For Data Science (R4DS) learning community. She especially emphasized that data science was her path out of lower incomes and that she thought it needs to be more accessible to all. She talked about the barriers to going from “Using your computer for Netflix” to “Using your computer for data science” as real, and what she learned as a community manager for the R4DS learning community. Finally, she ended with a call for data scientists to share their mistakes, in order to model that making mistakes is ok.\n\n\nCaitlin Hudon: Learning from Eight Years of Data Science Mistakes\nSpeaking of mistakes, Caitlin Hudon gave a talk called “Learning from Eight Years of Data Science Mistakes”, which was really helpful. One of her messages was that “Mistakes count as experience”, and learning from past mistakes is extremely helpful for the whole team. However, it must be safe for team members to share their mistakes, and so creating the right culture of respect and teaching is important. This includes documentation, and including pseudocode next to the actual code to talk non-data scientists through is important. Likewise, making sure that you are solving the correct business problem requires communication. I really liked her discussion of the Rhetorical Triangle as a way to frame communications: Speaker, Audience, and Context.\n\n\nTracy Teal: Teaching R using inclusive pedagogy: Practices and lessons learned from over 700 Carpentries workshops\nTracy Teal from Data Carpentries also talked about their efforts to Teaching R using inclusive pedagogy, including fostering a growth mindset. Their instructor training is a wonderful 2-day crash course in teaching, and I’m going to propose that it be offered to our graduate students to prep them for teaching. (Full disclosure: I’ve contributed to these course materials.) I also appreciated how candid she was about how much further they need to go to increase the diversity of participants.\nDiversity and Inclusiveness was also a theme of Angela Bassa’s talk about Team Data Science. She pointed out that data science teams need to be diverse to serve audiences who are highly diverse."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#making-teaching-resources-for-community-college-instructors",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#making-teaching-resources-for-community-college-instructors",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "Making Teaching Resources for Community College Instructors",
    "text": "Making Teaching Resources for Community College Instructors\n\nMary Rudis: Catching the R wave: How R and RStudio are revolutionizing statistics education in community colleges (and beyond)\nA really interesting talk for me was Mary Rudis’ talk about her work teaching data science at the community college level. As head of the math department at Penn State Harrisburg, Mary has done a lot of work developing the first data science certification at the community college level. There are a lot more people attending community college than 4 year colleges, and we need to make paths to data science accessible for these students.\nA lot more work needs to be done to make data science accessible at this level. Mary has pointed out a number of efforts that need help, including StatPREP and the American Mathematical Association of Two Year Colleges. She also ended with a call for Shiny Developers to help Community College instructors develop tools they can use to teach data science."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#organizational-considerations-in-data-science",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#organizational-considerations-in-data-science",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "Organizational Considerations in Data Science",
    "text": "Organizational Considerations in Data Science\n\nJoe Cheng: Shiny in Production\n\n\nTonya Filz: The Resilient R Champion\nOrganizational considerations were also a really interesting part of RStudio Conf for me. Joe Cheng’s keynote on Shiny in Production spent some time talking about overcoming resistance to Shiny in your organization, including talking with Data Engineers and IT Security. His point was that oftentimes, resistance comes from the feelings of territorialism and how to overcome them. Tonya Filz also gave a talk about being a Resilient R Champion at your organization, stressing these issues and giving us case studies/examples to show leaders at your organization.\n\n\nHilary Parker: Cultivating creativity in Data work\nHilary Parker’s talk about Creativity and Data Science in the Organization, about applying system design principles to data science. Rather than thinking of ourselves as having a traditional role at the end of the analysis, Hilary suggested that we partipate in all parts of the process, including data collection. Her group suggested a change in an app, which resulted in a brand new data stream for them to analyse.\n\n\nAngela Bassa: Data science as a team sport\nAngela Bassa’s Talk about Team Data Science talked about the organizational aspects of making a data team, including specializing roles, making sure that knowledge and expertise was distributed, and managing burnout by making sure that no one team member is overloaded. She stressed resilience within a team by making sure that people are taking care of themselves, having redundancy in the team. Process is also important, by making sure data is well documented, by allotting time. She also stressed the importance of young and new data scientists to the team, because they aren’t biased with previous knowledge. I also thought she had excellent arguments about the cost of not expanding your data team - potential users who would be lost."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#encouraging-a-growth-mindset",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#encouraging-a-growth-mindset",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "Encouraging a Growth Mindset",
    "text": "Encouraging a Growth Mindset\n\nPanel: Growth & change of careers, organizations and responsibility in data science\nI really enjoyed the final panel discussion, which was about encouraging a Growth Mindset for Data Scientists in their careers. Karthik Ram, Tracy Teal, Angela Bassa, and Hilary Parker all fielded questions about growing as a data scientist. One of my favorite remarks was from Tracy Teal, who mused about the possibility of a Leadership Carpentry, teaching potential leaders essential skills such as mentoring, faciliation skills, and valuing team contributions. Angela Bassa also had a great quote that “leadership is programming people”. She also pointed out that the role of Data Scientists is to “Question Dogma” and pointed out the importance of saying “I don’t know”.\n\n\nDavid Robinson: The Unreasonable Effectiveness of Public Work\n\n\n\nMaking your work public\n\n\nThe growth mindset was also a theme of Dave Robinson’s Keynote, which was about making your data science work public at a number of levels. He outlined a number of different types of public contributions, such as short form (tweets, discussions about data), long form (blogs), code contributions, and even writing a book."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#direct-explicit-instruction-in-education",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#direct-explicit-instruction-in-education",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "Direct Explicit Instruction in Education",
    "text": "Direct Explicit Instruction in Education\n\nFelienne Hermans: Explicit Direct Instruction in Education\nFelienne Hermans gave a wonderful keynote about teaching computer programming. I won’t crib her story, because her telling it was a delightful journey and I encourage you to watch it for yourself at the link above. Part of her point was that programming pedagogy is dominated by the “tyranny of fun”, rather than focusing on teaching fundamentals. One of the problems with programming education is that we haven’t defined what these fundamentals are, compared to the pedagogy of reading, which talks about teaching with fundamentals such as phonics.\nWe expect students to explore things without teaching them enough, and that actually hurts their progress. Instead, we should also be teaching fundamentals, such as code reading out loud and rote memorization of programming patterns. By utilizing these fundamentals, she has found improved learning outcomes. As she says, motivation comes from seeing progress, not necessarily from exploration alone. Her results on using direct instruction have showed improved outcomes in terms of motivation.\nI have to admit that this talk really opened my eyes to the blind spots we have in programming education. Looking forward to incorporating these ideas in my coursework."
  },
  {
    "objectID": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#so-many-people-to-teach-so-many-ways-to-teach",
    "href": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/index.html#so-many-people-to-teach-so-many-ways-to-teach",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "section": "So Many People to Teach, So Many Ways to Teach!",
    "text": "So Many People to Teach, So Many Ways to Teach!\nSo that’s my summary about the education and organizational talks/posters at RStudio Conf. Hopefully it gives you an idea of what resources, opportunities, and pedagogy is out there."
  },
  {
    "objectID": "posts/2021-07-20-life-after-academia/index.html",
    "href": "posts/2021-07-20-life-after-academia/index.html",
    "title": "Life After Academia",
    "section": "",
    "text": "Well, it’s been over 3 months since I left my academic position to become a Bioinformatics Trainer for DNAnexus.\nFor the most part, it has been a very positive shift. And I do mean shift - there was a bit of culture shock coming to DNAnexus from academia. There are a lot of differences.\nI thought I’d summarize my experience here so far in terms of both positives and negatives."
  },
  {
    "objectID": "posts/2021-07-20-life-after-academia/index.html#real-positives",
    "href": "posts/2021-07-20-life-after-academia/index.html#real-positives",
    "title": "Life After Academia",
    "section": "Real positives",
    "text": "Real positives\n\nMy collaboration skills that I spent a lot of time developing has served me well in industry so far. DNAnexus has a very collaborative culture and there is not that much distance between new employees and senior employees. I also never liked the hierarchy between PhDs and non-PhDs, and there is very little condescension here.\nMy desire and passion to onboard students has directly translated to training in industry. There is a need for developing training that uses active learning techniques, and our customers want it. Not all of them are highly technical and used to the cloud, and it is an exciting challenge trying to help them learn what they need to learn.\nThere is program management. One of the worst things about being paid under multiple grants was that the sum total of my work was not visible. Everyone was always expecting everything, all the time. I really like how program management here helps to balance everyone’s workload and help them avoid burnout.\nRoles and responsibilities are specialized. I like this as one of the best benefits. I don’t have to wear all the hats, so I can specialize and get better at my specialty. I really want to help develop a kick-ass training plan to get more people working efffectively on our platform.\nBeing a beginner really helps in developing training. I have to confess that I haven’t stayed 100% on top of the cutting edge of bioinformatics. I am well versed in high performance computing, but transitioning to cloud computing has had a pretty high learning curve.\nCulture is a positive force and drives almost everything. Management, including PeopleOps (our version of HR) has been aware that the pandemic has increased productivity, but also that burnout has been increasing. They’ve been trying to be aware of this and instituting blackout days where the office is closed, encouraging employees to unplug. A lot of my understanding of organizations has helped me realize that this is a pretty positive place.\nPay and benefits are definitely better than academia. The pay increase is considerable, since they actually value your contributions."
  },
  {
    "objectID": "posts/2021-07-20-life-after-academia/index.html#real-negatives",
    "href": "posts/2021-07-20-life-after-academia/index.html#real-negatives",
    "title": "Life After Academia",
    "section": "Real Negatives",
    "text": "Real Negatives\nIt’s not all roses and puppies, however. Here are a few negatives.\n\nStarting is pretty overwhelming. There is a lot to learn. The first two weeks here were dedicated to a very extensive onboarding process. Every quarter, we have a Bootcamp for new employees, and it is about 2 weeks of classes, where we meet people from each team, and learn about the platform. It’s a pretty hardcore onboarding process, which was pretty overwhelming. On the plus side, I have a big picture of how the parts work together and who to ask for help.\nYou are specialized and part of a larger plan. I am part of the service organization within DNAnexus, which helps customers implement and utilize the platform effectively. I work with customers who need training, and we provide a set of coursework to get them onboard the platform. This means that my work is always in the context of the organization, and that thinking about my work as billable is important to achieving our goals as an organization.\nThe pace is faster than academia. Projects need to be in the context of the overall strategy of the organization. If you can pitch in, you can do some neat things. But your time is valuable, so you have to spend your time wisely.\n\nIn all, was the transition what I expected? I would say yes, but it’s not for everyone. I think that overall, my emotional intelligence serves me much better in the current position than it has in academia, where it was just a way for others to mindlessly take advantage of me. My desire to be a respectful collaborator has served me well so far.\nDefinitely do your research and ask the hard questions if you are making the switch. And find other people who are thinking about making the change, and ask others who have made the switch. You will need their support."
  },
  {
    "objectID": "posts/2018-01-23-shiny-workshop-materials-posted/index.html",
    "href": "posts/2018-01-23-shiny-workshop-materials-posted/index.html",
    "title": "A gRadual Introduction to Shiny",
    "section": "",
    "text": "Learning the flow of information between server and ui\n\n\n\n\nI just gave a workshop teaching the basics of Shiny (the interactive web visualization framework) for a group of PDX R users. We had 10 people attend, and most of the attendees managed to get through the material and had lots of good questions. I really enjoyed talking with everyone and I hope everyone learned something. We’re planning to give the workshop again to the larger PDX R user community, and some of the attendees last night have volunteered to be TAs.\nThe workshop materials consist of a GitHub repo and a Markdown document that can be done either in person or independently. The materials are freely available under an Apache 2.0 License.\nIn the workshop, we build a flexible csv (comma separated value) explorer that can load in csv data files with adaptive controls and tooltips.\nIn terms of packages, the workshop uses the tidyverse (mostly dplyr and ggplot2), and plotly to show some basic programming patterns in shiny:\n\nConnecting controls to ggplot2 aesthetics\nFiltering data using reactives\nThe observe/update_ pattern\nTooltips (the hard way/the plotly way)\nThe final product\n\nI’d love for more people to take a look at the workshop and would love any suggestions for making it better!\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {A {gRadual} {Introduction} to {Shiny}},\n  date = {2018-01-24},\n  url = {https://laderast.github.io//posts/2018-01-23-shiny-workshop-materials-posted},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “A gRadual Introduction to Shiny.”\nJanuary 24, 2018. https://laderast.github.io//posts/2018-01-23-shiny-workshop-materials-posted."
  },
  {
    "objectID": "posts/2018-10-15-clinical-data-wrangling/index.html",
    "href": "posts/2018-10-15-clinical-data-wrangling/index.html",
    "title": "Things we learned teaching clinical data wrangling",
    "section": "",
    "text": "Well, we just finished our clinical data wrangling workshop. This was a 12 hour workshop (spread over 4 days) where students got to work with a real research dataset (the Sleep Heart Health Study data). This is a workshop that we developed as part of an National Library of Medicine T15 training supplement in Data Science. The following is a short report describing the workshop and its outcomes."
  },
  {
    "objectID": "posts/2018-10-15-clinical-data-wrangling/index.html#intended-audience",
    "href": "posts/2018-10-15-clinical-data-wrangling/index.html#intended-audience",
    "title": "Things we learned teaching clinical data wrangling",
    "section": "Intended Audience",
    "text": "Intended Audience\nWe designed the workshop for our incoming informatics students (both clinical and biological majors) in order to introduce them to the difficulties of working with clinical data."
  },
  {
    "objectID": "posts/2018-10-15-clinical-data-wrangling/index.html#learning-objectives",
    "href": "posts/2018-10-15-clinical-data-wrangling/index.html#learning-objectives",
    "title": "Things we learned teaching clinical data wrangling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThese were our learning objectives for the workshop:\n\nUnderstand the biology of sleep and sleep apnea and how the biology informs the covariates measured in the Sleep Heart Health Study\nUnderstand why clinical data is useful and also why it’s difficult to work with\nLearn Exploratory Data Analysis techniques and use them to inform model building.\nLearn to assess logistic regression models using simple diagnostics."
  },
  {
    "objectID": "posts/2018-10-15-clinical-data-wrangling/index.html#the-dataset",
    "href": "posts/2018-10-15-clinical-data-wrangling/index.html#the-dataset",
    "title": "Things we learned teaching clinical data wrangling",
    "section": "The Dataset",
    "text": "The Dataset\nWe used the Sleep Heart Health Study dataset from the National Sleep Research Resource. This is a dataset of approximately 5800 patients that have over 3000 covariates. We limited our students to a smaller number of covariates (17), including our outcome of interest, cardiovascular disease."
  },
  {
    "objectID": "posts/2018-10-15-clinical-data-wrangling/index.html#workshop-format",
    "href": "posts/2018-10-15-clinical-data-wrangling/index.html#workshop-format",
    "title": "Things we learned teaching clinical data wrangling",
    "section": "Workshop Format",
    "text": "Workshop Format\nWe designed the workshop to be a mix of didactic lectures and active learning exercises. Where possible, we had students work in groups to answer questions about the data. These activities included a data scavenger hunt using our EDA exploration app, and a logistic modeling exercise.\n\nDay 1 Outline\n\nIntroduction, logistics, and groups assigned (30 minutes)\nBiology of Sleep and Cardiovascular Disease (40 minutes, format: in-person lecture)\nBreak Time (15 minutes)\nThe Value of Clinical Data (15 minutes, in-person lecture)\nClinical Data Quality (40 minutes, in-person lecture)\nLunch (90 minutes, with optional R setup session)\nExploring the SHHS Dataset (60 minutes, format: Data Scavenger Hunt w/ Shiny App, each team gets a task and has to show the class how to find the information)\nApplying the Clinical Wrangling Process: Diabetes (45 minutes, format: in-person lecture)\nLogistic Regression Model Basics (60 minutes, format: walkthrough of R Notebook)\n\n\n\nDay 2\n\nQuestion/Answer session about Logistic Regression and Modeling (50 minutes)\nAssignment about race variable (assigned to groups, take-home assignment)\n\n\n\nDay 3\n\nDiscussion about race as a covariate, sharing of findings\nOverview of hypertension and how it relates to cardiovascular disease and sleep apnea\nTemplate/R Notebook given for final presentation in groups (in-class lab time, template is structured as a series of decisions.)\n\n\n\nDay 4\n\nGroup presentations about covariate decisions and resulting model (1 hour, present final version of R notebook). At each decision stage, teams must decide on whether or not to include covariates or not given what they have found from exploring the data and justify their decision using EDA visualizations."
  },
  {
    "objectID": "posts/2018-10-15-clinical-data-wrangling/index.html#lessons-learned",
    "href": "posts/2018-10-15-clinical-data-wrangling/index.html#lessons-learned",
    "title": "Things we learned teaching clinical data wrangling",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nOverall, we believe the workshop went well, as it encouraged discussion about data and its appropriateness among the students. Students were engaged overall and asked lots of questions.\nThe final reports for each group were generated from a R Notebook. All three groups showed a thoughtful narrative and justification for each of the covariates included in the model.\n\nInteractive visualization removes barriers to understanding issues in data. Ted developed a Shiny App that allowed the students to visually browse and understand the data. Along with the EDA scavenger hunt (see below), this served as a good introduction for students to get their feet wet with the SHHS dataset.\nOur diverse backgrounds helped make the workshop accessible. Nicole Weiskopf has a background in data quality of clinical data, Eilis Boudreau does sleep study work, and I’m a bit of a mongrel.\nSecuring the cooperation of the data holders made the workshop possible. The dataset comes from the National Sleep Study Resource. Eilis knows Susan Redline, who heads that group and pitched the idea (over two sessions) to her group. Susan’s group was very enthusiastic and helpful, especially in helping the students get their data use agreements in so they could access the dataset.\nGroup work is learning work. We assigned each student to a group, and gave each group questions to answer and teach the class about the dataset. By pointing them to specific aspects of the data, we opened the door to discussion.\nEDA scavenger hunt. We had the students learn data exploration by giving them a scavenger hunt to look at the relationship between variables. Each group was then required to talk about their findings and which visualization helped them discover that relationship. For example, there is a relationship between age and race in our dataset; the “Other” category of race has a lower median age than the other two categories, “White” and “Black”.\nDidactic Teaching is also important. Nicole and Eilis covered both the biology of sleep apnea and the difficulty of understanding the implications of clinical data. Without this background, students would not be able to make informed decisions about their final model.\nGuide the Students, but don’t force discussion. This was important. We think the students need to connect the dots to really understand the issues. The final product (a logistic regression model predicting cardiovascular disease with an R Notebook) had steps and choices. But the choices for each group of students was different.\nA Code of conduct is necessary and important. We are big believers in psychological safety. If people don’t feel safe in the classroom environment (and let’s face it, grad school classrooms rarely are), they will be less likely to learn and contribute.\nData restrictions made deploying difficult. The activity materials were deployed as an RStudio project. However, we couldn’t share the data within a GitHub repo. As OHSU’s approved vendor is Box, we setup a box folder containing the material to be shared with students.\n\nWe were grateful for the incoming informatics students’ enthusiasm and patience as we got this workshop going. Also thanks to the NLM T15 Supplement in Data Science, without which we would not have gotten the opportunity to conceptualize, put together, and deliver this workshop. Thanks again to Susan Redline and the National Sleep Research Resource group, especially Dan Mobley who helped us with the last-minute data use agreements.\nLink to Workshop: https://github.com/laderast/clinical_data_wrangling"
  },
  {
    "objectID": "posts/2023-01-04-retrospective-laderas/index.html",
    "href": "posts/2023-01-04-retrospective-laderas/index.html",
    "title": "Retrospective and Themes for 2022",
    "section": "",
    "text": "Note: I found it incredibly useful (and empowering) to write a retrospective of everything that I worked on last year. This is a list of things I accomplished in 2022 at DNAnexus that do not include course development. I may summarize course development in a separate post.\n2022 was a fairly busy year for me, not just with DNAnexus Academy engagements, but also in outreach and working on improving the user experience with effective workshops, courses, and developing optimal workflows for getting the main tasks on the platform done.\nI’ve achieved this by observing user experiences and questions from the UKB RAP community, and from questions customers have asked during training sessions."
  },
  {
    "objectID": "posts/2023-01-04-retrospective-laderas/index.html#improving-user-experience-through-establishing-best-practices",
    "href": "posts/2023-01-04-retrospective-laderas/index.html#improving-user-experience-through-establishing-best-practices",
    "title": "Retrospective and Themes for 2022",
    "section": "Improving User Experience Through Establishing Best Practices",
    "text": "Improving User Experience Through Establishing Best Practices\nOne of my major passions in my work is to improve the overall user experience of the DNAnexus platform by establishing and suggesting effective workflows and best practices for getting tasks accomplished on the platform. These tasks include:\n\nExecuting batch jobs on the platform\nWorking with RStudio Effectively on RAP\nWorking with Docker\nDecoding pheno data from raw values\nUsing bash effectively on RAP.\n\nEach of these accomplishments has stemmed from observing gaps in customer knowledge in the platform. I believe that one of the biggest barriers to using the platform is still the feeling of impostor syndrome and I am passionate about giving customers the knowledge they need in formats that are helpful for them.\n\n104 - Cloud Computing for HPC Users. This was a course and webinar that I developed with Anastazie Sedlakova and Scott Funkhouser. It attempts to directly map skills and concepts that incoming users already have with on-premise HPC and directly translates these skills to cloud HPC. It has been rated highly by both UKB RAP users and other customers.\n312 - JSON for the DNAnexus Platform - This is a course that I developed that fills in a lot of the gaps with utilizing JSON effectively on the DNAnexus platform. It helps users who are unfamiliar with JSON read and modify JSON for use in applets, but also in parsing JSON responses with the platform using jq, and doing batch submissions with jq.\nBash for Bioinformatics - In response to many of the challenges that I have seen with customers and app development and running jobs, I put together a “missing manual” that addresses the gaps in knowledge of bash scripting and where this knowledge is helpful on the platform. It has gotten positive reception not only on the UKB RAP community, but also by Solution Science as a resource they can point potential customers to.\nRStudio for RAP webinar - With Anastazie Sedlakova, I developed a webinar that outlined how to work reproducibly with RStudio on UKB RAP. This involved not only showing users how to use RStudio Projects, but also how to save software environments for reproducible research, and run RStudio/Bioconductor via Docker containers.\nDocker for RAP webinar - This was a webinar that I developed with Ondrej Klempir on how to effectively use Docker on UKB RAP for batch processing. It provides specific bash recipes for saving docker image files, extending docker images, and running batch jobs with Docker image files.\nxvhelper - this is an R Package I have developed to help R users with decoding Pheno data extracted from the new dx extract_dataset functionality in dx-toolkit. R users are a major percentage of UKB RAP users, and this package will helps them with their overall user experience with the pheno data."
  },
  {
    "objectID": "posts/2023-01-04-retrospective-laderas/index.html#improving-internal-training-and-communication",
    "href": "posts/2023-01-04-retrospective-laderas/index.html#improving-internal-training-and-communication",
    "title": "Retrospective and Themes for 2022",
    "section": "Improving Internal Training and Communication",
    "text": "Improving Internal Training and Communication\nI have helped onboard a number of our new employees through both Bootcamp and a formal set of courses. This was in direct response to Dick's call for more internal training specifically for our sales team.\n\nTrained 2 incoming groups on platform, including Apollo features. We have increased our overall training in our onboarding program, including incoming members of the Sales team.\n\nThis has included frank discussions and addressing questions the Sales team has about the product, and helping them develop the language to effectively communicate about the platform.\nI believe this has helped not only in increasing knowledge, but also communication between various groups at DNAnexus.\n\nGave Science Deep Dive talk on Teaching: Using Cognitive Science to Make Training Stick\nTalk for xVantage Day: Using Quarto: https://laderast.github.io/qmd_rmd/\nTrained VN team on platform. As part of the Vietnam Team's onboarding, we delivered the Titan/Apollo courses to them.\nOutreach to other groups. I continue to work and communicate with members of Customer Care, Translational Medicine, Customer Success, UKB RAP, and Solution Science"
  },
  {
    "objectID": "posts/2023-01-04-retrospective-laderas/index.html#outreach-to-new-audiences",
    "href": "posts/2023-01-04-retrospective-laderas/index.html#outreach-to-new-audiences",
    "title": "Retrospective and Themes for 2022",
    "section": "Outreach to New Audiences",
    "text": "Outreach to New Audiences\nI want others to effectively utilize the platform and accomplish work with it. To this end, I have helped with outreach to a number of new audiences with Ben Busby and the UKB RAP team.\n\nRAP getting started workshops. I sat in on 5 sessions that were a combination of workshop and Q&A for new UKB RAP users. I was actively answering questions in the chat and providing links to material.\nHDR UK - I was part of a group of educators talking about TREs and their role in working with clinical data. Specifically I talked about the role of UKB RAP and how it fulfilled the requirements of a TRE (patient security) in spite of it being a cloud-based solution\nImaging Workshop - I helped Renee Qian with setup and running the MATLAB container on UKB RAP for the imaging workshop, including showing her how to use dxFUSE for her demo.\nISCB Academy - Ben and I presented various UKB RAP features as a workshop for ISCB Academy, and shared Bash for Bioinformatics as a resource for others.\nI developed and shared an R specific workflow for UKB RAP for exporting, decoding, and working with Pheno data using my dxhelper package. It was used for the in-person UKB RAP workshop this December."
  },
  {
    "objectID": "posts/2018-08-02-turning-down-the-noise/2018-08-02-turning-down-the-noise.html",
    "href": "posts/2018-08-02-turning-down-the-noise/2018-08-02-turning-down-the-noise.html",
    "title": "Turning down the noise",
    "section": "",
    "text": "I’m still in the process of recovering from my current bout of depression and anxiety. I’d like to talk about what is currently helping me moderate my anxiety. I have been practicing mindfulness and meditation for the past three years and I’m beginning to realize how necessary it is in our information dense age. Many of my symptoms of anxiety are really from an information overglut.\nI’m currently on way too many projects and am teaching as well. Everything wants my attention. We need to decide dates for a visit, etc. Booking travel, grading students, etc. The noise of academic life can be overwhelming and can prevent me from working effectively. A book I’m reading, Real Happiness at Work by Sharon Salzberg, talks about Attention Deficit Trait disorder:\nThis is especially prevalent for people like me who are on multiple grants and have many collaborators. I’m okay with putting out the occasional fire or dealing with an emergency deadline; if I believe in the project, I can muster the energy. However, the problem is when I have multiple fires to deal with from multiple people. The task switching leads to stress and leads to an inability to prioritize. This is where I’ve been the last few months.\nAnd this is when the voices of doubt begin to fuel my anxiety. On top of the enormous task list, there’s the feelings of failure and disappointment because I can’t get simple things done. In my head the voices reach a frenzy, a cacophony, a noise. And then I can’t think straight, prioritize, work on one thing.\nWhat is the solution to ADT and the noise of life? Mindfulness and unitasking. As Sharon Salzberg notes:\nThis idea (being a master of my environment) appeals to me in the midst of my academic anxiety. Practicing mindfulness (through daily meditation) helps me to focus on the here and now. We are wired to ruminated about the past (things we wish we’d done better) or the future (oh crap, I need to prepare upcoming stuff). Meditation is all about building that focus and attention on what’s before us. If we can’t do it all, we can at least work on what’s right in front of us.\nI’ve also been reading Anne Lamott’s Bird by Bird, a book reflecting on the hows and whys of writing and living your life while doing it. It’s a sobering view of why we write and how to keep on in the face of numerous adversities. There’s one passage that really resonated with me in the chapter called “Shitty First Drafts” in dealing with the incessant chatter of life:\nThis is another idea in mindfulness: welcoming and acknowledging our worries and negative voices. If we ignore or refuse these feelings, they just become stronger and louder in the din and the noise. I have all sorts of these feelings: I don’t work hard enough, I’m a failure, I’m letting down people who depend on me, Everyone is out there working on cooler things than me. Now I’m trying to take an effort to welcome these feelings into my mental space, saying “okay, I hear you and acknowledge you, so you don’t have to be yelling anymore”. And surprisingly, they do quiet down. Acknowledging the feelings of failure, thanking them for their contribution to the discussion and showing them the door is important. As Kelly Boys says in The Blind Spot Effect:\nAnd that helps lower the volume and free your attention.\nIf you are feeling generalized anxiety or attention deficit trait, I encourage you to look into mindfulness as a way of turning down the noise."
  },
  {
    "objectID": "posts/2018-08-02-turning-down-the-noise/2018-08-02-turning-down-the-noise.html#resources",
    "href": "posts/2018-08-02-turning-down-the-noise/2018-08-02-turning-down-the-noise.html#resources",
    "title": "Turning down the noise",
    "section": "Resources",
    "text": "Resources\nThese are some of the resources that I’ve used when writing this post.\n\nThe Mindful Geek: Secular Meditation for Smart Skeptics by Michael Taft. This is a great book for people who are interested in Mindfulness and the psychological and neuroscience research about why mindfulness works in reducing depression and anxiety. I started here. Michael Taft calls meditation “a technology for hacking the human wetware to improve your life”.\nReal Happiness at Work: Meditations for Accomplishment, Achievement, and Peace by Sharon Salzberg. This is a nice followup to Mindful Geek, talking about how we can regain control in our workplace.\nThe Blind Spot Effect: How to Stop Missing What’s Right in Front of You by Kelly Boys. I am really liking this book so far. It’s about how meditation and mindfulness can help us find our cognitive blind spots and move beyond them.\nBird by Bird: Some Instructions on Writing and Life by Anne Lamott. I recently started reading this on vacation. Writing can be a lonely and solitary life, and I find many of her suggestions about living to apply equally well to research."
  },
  {
    "objectID": "posts/2018-04-25-health-informatics-course-materials/index.html",
    "href": "posts/2018-04-25-health-informatics-course-materials/index.html",
    "title": "Health Informatics Course Materials",
    "section": "",
    "text": "Sorry for the lack of posts. I have been busy with co-teaching our Health Informatics course (HSMP410) for the OHSU/PSU School of Public Health.\nI’m trying to make most of my lectures activity-driven for my students, who are Community Health Education and Nursing majors. I believe that you can teach mathematical concepts visually, so I am experimenting with using LearnR/Shiny to teach the basics of data literacy. I’m also using datacamp-light to show my students a simple intro to data science.\nMore information here: HMSP410 Health Informatics and the HMSP410 repository\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Health {Informatics} {Course} {Materials}},\n  date = {2018-04-25},\n  url = {https://laderast.github.io//posts/2018-04-25-health-informatics-course-materials},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Health Informatics Course Materials.”\nApril 25, 2018. https://laderast.github.io//posts/2018-04-25-health-informatics-course-materials."
  },
  {
    "objectID": "posts/2015-05-24-The-Rise-Of-Fraud-in-High-IF-journals/index.html",
    "href": "posts/2015-05-24-The-Rise-Of-Fraud-in-High-IF-journals/index.html",
    "title": "High Impact Factor Journals Have Higher Retraction Rates",
    "section": "",
    "text": "Very interesting New York Times article about the rise of frauds and retractions in High Impact Factor journals. The retraction rates for High IF journals (such as Science, Cell, and Nature) are much higher than lower IF journals.\nFrom the article:\n\nJournals with higher impact factors retract papers more often than those with lower impact factors. It’s not clear why. It could be that these prominent periodicals have more, and more careful, readers, who notice mistakes. But there’s another explanation: Scientists view high-profile journals as the pinnacle of success — and they’ll cut corners, or worse, for a shot at glory.\n\nI would say that this is sad, but this is a consequence of the currently terrible funding climate and unreasonable expectations of study sections. If study sections dismiss grant writers because of an unreasonable expectation of past productivity, then it shouldn’t be surprising that the drive to make oneself look productive actively encourages fraud to get ahead.\n\n\n\nCitationBibTeX citation:@online{laderas2015,\n  author = {Ted Laderas},\n  title = {High {Impact} {Factor} {Journals} {Have} {Higher}\n    {Retraction} {Rates}},\n  date = {2015-05-24},\n  url = {https://laderast.github.io//posts/2015-05-24-The-Rise-Of-Fraud-in-High-IF-journals},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2015. “High Impact Factor Journals Have Higher\nRetraction Rates.” May 24, 2015. https://laderast.github.io//posts/2015-05-24-The-Rise-Of-Fraud-in-High-IF-journals."
  },
  {
    "objectID": "posts/2018-05-17-a-introvert-s-survival-guide-to-conferences/index.html",
    "href": "posts/2018-05-17-a-introvert-s-survival-guide-to-conferences/index.html",
    "title": "A Introvert’s Survival Guide to Conferences",
    "section": "",
    "text": "In academia, it’s inevitable to have to travel and present at conferences and meetings. As an introvert, I’ve been trying to compile a few tips that have helped me navigate large conferences so I don’t feel overwhelmed.\nIt is unfortunate that though the academic community has many introverts, conference and meeting structure is heavily biased towards extroverts. (No offense to extroverts, but some of you can sometimes seem like blowhards to us introverts.)\nThe question for introverts is how to connect with people at meetings while being true to our nature. We value meaningful and deep connections rather than shallow ones.\nThe world of conferences doesn’t really value introverts, but it needs us (at least there are some people thinking about making conferences more introvert friendly). We’re thoughtful, slow to react, but we can synthesize and bring deeper understanding. Give us space to think and be ourselves, and we will contribute and add diversity to the conversation."
  },
  {
    "objectID": "posts/2018-05-17-a-introvert-s-survival-guide-to-conferences/index.html#for-more-information",
    "href": "posts/2018-05-17-a-introvert-s-survival-guide-to-conferences/index.html#for-more-information",
    "title": "A Introvert’s Survival Guide to Conferences",
    "section": "For more information",
    "text": "For more information\nThere are lots of blogs written about this. Here are some of my favorites.\n\nHow to Make the Most of a Conference as an Introvert\nHow to Survive Big Conferences As an Introvert\nHow Introverts can Make the Most of Conferences"
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html",
    "title": "Understanding Tidy Evaluation in R",
    "section": "",
    "text": "Have you ever had something that no matter how many times someone explained it, you really had no idea what it was for? For me, that was Non Standard Evaluation (NSE) in R, and its newer cousin Tidy Evaluation, or tidyeval. I had a real learning block about it. I really wanted to understand it, but for some reason I just really wasn’t getting the general concepts.\nWhat is evaluation, really? For the longest time, I was extremely confused about it. When you provide an expression to R such as:\n\nlibrary(tidyverse)\nlibrary(rlang)\nthis_variable <- 2\nthis_variable * 6\n\n[1] 12\n\n\nYou notice that there is an output to this_variable * 6, which is 12. Evaluation is really about looking up variable names in an environment and then acting on the results. What is going on here is that R looks for an object that is named this_variable in our global environment, and then returns the value, 2, which it then substitutes in the expression. So our original expression:\n\nthis_variable * 6\n\n[1] 12\n\n\nBecomes this expression:\n\n2 * 6\n\n[1] 12\n\n\nWhich R knows how to calculate, the output of which is 12. But sometimes you want to pass an expression or a variable, as is, without evaluating it first. The best case for this is to passing a variable into a function. We can do this by wrapping them up in quosures or enquosures.\n\n\nA quosure and an enquosure can be thought of as envelopes around an object. They obscure certain properties of the object until they can be delivered into a function. The envelopes basically are a way to sneak variables and expressions into a function’s environment. When the envelope is in the function, we can open it up and evaluate what’s in the envelope. The trick to NSE and tidyeval is that we can control when the function evaluates the expression, by controlling when we open this envelope. We do this by using the UQ() or !! functions.\nIn other words, quosures and enquosures are ways to prevent R from looking up a variable’s value in our current environment (usually the global environment), and delay this lookup until we get them into the environment of interest. This might be one level down (in our function of interest), or several levels down (in a function called by our function).\nThe point is, R won’t open the envelope with our variable in it until we tell it to."
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html#with-quosures-values-can-come-along-for-the-ride",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html#with-quosures-values-can-come-along-for-the-ride",
    "title": "Understanding Tidy Evaluation in R",
    "section": "With quosures, values can come along for the ride",
    "text": "With quosures, values can come along for the ride\nWhy would we use quosures at all, instead of enquosures? Because with quosures we can actually bring some needed values along for the ride."
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html#what-about-lots-of-arguments",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html#what-about-lots-of-arguments",
    "title": "Understanding Tidy Evaluation in R",
    "section": "What about lots of arguments?",
    "text": "What about lots of arguments?\nThat’s what quos() is for. Ever notice that you can specify a number of unnamed arguments by specifying a ... in your function definition? And did you ever notice that select() can take lots of arguments such as select(mpg, cyl, wt)? That is the power of ... combined with quos()!\nquos takes a list and makes each element of the list a quosure."
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html#what-about-expressions",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html#what-about-expressions",
    "title": "Understanding Tidy Evaluation in R",
    "section": "What about expressions?",
    "text": "What about expressions?\nSay we wanted to pass an expression such as cyl > 2 into our function. We’ll need to wrap it up in enexpr() instead of enquo():\n\nfilter_on_column <- function(x, col_expr){\n  c_e <- rlang::enexpr(col_expr)\n\n  x %>%\n    ## The !! (called a bangbang) is just another way to use UQ()\n    ## I don't really like it, I'd rather use UQ()\n    filter(!! c_e)\n}\n\n#pass in a simple expression\nmtcars %>% filter_on_column(cyl > 2) %>% head(5)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n#pass in a compound expression\nmtcars %>% filter_on_column(cyl > 2 & qsec > 18) %>% head(5)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2"
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html#be-really-careful-with",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html#be-really-careful-with",
    "title": "Understanding Tidy Evaluation in R",
    "section": "Be really careful with !!",
    "text": "Be really careful with !!\nIn the above example, we used !!, called a bangbang, to unquote and evaluate our expression. Be really careful with what you put after the !!, since everything after it will be evaluated. If you have elements after the expression you don’t want to unquote, wrap the !! up in a set of parentheses:\n\nbang <- function(val2){\n  x <- enquo(val2)\n  return((!! x) + 10)\n}\n\nbang(5)"
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html#other-applications",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html#other-applications",
    "title": "Understanding Tidy Evaluation in R",
    "section": "Other applications",
    "text": "Other applications\nOne of the coolest applications of NSE is to write code that writes code. You have to be very careful with this, but it’s potentially really useful. On my list of things to do for my flowDashboard package is to write code that generates a standalone app given the data objects you supply it."
  },
  {
    "objectID": "posts/2017-12-19-understanding-tidyeval/index.html#for-more-information",
    "href": "posts/2017-12-19-understanding-tidyeval/index.html#for-more-information",
    "title": "Understanding Tidy Evaluation in R",
    "section": "For more information",
    "text": "For more information\nHopefully this was helpful in understanding NSE and tidyeval. I find that sometimes I have to write things up so I more clearly understand it. So, if anything, writing this was useful for clarifying my thinking.\nI’m indebted to Edwin Thoen’s code examples that helped me finally understand what’s going on with tidyeval: https://edwinth.github.io/blog/dplyr-recipes/\nI didn’t really talk about Base-R’s NSE, but I would say that this should at least give you enough background to understand what’s going on there."
  },
  {
    "objectID": "posts/2017-07-05-interesting-user2017talks/index.html",
    "href": "posts/2017-07-05-interesting-user2017talks/index.html",
    "title": "Interesting useR 2017 Talks",
    "section": "",
    "text": "Since I didn’t get to go to useR 2017 this year, I’m compiling the interesting talks. This is an ongoing list.\n\nhttps://user2017.sched.com/event/AxqM/automatically-archiving-reproducible-studies-with-docker\nhttps://user2017.sched.com/event/Axq4/clouds-containers-and-r-towards-a-global-hub-for-reproducible-and-collaborative-data-science\nhttps://user2017.sched.com/event/Axq9/scraping-data-with-rvest-and-purrr\nhttps://user2017.sched.com/event/Axq1/using-the-alphabetr-package-to-determine-paired-t-cell-receptor-sequences\nhttps://user2017.sched.com/event/AxqG/show-me-the-errors-you-didnt-look-for\nhttps://user2017.sched.com/event/AxqR/community-based-learning-and-knowledge-sharing\nhttps://user2017.sched.com/event/AxqT/r-based-computing-with-big-data-on-disk\nhttps://user2017.sched.com/event/AxqA/codebookr-codebooks-in-r\nhttps://user2017.sched.com/event/Axor/how-we-built-a-shiny-app-for-700-users Useful concepts: reactiveTrigger to force a rerender.\nhttps://user2017.sched.com/event/AxsL/ensemble-packages-with-user-friendly-interface-an-added-value-for-the-r-community\n\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {Interesting {useR} 2017 {Talks}},\n  date = {2017-07-05},\n  url = {https://laderast.github.io//posts/2017-07-05-interesting-user2017talks},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “Interesting useR 2017 Talks.” July 5,\n2017. https://laderast.github.io//posts/2017-07-05-interesting-user2017talks."
  },
  {
    "objectID": "posts/2017-12-14-using-synthetic-data/index.html",
    "href": "posts/2017-12-14-using-synthetic-data/index.html",
    "title": "Using Synthetic Data for Teaching Data Science",
    "section": "",
    "text": "Hi Everyone, our paper called Teaching data science fundamentals through realistic synthetic clinical cardiovascular data is now available to read on Biorxiv.\nIn this paper, we talk about a dataset that we synthesized for teaching aspects of clinical data that may be tricky to understand in data science. This dataset is interesting because it’s derived from a multivariate distribution based on real patient data, modeled as a Bayesian Network. Even when we knew true marginals for the real data, there was a lot of fine tuning to the Bayesian Network.\nWe’ve used this dataset for a couple of classes, and we’ve found that it helps highlight real issues in predictive modeling of clinical data. One of the largest is that most predictive models are based on a much older patient cohort (50+), which means that we don’t know much about how to predict cardiovascular risk in younger patients. Part of the teaching exercise is having the students choose a cohort of interest and then attempt to predict on that patient cohort.\nThe data is currently available as an R package here, including vignettes about how the data was generated: https://github.com/laderast/cvdRiskData\n\n\n\nCitationBibTeX citation:@online{laderas2017,\n  author = {Ted Laderas},\n  title = {Using {Synthetic} {Data} for {Teaching} {Data} {Science}},\n  date = {2017-12-14},\n  url = {https://laderast.github.io//posts/2017-12-14-using-synthetic-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2017. “Using Synthetic Data for Teaching Data\nScience.” December 14, 2017. https://laderast.github.io//posts/2017-12-14-using-synthetic-data."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "The Value of Compassion in Learning Data Science\n\n\n\n\n\n\n\n\n\n\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nUsing Cognitive Science to Make Training Stick\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nQuarto vs. RMarkdown - What’s Different?\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nWhy Shiny Modules\n\n\n\n\n\nLightning talk for PDX-R Shiny Happy Hour on why to use Shiny Modules.\n\n\n\n\n\n\nJun 6, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nUsing gRatitude to learn the tidyverse together\n\n\n\n\n\nMy lightning talk for Cascadia-R 2021 talking about our tidyverse assignment.\n\n\n\n\n\n\nJun 4, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nThe MD in .Rmd: Teaching Clinicians Analytics using R\n\n\n\n\n\nA talk given for R/Medicine 2020 about our Data Analytics course.\n\n\n\n\n\n\nMar 22, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nPsychological Safety\n\n\n\n\n\nSlides talking about the importance of psychological safety and codes of conduct in the classroom.\n\n\n\n\n\n\nMar 22, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nConversations about Sleep: Clinical Data Wrangling\n\n\n\n\n\nTalk given for AMIA Informatics Educators Meeting about our clinical data wrangling workshop.\n\n\n\n\n\n\nMar 22, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nData Storytelling\n\n\n\n\n\nLearn about tailoring your visualization to an audience through reducing cognitive load.\n\n\n\n\n\n\nMar 20, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nObject Oriented Systems in R\n\n\n\n\n\nA talk/workshop talking about three object oriented programming systems in R.\n\n\n\n\n\n\nMay 21, 2019\n\n\nTed Laderas, Scott Chamberlain\n\n\n\n\n\n\n\n\nData Scavenger Hunt: Exploring Data Together\n\n\n\n\n\nTalk for CSV Conf 2019 about the {burro} package.\n\n\n\n\n\n\nMay 9, 2019\n\n\nTed Laderas\n\n\n\n\n\n\n\n\nSystem Science and Data Science\n\n\n\n\n\nHow are System Science and Data Science connected?\n\n\n\n\n\n\nFeb 15, 2018\n\n\nTed Laderas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/2020-9-3_crops/index.html",
    "href": "articles/2020-9-3_crops/index.html",
    "title": "Tidy Tuesday: Crop Production",
    "section": "",
    "text": "library(tidytuesdayR)\n#This will open up in the help window\ntidytuesdayR::tt_available()"
  },
  {
    "objectID": "articles/2020-9-3_crops/index.html#key-crop-yields",
    "href": "articles/2020-9-3_crops/index.html#key-crop-yields",
    "title": "Tidy Tuesday: Crop Production",
    "section": "Key Crop Yields",
    "text": "Key Crop Yields\n\nkey_crop_yields <- datasets$key_crop_yields"
  },
  {
    "objectID": "articles/2020-9-3_crops/index.html#visdat",
    "href": "articles/2020-9-3_crops/index.html#visdat",
    "title": "Tidy Tuesday: Crop Production",
    "section": "Visdat",
    "text": "Visdat\n\nvisdat::vis_dat(key_crop_yields)"
  },
  {
    "objectID": "articles/2020-9-3_crops/index.html#skimr",
    "href": "articles/2020-9-3_crops/index.html#skimr",
    "title": "Tidy Tuesday: Crop Production",
    "section": "Skimr",
    "text": "Skimr\n\nskimr::skim(key_crop_yields)\n\n\nData summary\n\n\nName\nkey_crop_yields\n\n\nNumber of rows\n13075\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEntity\n0\n1.00\n4\n39\n0\n249\n0\n\n\nCode\n1919\n0.85\n3\n8\n0\n214\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1.00\n1990.37\n16.73\n1961.00\n1976.00\n1991.00\n2005.00\n2018.00\n▇▆▇▇▇\n\n\nWheat (tonnes per hectare)\n4974\n0.62\n2.43\n1.69\n0.00\n1.23\n1.99\n3.12\n10.67\n▇▅▂▁▁\n\n\nRice (tonnes per hectare)\n4604\n0.65\n3.16\n1.85\n0.20\n1.77\n2.74\n4.16\n10.68\n▇▇▃▁▁\n\n\nMaize (tonnes per hectare)\n2301\n0.82\n3.02\n3.13\n0.03\n1.14\n1.83\n3.92\n36.76\n▇▁▁▁▁\n\n\nSoybeans (tonnes per hectare)\n7114\n0.46\n1.45\n0.75\n0.00\n0.86\n1.33\n1.90\n5.95\n▇▇▂▁▁\n\n\nPotatoes (tonnes per hectare)\n3059\n0.77\n15.40\n9.29\n0.84\n8.64\n13.41\n20.05\n75.30\n▇▅▁▁▁\n\n\nBeans (tonnes per hectare)\n5066\n0.61\n1.09\n0.82\n0.03\n0.59\n0.83\n1.35\n9.18\n▇▁▁▁▁\n\n\nPeas (tonnes per hectare)\n6840\n0.48\n1.48\n1.01\n0.04\n0.72\n1.15\n1.99\n7.16\n▇▃▁▁▁\n\n\nCassava (tonnes per hectare)\n5887\n0.55\n9.34\n5.11\n1.00\n5.55\n8.67\n11.99\n38.58\n▇▇▁▁▁\n\n\nBarley (tonnes per hectare)\n6342\n0.51\n2.23\n1.50\n0.09\n1.05\n1.88\n3.02\n9.15\n▇▆▂▁▁\n\n\nCocoa beans (tonnes per hectare)\n8466\n0.35\n0.39\n0.28\n0.00\n0.24\n0.36\n0.49\n3.43\n▇▁▁▁▁\n\n\nBananas (tonnes per hectare)\n4166\n0.68\n15.20\n12.08\n0.66\n5.94\n11.78\n20.79\n77.59\n▇▃▁▁▁"
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html",
    "title": "Registered Nurses in the United States and Territories",
    "section": "",
    "text": "Which states have the highest overall wages for registered nurses? When did this happen?\nHave wages increased overall for registered nurses across all states?"
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#scaling-the-data-by-state",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#scaling-the-data-by-state",
    "title": "Registered Nurses in the United States and Territories",
    "section": "Scaling the data by state",
    "text": "Scaling the data by state\nLooking for trends in the nurses data, let’s try and scale each income so we can emphasize whether there were increases or decreases within each state. We’re just looking for trends here and whether the slope of these trends is the same for each state.\nNote that by scaling within a state (transforming each value to a z-score), we are losing information, but we can see whether wages are steadily increasing for each of the states/territories.\nIn general, with some exceptions (Guam and Virgin Islands), most registered nurses saw an increase in median hourly wages from 1998 to 2020.\n\nnurses %>%\n  mutate(state=forcats::fct_rev(state)) %>%\n  group_by(state) %>%\n  mutate(scaled_income = scale(hourly_wage_median)) %>%\n  ggplot() +\n  aes(x=year, y=state, fill=scaled_income) +\n  geom_tile(color=\"grey10\") +\n  scale_fill_distiller() +\n  #theme_ipsum_ps()\n  bplots::theme_avenir()\n\n\n\n\nSince we looked at median hourly income, the question is whether these trends are the same or different for the 10th and 90th percentiles of registered nurses."
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#th-percentile",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#th-percentile",
    "title": "Registered Nurses in the United States and Territories",
    "section": "10th Percentile",
    "text": "10th Percentile\n\nnurses %>%\n  mutate(state=forcats::fct_rev(state)) %>%\n  group_by(state) %>%\n  mutate(scaled_income = scale(hourly_10th_percentile)) %>%\n  ggplot() +\n  aes(x=year, y=state, fill=scaled_income) +\n  geom_tile(color=\"grey10\") +\n  scale_fill_distiller() +\n  bplots::theme_avenir() +\n  theme(axis.text.x=element_text(angle=90))"
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#th-percentile-1",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#th-percentile-1",
    "title": "Registered Nurses in the United States and Territories",
    "section": "90th Percentile",
    "text": "90th Percentile\nFor the most part, if you are in the 90th percentile of hourly wages, you have seen a leveling off of income after about 2008. After 2008, the 90th income seems pretty static and unchanging.\n\nnurses %>%\n  mutate(state=forcats::fct_rev(state)) %>%\n  group_by(state) %>%\n  mutate(scaled_income = scale(hourly_90th_percentile)) %>%\n  ggplot() +\n  aes(x=year, y=state, fill=scaled_income) +\n  geom_tile(color=\"grey10\") +\n  scale_fill_distiller() +\n  bplots::theme_avenir() +\n  ggtitle(\"90 percentile RNs have slower increases in income than the 10%\")"
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#pivoting-the-data-to-be-wider",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#pivoting-the-data-to-be-wider",
    "title": "Registered Nurses in the United States and Territories",
    "section": "Pivoting the data to be wider",
    "text": "Pivoting the data to be wider\nOne question we might ask are whether there are groupings by states in terms of the wage increases.\nWe can do this by pivoting the data and using the {heatmaply} package to make a matrix input suitable for heatmaply::heatmaply().\nHere, we take hourly_wage_median and use it in the values of our matrix. Our rows correspond to state and our columns correspond to year.\n\nnurse_median_frame <- nurses %>%\n  select(state, year, hourly_wage_median) %>%\n  arrange(year) %>%\n  tidyr::pivot_wider(names_from = year, values_from = hourly_wage_median) \n\nnurse_median_matrix <- nurse_median_frame[,-1]\nrownames(nurse_median_matrix) <- nurse_median_frame[[\"state\"]]\n\nWarning: Setting row names on a tibble is deprecated.\n\nnurse_median_matrix <- as.matrix(nurse_median_matrix)\n\nhead(nurse_median_matrix)\n\n            1998  1999  2000  2001  2002  2003  2004  2005  2006  2007  2008\nAlabama    17.63 18.09 19.60 19.99 20.60 20.81 21.23 22.43 23.52 24.92 25.80\nAlaska     22.37 23.02 24.90 26.13 26.45 26.47 28.69 28.54 30.41 33.48 34.42\nArizona    19.37 20.26 21.97 22.23 23.35 23.88 25.12 26.90 28.06 29.17 30.59\nArkansas   16.66 17.18 18.02 18.44 19.20 19.98 21.17 22.63 23.62 24.17 24.78\nCalifornia 23.95 25.12 26.50 27.36 28.38 29.47 31.61 33.15 35.23 36.77 38.93\nColorado   19.79 20.47 21.77 22.56 23.17 23.88 25.60 26.91 28.15 29.69 30.76\n            2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019\nAlabama    26.48 26.44 26.41 26.02 26.20 26.39 26.70 26.68 27.20 27.85 28.27\nAlaska     35.33 37.39 38.67 38.73 40.08 41.12 42.37 41.01 41.45 42.14 43.54\nArizona    31.78 33.11 34.42 34.24 34.14 34.00 34.38 34.94 35.70 36.43 36.93\nArkansas   25.10 25.28 25.90 26.16 26.56 26.72 26.76 27.26 27.68 28.68 29.01\nCalifornia 39.86 41.03 42.51 43.88 45.34 46.38 48.27 48.30 48.43 50.20 53.18\nColorado   31.74 31.81 32.35 32.22 32.73 32.83 32.95 33.05 34.27 35.03 36.10\n            2020\nAlabama    28.19\nAlaska     45.23\nArizona    37.98\nArkansas   29.97\nCalifornia 56.93\nColorado   36.78"
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#heatmap-with-no-scaling",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#heatmap-with-no-scaling",
    "title": "Registered Nurses in the United States and Territories",
    "section": "Heatmap with No scaling",
    "text": "Heatmap with No scaling\nWe can now ask questions about the actual income values. We make heatmaply only look at computing a dendrogram for the rows (states) to look for clustering patterns.\nNote we have to set our scale argument to none here.\n\nheatmaply(nurse_median_matrix, dendrogram = \"row\", \n          Colv = c(1:23), scale=\"none\",\n          main = \"Oregon, California, and Hawaii have the highest median wage from 2017-2020\")\n\nWarning in doTryCatch(return(expr), name, parentenv, handler): unable to load shared object '/Library/Frameworks/R.framework/Resources/modules//R_X11.so':\n  dlopen(/Library/Frameworks/R.framework/Resources/modules//R_X11.so, 0x0006): Library not loaded: '/opt/X11/lib/libSM.6.dylib'\n  Referenced from: '/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/modules/R_X11.so'\n  Reason: tried: '/opt/X11/lib/libSM.6.dylib' (no such file), '/Library/Frameworks/R.framework/Resources/lib/libSM.6.dylib' (no such file), '/Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home/lib/server/libSM.6.dylib' (no such file)\n\n\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\n`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead."
  },
  {
    "objectID": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#scaling-by-state",
    "href": "articles/2021-10-05-registered-nurses/2021-10-05-registered-nurses.html#scaling-by-state",
    "title": "Registered Nurses in the United States and Territories",
    "section": "Scaling by state",
    "text": "Scaling by state\nIf we are interested in relative (scaled) values, the dendrogram is a little less interesting. Overall you can see that all states showed an increase in hourly median wage over the years.\n\nheatmaply(nurse_median_matrix, dendrogram = \"row\", \n          Colv = c(1:23), scale=\"row\", \n          main=\"Upward trends overall in terms of hourly median wage\")\n\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\n`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead."
  },
  {
    "objectID": "articles/2020-08-11-avatar_last_airbender/index.html",
    "href": "articles/2020-08-11-avatar_last_airbender/index.html",
    "title": "Sentiment analysis of Avatar",
    "section": "",
    "text": "Look at the available datasets\n\nlibrary(tidytuesdayR)\n\n\n\nLoading the Data\n\n#incoming data comes in as a list\ndatasets <- tidytuesdayR::tt_load(\"2020-08-11\")\n\n--- Compiling #TidyTuesday Information for 2020-08-11 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `avatar.csv`\n    Downloading file 2 of 2: `scene_description.csv`\n\n\n--- Download complete ---\n\n#show the names of the individual datasets\nnames(datasets)\n\n[1] \"avatar\"            \"scene_description\"\n\n\n\navatar <- datasets$avatar\navatar[1:5,]\n\n# A tibble: 5 × 11\n     id book  book_num chapter   chapter_num character full_text character_words\n  <dbl> <chr>    <dbl> <chr>           <dbl> <chr>     <chr>     <chr>          \n1     1 Water        1 The Boy …           1 Katara    \"Water. … Water. Earth. …\n2     2 Water        1 The Boy …           1 Scene De… \"As the … <NA>           \n3     3 Water        1 The Boy …           1 Sokka     \"It's no… It's not getti…\n4     4 Water        1 The Boy …           1 Scene De… \"The sho… <NA>           \n5     5 Water        1 The Boy …           1 Katara    \"[Happil… Sokka, look!   \n# … with 3 more variables: writer <chr>, director <chr>, imdb_rating <dbl>\n\n\n\nscenes <- datasets$scene_description\nscenes[1:5,]\n\n# A tibble: 5 × 2\n     id scene_description                                                       \n  <dbl> <chr>                                                                   \n1     3 [Close-up of the boy as he grins confidently over his shoulder in the d…\n2     5 [Happily surprised.]                                                    \n3     6 [Close-up of Sokka; whispering.]                                        \n4     6 [A look of bliss adorns his face. He licks his lips and wiggles his fin…\n5     8 [Struggling with the water that passes right in front of her.]          \n\n\n\n\nMy Research Question\nDoes the sentiment of each character change over the multiple seasons? That is, does a character become more positive or more negative as their character develops?\nI will attempt to summarize the sentiment of each character across each episode.\nUsing tidytext to unnest_tokens() - that is, split each line into 1 word per row.\n\nlibrary(tidytext)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.6     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.3     ✓ stringr 1.4.0\n✓ readr   2.0.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\navatar_words <- avatar %>%\n  select(id, book, book_num, chapter, chapter_num, character, character_words) %>%\n  filter(character != \"Scene Description\") %>%\n  unnest_tokens(word, character_words)\n\navatar_words[1:10,]\n\n# A tibble: 10 × 7\n      id book  book_num chapter                chapter_num character word       \n   <dbl> <chr>    <dbl> <chr>                        <dbl> <chr>     <chr>      \n 1     1 Water        1 The Boy in the Iceberg           1 Katara    water      \n 2     1 Water        1 The Boy in the Iceberg           1 Katara    earth      \n 3     1 Water        1 The Boy in the Iceberg           1 Katara    fire       \n 4     1 Water        1 The Boy in the Iceberg           1 Katara    air        \n 5     1 Water        1 The Boy in the Iceberg           1 Katara    my         \n 6     1 Water        1 The Boy in the Iceberg           1 Katara    grandmother\n 7     1 Water        1 The Boy in the Iceberg           1 Katara    used       \n 8     1 Water        1 The Boy in the Iceberg           1 Katara    to         \n 9     1 Water        1 The Boy in the Iceberg           1 Katara    tell       \n10     1 Water        1 The Boy in the Iceberg           1 Katara    me         \n\n\n\n\nCharacters by Episode/Chapter\n\nepisode_count <- avatar %>%\n  count(character, chapter) %>%\n  select(character, chapter) %>%\n  filter(character != \"Scene Description\") %>%\n  distinct() %>%\n  count(character) %>%\n  arrange(desc(n))\n\nepisode_count %>% \n  DT::datatable()\n\n\n\n\n\n\n\n\nBit Players\n\nepisode_count %>%\n  filter(n == 1) %>%\n  arrange(character)\n\n# A tibble: 272 × 2\n   character         n\n   <chr>         <int>\n 1 Aang and Zuko     1\n 2 Aang:             1\n 3 Actor Bumi        1\n 4 Actor Iroh        1\n 5 Actor Jet         1\n 6 Actor Ozai        1\n 7 Actor Sokka       1\n 8 Actor Toph        1\n 9 Actor Zuko        1\n10 Actress Azula     1\n# … with 262 more rows\n\n\n\n\nCabbage merchant\nThe cabbage merchant appears in 4 episodes, and you can see his path to resignation as Aang and company keep busting up his cabbage kiosk.\n\navatar %>%\n  filter(character == \"Cabbage merchant\") %>%\n  select(chapter, character_words) %>%\n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      chapter\n      character_words\n    \n  \n  \n    The King of Omashu\nNo! My cabbages!\n    The King of Omashu\nMy cabbages!  You're gonna pay for this!\n    The King of Omashu\nOff with their heads! One for each head of cabbage!\n    The King of Omashu\nMy cabbages!\n    The Waterbending Scroll\nMy cabbages! This place is worse than Omashu!\n    The Serpent's Pass\nAhhh! My cabbages!\n    The Tales of Ba Sing Se\nMy cabba-  Oh, forget it.\n  \n  \n  \n\n\n\n\n\n\nWho Spoke the Most?\nSurprisingly, Sokka has the most lines.\n\nline_count <- avatar_words %>% \n  count(character) %>%\n  arrange(desc(n)) \n\nline_count[1:20,] %>%\n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      character\n      n\n    \n  \n  \n    Sokka\n18293\n    Aang\n17821\n    Katara\n14961\n    Zuko\n8972\n    Toph\n5434\n    Iroh\n5252\n    Azula\n3299\n    Zhao\n1607\n    Jet\n1604\n    Suki\n1221\n    Hakoda\n1065\n    Pathik\n1030\n    Roku\n1015\n    Ozai\n1002\n    Hama\n955\n    Mai\n844\n    Bumi\n818\n    Long Feng\n757\n    Warden\n722\n    Ty Lee\n705\n  \n  \n  \n\n\n\n\n\n\nUnderstanding Each Character’s Journey\nUsing tidytext, I do a sentiment analysis of each episode (here called a chapter) to determine the overal sentiment for a character.\n\nbing <- get_sentiments(\"bing\")\n\ncharacters <- c(\"Aang\", \"Katara\", \"Zuko\", \"Toph\", \"Iroh\", \"Sokka\", \"Azula\", \"Mai\", \"Ty Lee\")\n\nsentiment_summary <- avatar_words %>%\n  inner_join(bing) %>%\n  count(book_num, chapter_num, chapter, character, sentiment) %>%\n  filter(character %in% characters) %>%\n  arrange(book_num, chapter_num) %>%\n  pivot_wider(names_from = sentiment, values_from = n) %>%\n  mutate(positive = tidyr::replace_na(positive, 0),\n         negative = tidyr::replace_na(negative, 0)) %>%\n  mutate(sentiment = positive - negative)\n\nJoining, by = \"word\"\n\n\n\nindex_chapters <- avatar_words %>%\n  select(book_num, chapter_num) %>%\n  distinct() %>%\n  mutate(index = row_number())\n\n\nsentiment_index <- sentiment_summary %>% \n  inner_join(y= index_chapters, by=c(\"book_num\", \"chapter_num\"))\n\n\nout_plot <- ggplot(sentiment_index) +\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, episode_number=chapter_num) +\n  geom_col(show_legend = FALSE) +\n  facet_wrap(~character, ncol=2) +\n  labs(title= \"Each Character's Sentiment Journey\", x=\"Episode Number\",\n       subtitle = \"mouse over each graph for more information\") + \n  geom_vline(xintercept = 21, lty=2) +\n  geom_vline(xintercept = 41, lty=2)\n\nWarning: Ignoring unknown parameters: show_legend\n\nplotly::ggplotly(out_plot)\n\n\n\n\n\n\n\nSentiment Heatmap\n\ns_index <- sentiment_index %>%\n  tidyr::complete(chapter_num, character)\n\nggplot(s_index) +\n  aes(x=index, y=character, fill=sentiment) +\ngeom_tile() +\n  scale_fill_viridis_b(na.value=\"black\") \n\nWarning: Removed 30 rows containing missing values (geom_tile).\n\n\n\n\n\n\n\nZuko has the most interesting journey\nZuko has many ups and downs, which may reflect his overall lack of confidence and his tendency for self-loathing.\n\nzuko <- sentiment_index %>%\n  filter(character==\"Zuko\")\n\nout_plot <- ggplot(zuko) +\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, group=character, episode_number=chapter_num) +\n  geom_col(show_legend = FALSE) +\n  facet_wrap(~character, ncol=2) +\n  annotate(geom=\"text\", x=27, y= -8 , label = \"Zuko Alone\\nA Turning Point\") +\n  annotate(geom=\"text\", x=53, y = 11, label = \"Where Zuko\\ntrains Aang\") +\n  labs(title= \"Zuko has lots of ups and downs\", x=\"Episode Number\",\n       subtitle = \"mouse over for more episode information\") +\n  ylim(c(-13, 13)) +\n  geom_vline(xintercept = 21, lty=2) +\n  geom_vline(xintercept = 41, lty=2)\n\nWarning: Ignoring unknown parameters: show_legend\n\nplotly::ggplotly(out_plot)\n\n\n\n\n\n\n\nAang and Zuko’s Journeys\nPlotting the sentiment journey of Zuko and Aang together shows that they often mirror each other, except in the last parts of Book 3.\n\nzuko_aang <- sentiment_index %>%\n  filter(character %in% c(\"Zuko\", \"Aang\"))\n\nout_plot <- ggplot(zuko_aang) +\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, episode_number=chapter_num) +\n  geom_col(show_legend = FALSE, alpha=0.7) +\n  labs(title= \"Aang and Zuko's Journeys Often Mirror Each Other\", \n       x=\"Episode Number\",\n       subtitle = \"mouse over for more episode information\") +\n  ylim(c(-13, 13)) +\n    geom_vline(xintercept = 21, lty=2) +\n  geom_vline(xintercept = 41, lty=2)\n\nWarning: Ignoring unknown parameters: show_legend\n\nplotly::ggplotly(out_plot)\n\nWarning: Removed 4 rows containing missing values (position_stack).\n\n\n\n\n\n\n\n\nIroh is so chill and positive\n\niroh <- sentiment_index %>%\n  filter(character==\"Iroh\")\n\nout_plot <- ggplot(iroh) +\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, episode_number=chapter_num) +\n  geom_col(show_legend = FALSE) +\n  labs(title= \"Iroh is just so chill and positive\", x=\"Episode Number\",\n       subtitle = \"mouse over for more episode information\") +\n  ylim(c(-13, 13)) +\n    geom_vline(xintercept = 21, lty=2) +\n  geom_vline(xintercept = 41, lty=2)\n\nWarning: Ignoring unknown parameters: show_legend\n\nplotly::ggplotly(out_plot)\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{laderas2020,\n  author = {Ted Laderas and Ted Laderas},\n  title = {Sentiment Analysis of {Avatar}},\n  date = {2020-08-11},\n  url = {https://laderast.github.io//articles/2020-08-11-avatar_last_airbender},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, and Ted Laderas. 2020. “Sentiment Analysis of\nAvatar.” August 11, 2020. https://laderast.github.io//articles/2020-08-11-avatar_last_airbender."
  },
  {
    "objectID": "articles/2023-02-14-learning-data-science-is-hard/index.html",
    "href": "articles/2023-02-14-learning-data-science-is-hard/index.html",
    "title": "Learning Data Science is Hard (and why that matters)",
    "section": "",
    "text": "When I gave my keynote for University of Pittsburgh’s Love Data week about being compassionate when learning data science, there were a couple of great questions that came up.\nA lot of my thinking about this has come from my Carpentries Instructor Training, but I thought I’d just talk about this in a post for others who teach Data Science, especially to new learners."
  },
  {
    "objectID": "articles/2023-02-14-learning-data-science-is-hard/index.html#demotivation",
    "href": "articles/2023-02-14-learning-data-science-is-hard/index.html#demotivation",
    "title": "Learning Data Science is Hard (and why that matters)",
    "section": "Demotivation",
    "text": "Demotivation\nFor new learners, we have to be aware of demotivating them. Learning Data Science is an inherently difficult task, and it’s even harder to learn when the learner is beating themselves up because they feel inadequate or don’t understand what you’re demonstrating.\nThink about it this way: I think all of us have had a really bad mathematics instructor who made us feel like math was not something for us. That’s what we’re working against. We have to"
  },
  {
    "objectID": "articles/2023-02-14-learning-data-science-is-hard/index.html#normalize-the-data-science-struggle",
    "href": "articles/2023-02-14-learning-data-science-is-hard/index.html#normalize-the-data-science-struggle",
    "title": "Learning Data Science is Hard (and why that matters)",
    "section": "Normalize the Data Science Struggle",
    "text": "Normalize the Data Science Struggle\nAgain, Data Science isn’t easy to learn. A lot of the Data Science tasks we do requires understanding the paradigm of tabular data, which can be a heavy lift for some learners."
  },
  {
    "objectID": "articles/2023-02-14-learning-data-science-is-hard/index.html#reward-questions",
    "href": "articles/2023-02-14-learning-data-science-is-hard/index.html#reward-questions",
    "title": "Learning Data Science is Hard (and why that matters)",
    "section": "Reward Questions",
    "text": "Reward Questions\nBefore we get started, I acknowledge two things about questions:\n\nAsking questions requires courage\nAsking questions is a way of taking care other people in your group, because they probably have that same question."
  },
  {
    "objectID": "articles/2023-02-14-learning-data-science-is-hard/index.html#avoid-demotivating-terms-like-easy-or-just",
    "href": "articles/2023-02-14-learning-data-science-is-hard/index.html#avoid-demotivating-terms-like-easy-or-just",
    "title": "Learning Data Science is Hard (and why that matters)",
    "section": "Avoid demotivating terms like “easy” or “just”",
    "text": "Avoid demotivating terms like “easy” or “just”\nWhy should we avoid using “easy” and “just” when we talk about data science? Such as “Oh you just need to do XXXX”, or “YYYYY is really easy”.\nThink about those who don’t get something you say is “easy”. It makes them feel less than the rest of your learners who “get it”."
  },
  {
    "objectID": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery.html",
    "href": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery.html",
    "title": "Reframing Impostor Syndrome as the Road to Mastery",
    "section": "",
    "text": "At some point in your life in Data Science, you will probably struggle with impostor syndrome. We all do - in fact, even though I have used R and have done bioinformatics and data science for more than 15 years, I still struggle with this feeling. As a beginner, the mountain you must climb to master skills in data science seems like a long and impossible one.\nCaitlin Hudon, in her post about dealing with impostor syndrome has this to say about countering impostor syndrome:\nI think it’s important to try and reframe the feelings of impostor syndrome into something more positive. I think having self-compassion about the difficulties of the learning process can help.\nGeorge Leonard’s Mastery is a short book that I think can help provide the antidote to these feelings of fraud and inadequacy. I feel that beginners and learners would feel much better if their instructors would own up to their own personal shortcomings as learners. That is, instead of trying to project the image of the all knowledgable guru, instructors should show themselves as humble, lifelong learners as well.\nIn Mastery, Leonard talks about our unrealistic expectations and how these expectations can get in the way of actually learning and mastering a craft. We are conditioned by ads, movies, and social media that mastering a craft is a never ending set of ever rising climaxes (cue the training montage), that we can make continuous and steady progress by working hard enough.\nMastering a craft takes practice, and lots of it. We must learn to be contented to practice when we are on a plateau and are not making visible progress. As Leonard notes,\nLeonard outlines 5 principles that can sustain us in our road to mastery and away from impostor syndrome: Instruction, Practice, Surrender, Intentionality, and The Edge. I’m trying to map common feelings of impostor syndrome and show how these principles can counteract these feelings.\nInstruction. Leonard emphasizes the importance in finding good instructors and good mentorship that will help us to grow. Finding good instructors can actually be difficult and finding someone who remembers what it was like to be learning something is important. Avoid those instructors who say things like “it should now be obvious” or are disparaging when you don’t understand something.\nWhat you can do today: look at twitter and other forums for your community of learners and support those who give good instruction. Realize that not all teachers are good teachers; leave them and seek better ones if necessary.\nPractice. Practice for practices’ sake. Deliberate practice where you slowly build up your understanding and perceptions is important to your growth.\nWhat you can do today: Join communities of practice such as Tidy Tuesday and share your learning with others. Tidy Tuesday is extremely friendly and encouraging for beginners. Learn together and grow together. These are safe communities to share knowledge.\nIntentionality. This goes hand in hand with practice. Deliberate practice requires visualizing your process and guiding yourself gently.\nWhat you can do today: find small exercises and projects that help you reinforce what you’ve learned so far. Find people’s code and vignettes and modify them until you understand what they’ve done and how they structured their work.\nSurrender. At some point, you will have to give up your own social position as an expert to grow as a learner. When this happens, you must be willing to risk that standing to progress further. Leonard talks about a karate master learning aikido who was not willing to start from scratch, which impeded his learning. For many of us academics, being willing to abandon the comfort of what we have learned is especially difficult. We feel like we are risking our own social standing and reputation.\nWhat you can do today: be humble when faced with new concepts (for many impostor syndrome sufferers this is not the hard part). Recognize when you need to grow and when you have to leave old concepts behind.\nThe Edge. This is where things are undefined and scary. Still, part of the journey to mastery is a willingness to push your thoughts to beyond the horizon of what you thought was possible.\nWhat you can do today: Identify some goals that are just beyond your current skillset and be willing to push your learning to that point.\nLeonard maintains that true mastery is not due to innate talent. True mastery is due to tenacity and perserverance in the face of difficult learning. In fact, he suggests that learning things too easily means that you might lack perserverance when the going gets rough and your progress slows. He maintains that someone who perseveres will “have learned whatever [they] are practicing to the marrow of [their] bones.”"
  },
  {
    "objectID": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery.html#encouraging-mastery-as-a-community",
    "href": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery.html#encouraging-mastery-as-a-community",
    "title": "Reframing Impostor Syndrome as the Road to Mastery",
    "section": "Encouraging Mastery as a Community",
    "text": "Encouraging Mastery as a Community\nI think Caitlin’s prescriptions for community wide suggestions for reducing impostor syndrome are wonderful. Especially the advice to “Get comfortable with I don’t know”. Normalizing “I don’t know” within a community is incredibly important to making a psychologically safe learning environment.\nTo encourage learners, I think that creating a community of practice and helpfulness is vitally important to give new learners the support they need. When communities take responsibility for the learning of their members, something magic happens. Learning no longer feels lonely and there is no shame when you don’t immediately grasp a concept. Patience becomes the norm and people become more confident.\nFor me, this is the true value of schools and universities. To get the most out of online learning, you need to participate within a community that encourages you to learn further. Be on the pathway to mastery by participate within learning communities."
  },
  {
    "objectID": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery.html#further-reading",
    "href": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery.html#further-reading",
    "title": "Reframing Impostor Syndrome as the Road to Mastery",
    "section": "Further Reading",
    "text": "Further Reading\nMastery: https://www.goodreads.com/book/show/81940.Mastery"
  },
  {
    "objectID": "articles/tidyverse_functions/index.html",
    "href": "articles/tidyverse_functions/index.html",
    "title": "Underrated Tidyverse Functions",
    "section": "",
    "text": "I’m teaching an R Programming course next term. Jessica Minnier and I are developing the Ready for R Materials into a longer and more involved course.\nI think one of the most important things is to teach people how to self-learn. As learning to program is a lifelong learning activity, it’s critically important to give them these meta-learning skills. So that’s the motivation behind the Tidyverse function of the Week assignment.\nI asked on Twitter:\n\n\nHi Everyone. I’m teaching an #rstats course next quarter. One assignment is to have each student write about a #tidyverse function. What it’s for and an example.What are some less known #tidyverse functions that do a job you find useful?\n\n— Ted Laderas, PhD 🏳️‍🌈 (@tladeras) November 30, 2020\n\n\n\n\nHere are some of the highlights from the thread.\nI loved all of these. Danielle Quinn wins the MVP award for naming so many useful functions:\n\n\ndplyr::uncount()tidyr::complete()tidyr::fill() / replace_na()stringr::str_detect() / str_which()lubridate::ymd_hms() and related functionsggplot2::labs() - so simple, yet under appreciated!\n\n— Danielle Quinn (she/her) (@daniellequinn88) December 1, 2020\n\n\nfill() was highly suggested:\n\n\ntidyr::fill() - extremely useful when creating a usable dataset out of a spreadsheet originally built for data entry, in which redundant informations are only reported once at the beginning of the group they refer to, rather than in every row as needed for the analysis.\n\n— Luca Foppoli (@foppoli_luca) December 1, 2020\n\n\nMany people suggested the window functions, including lead() and lag() and the cumulative functions:\n\n\nCheck out the dplyr window functions, cummin, cummax, cumany and cumall. They don’t seen useful at first but they can solve really tricky aggregation problems. https://t.co/aDpXqSB2Vx\n\n— Robert Kubinec (@rmkubinec) December 1, 2020\n\n\nAlison Hill suggested problems(), which helps you diagnose why your data isn’t loading:\n\n\nOoh problems is a good function for importing rx https://t.co/P4ZR57PgOG\n\n— Alison Presmanes Hill (@apreshill) December 1, 2020\n\n\nI think that deframe() and enframe() are really exciting, since I do this operation all the time:\n\n\ntibble::deframe(), tibble::deframe()coercing a two-column df to named vector, which I prefer immensely to names(df) <- vec_of_names\n\n— E. David Aja (@PeeltothePithy) December 1, 2020\n\n\nunite(), separate() and separate_rows() also had their own contingent:\n\n\nI find myself using tidyr::unite() a lot to clean messy data - particularly useful for making unique and informative ID’s for each row. coalesce() and fill() are also little known gems! :)\n\n— Guy Sutton🐝🌾🇿🇦🇿🇼 (@Guy_F_Sutton) December 1, 2020\n\n\n\n\n\nI was bowled over by all of the replies. This was an unexpectedly really fun thread, and lots of recommendations from others.\nI thought I would try and summarize everyone’s suggestions and compile a list of recommended functions. I used this script with some modifications to pull all the replies to my tweet. In particular, I had to request for extended tweet mode, and I extracted a few more fields from the returned JSON.\nThis wrote the tweet information into a CSV file.\nThen I started parsing the data. I wrote a couple of functions, remove_users_from_text(), which removes the users from a tweet (by looking for words that begin with @) and get_funcs(), which uses a relatively simple regular expression to try to return the function (it looks for paired parentheses () or an underscore “-” to extract the functions). It actually works pretty well, and grabs most of the functions.\nThen I use separate_rows() to split the multiple functions into their separate rows. This makes it easier to tally all the functions.\n\nremove_users_from_text <- function(col){\n  str_replace_all(col, \"\\\\@\\\\w*\", \"\")\n  \n}\n\nget_funcs <- function(col){\n  out <- str_extract_all(col, \"\\\\w*\\\\(\\\\)|\\\\w*_\\\\w*\")\n  paste(out[[1]], collapse=\", \")  \n}\n\nparsed_tweets <- tweets %>%\n  rowwise() %>%\n  mutate(text = remove_users_from_text(text)) %>%\n  mutate(funcs = get_funcs(text)) %>%\n  ungroup() %>%\n  separate_rows(funcs, sep=\", \") %>%\n  select(date, user, funcs, text, reply, parent_thread) %>%\n  distinct()\n\nwrite_csv(parsed_tweets, file = \"cleaned_tweets_incomplete.csv\")\n\nrmarkdown::paged_table(parsed_tweets[1:10,-c(5:6)])\n\n\n\n  \n\n\n\nAt this point, I realized that I just needed to hand annotate the rest of the tweets, rather than wasting my time trying to parse the rest of the cases. So I pulled everything into Excel and just annotated the ones which I couldn’t pull from.\n\n\n\nHere are the function suggestions by frequency. Unsurprisingly, case_when() (which I cover in the main course), has the most number of suggestions, because it’s so useful. tidyr::pivot_wider() and tidyr::pivot_longer() are also covered in the course.\nThere are some others which were new to me, and a bit of a surprise, such as coalesce(), fill().\n\ncleaned_tweets <- read_csv(\"cleaned_tweets.csv\") %>% select(-parent_thread) %>%\n  mutate(user = paste0(\"[\",user,\"](\",reply,\")\")) %>%\n  select(-reply)\n\nRows: 266 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): date, user, funcs, text, reply, parent_thread\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfunctions_by_freq <- cleaned_tweets %>%\n  janitor::tabyl(funcs) %>%\n  filter(!is.na(funcs)) %>%\n  arrange(desc(n)) \n\nwrite_csv(functions_by_freq, \"functions_by_frequency.csv\")\n\nfunctions_by_freq %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\n\n\nHere’s all of the tweets from this thread (naysayers included). They are in somewhat order (longer threads are grouped).\nHere’s a link to the cleaned CSV file\n\nrmarkdown::paged_table(cleaned_tweets)\n\n\n\n  \n\n\n\n\n\n\nFeel free to use and modify.\n\nRMarkdown file used to generate this post\nPython Twitter Scraper (by Giovanni Mellini) - I used this because there wasn’t a ready made recipe in rtweet to extract replies - you have to use recursion to extract all of the thread replies that belong to a tweet, and this was easily modifiable.\nCleaned Tweets File (CSV)\n\n\n\n\nThis post is my thank you for everyone who contributed to this thread. Thank you!"
  },
  {
    "objectID": "articles/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students.html",
    "href": "articles/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students.html",
    "title": "What We learned teaching Python to Neuroscience Students",
    "section": "",
    "text": "Well, the week of teaching our Python Bootcamp for Neuroscientists is over. I had the pleasure of working with a great group of students, professors and instructors in developing the material, and had a great time teaching complete beginners to programming and Python.\nWe had the overall goal of introducting 21 Neuroscience Graduate Program students at OHSU to the basics of programming in Python using data that they were interested in: electrophysiology data, and confocal microscopy data. The course was designed to be a 1 credit course to encourage students to persist and finish it.\nThe format of the class was spread over 5 days (2.5 hours a day) and had the following schedule:\n\nIntroduction to basic data types in Python\nIntroduction to for loops and Pandas DataFrames\nUsing Pandas to analyse electrophysiology data\nUsing NumPy to analyse confocal microscopy data\nEvaluation of students; installing Python/Juypter; wrap-up with questions.\n\nI’ll just write up some random thoughts about our experiences about the course. We are definitely planning to give the course again next year, given the enthusiastic reception.\n\nThings that really worked well\nAvoid the first day blues of installing Python by using JupyterHub. I think one of the major pain points for beginners is installing software before they can even learn. Instead of making them install Python the first day, we had them sign into an AWS server that had JuypterHub deployed. JupyterHub is a multi-user server for Juypter Notebooks which had the right version of Python and our need the dependencies installed. So our students just needed a laptop and a web browser to access our lessons. We could update the notebooks by pulling changes from our course repo.\nStephen David, my fellow instructor, figured a lot of the difficult deployment details out. He has put together some handy instructions about deploying JuypterHub to AWS and keeping the accounts updated via a GitHub repo in case other people are interested in using our bootcamp materials.\nMake the atmosphere welcoming to beginners. In order to do so, we used many great tips from Software/Data Carpentry: modeling resilience by using live coding (and making mistakes along the way), using post-it notes for students to signal when they need help or are finished, and having plenty of TAs per student (at least 4 students/TA or instructor). We tried to emphasize that learning programming is an ongoing process, and that even we still have to Google errors on Stack Overflow. Showing that you can make mistakes and still recover is a big part of that.\nPlan some early wins and make the exercises as interactive as possible. For the most part, we tried to avoid lecturing too long and break up the session with interactive exercises. I also really don’t like workshops where the trainer/teacher moves on no matter whether people understand the material or not. By using the post-its to signal when they were done, we were able to more appropriately pace the workshop. We also planned on stopping points if we couldn’t get through the day’s materials.\nEmphasize working together and building a community. From the beginning, we emphasized that everyone needed to work together. I always emphasize the chain of help: 1) First your programming partner, 2) then the TA help. Discussing and working on issues together fosters a sense of community. I think there will be a group of students who will really want to learn more because of this.\nGetting feedback along the way. I still feel like being a teacher is about 75% preparation and 25% improvisation. You need to be flexible enough to come up with examples on the fly, and you need to evaluate whether students are getting the material along the way. The exercises we tried to sprinkle throughout the notebooks helped us understand where people were stumbling.\nPlanning follow-up sessions. Through BioData Club, we’re planning some follow-up sessions. Through DataCamp in the Classroom, I also got our students premium access. We also pointed students out to other Python-based courses at OHSU.\n\n\nSome things we could improve on\nI believe that given our time frame, we couldn’t really have anticipated many of these issues. We did our best to deal with them in the moment, however.\nDescribing the difference between Jupyter Notebooks and Python. At the beginning, we glossed over what a Jupyter Notebook was and really didn’t describe its relation to Python. I think next time we will open with describing the relationship between Jupyter and Python with a diagram, and revisit it on the last day.\nAnticipate the JupyterHub server requirements better. On Day 3, we had a large dataset that basically hosed the server because 21 students were trying to open it up at once. We managed to recover by getting another AWS server and dividing the students among the two, but we could have stress tested that day a little more. Lesson learned.\nGoing slowly enough. I am a very excitable teacher, to the point of which sometimes I go a little too fast. I have to confess that I may have sped through some of the material a little too fast. As a result, some of the students didn’t quite get what functions like enumerate() were for and the concept of unpacking a list. Luckily, Brad Buran covered these on Day 4 and the students felt comfortable enough to finish the programming test on the final day.\nSetting student expectations. It’s vital to show the students that they can learn programming, but also what’s possible if they do. One of the days was a big leap from the previous day, but we did mention that it’s really to show them what’s possible if they continued to learn about programming.\n\n\nWould we do it again?\nI would definitely say yes! We had to waitlist some students who really wanted to take it, and our overall feedback about the course was really positive. I hope that we can have more TAs, and have the future data workshops be more student driven.\n\n\nAcknowlegements\nThis was a collaboration between the Neuroscience Graduate Program (NGP) and the Department of Medical Informatics and Clinical Epidemiology (DMICE).\nThe NGP students involved in designing and testing the material were\n\nDaniela Saderi\nLucille Moore\nCharles Heller\nZack Schwartz\n\nFaculty/Instructors involved were:\n\nBrad Buran (Research Instructor)\nStephen David (NGP Assistant Professor)\nLisa Karstens (DMICE Assistant Professor)\nMichael Mooney (DMICE Assistant Professor)\nTed Laderas (DMICE Assistant Professor)\n\nThanks very much to Gary Westbrook (Director of the NGP program), Shannon McWeeney (Head of the Division of Bioinformatics and Computational Biology within DMICE), and Bill Hersh (Head of DMICE).\n\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {What {We} Learned Teaching {Python} to {Neuroscience}\n    {Students}},\n  date = {2018-01-17},\n  url = {https://laderast.github.io//2018-01-17-what-we-learned-teaching-python-to-neuroscience-students.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “What We Learned Teaching Python to\nNeuroscience Students.” January 17, 2018. https://laderast.github.io//2018-01-17-what-we-learned-teaching-python-to-neuroscience-students.html."
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html",
    "title": "How to Network in a Non-slimy Way",
    "section": "",
    "text": "Everyone talks about the importance of networking, but there is still a negative perception of networking. In some ways, there is this negative perception that it is inauthentic, slimy, and self-serving.\nHere’s the thing. Networking is not about selling yourself. It’s about finding real and meaningful connections to people. And that comes from being curious about other people, not from selling yourself.\nAfter a long time of networking the wrong way, I think I’ve found a way of networking that works for me. There are some major ideas to this:"
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#networking-as-your-authentic-self",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#networking-as-your-authentic-self",
    "title": "How to Network in a Non-slimy Way",
    "section": "1. Networking as Your Authentic Self",
    "text": "1. Networking as Your Authentic Self\nBe Yourself. This can be really hard, especially if you’re uncomfortable in large social situations. You have to find ways to make yourself comfortable.\nI will be honest and say that I can be tremendously awkward. But I’ve learned to let my curiosity show, and that seems to be what works for me.\nDon’t come in with the intention to blow away or impress someone with your accomplishments. Another pitfall of networking is being too eager to impress someone. This can result in a lot of discomfort on both sides.\nThe best thing about informational interviewing is that it helps you find the deep connections with someone else. If you ask questions about these deep connections, you’ll start forging bonds. Remember, play the long game.\nWhen you meet someone, ask yourself: how could we work together? How could we help each other? This gets you into the right frame of mind. For one, you aren’t intimidated by the accomplishments of someone you meet. On the other hand, you aren’t being condescending to them. Remember, curiosity is a great way to forge connections.\nIf you can help someone with a five minute favor, that is a low key way to connect."
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#bring-your-curiosity",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#bring-your-curiosity",
    "title": "How to Network in a Non-slimy Way",
    "section": "2. Bring Your Curiosity",
    "text": "2. Bring Your Curiosity\nLet curiosity be your guide. Ask authentic questions. I was taught early on that informational interviewing was the most important way to network. What is an informational interview? I define it as being curious about someone and asking good questions to get the conversation going.\nActive listening and asking questions are some of the fastest ways to connect with people. It’s true, but you need to find a style that works for you.\nYou have to find yourself in a mindset where you are curious about the other person. And that mindset is where you can start asking interesting questions that show that you’re really curious about them and their passions. Curiosity is the great democratizer and leveler.\nIf someone asks an interesting question during a session, that’s a good person to talk with afterwards. You have something to talk about. Just be careful of those people who seem to be intentionally antagonistic; you want someone who is also curious.\nDon’t be so quick to swipe left. Give people a chance to share their story. It is true that most people are not immediately relevant to you and your chosen field. But these people can be an incredible asset to you. Again, let curiosity be your guide. This is your opportunity to learn from them, and the gratitude that arises from learning something new can be a powerful motivator.\nYou never know. Most of the literature of successful networkers point out that it is usually our acquaintances to which we have weak ties that helps us find new work."
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#participate-in-activities",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#participate-in-activities",
    "title": "How to Network in a Non-slimy Way",
    "section": "3. Participate in Activities",
    "text": "3. Participate in Activities\nDo activities together. I have mentioned before that organizing and actively participating in conferences has been one way that I’ve met a ton of people. If there are volunteer opportunities or hackathons, these are low key ways of doing activities together. Especially when you’re working on things together in small groups, the opportunities to be curious about each other will come up.\nOne of the most fruitful activities I participated in was the rOpenSci unconference in 2018. I met a ton of people I am still really good friends with. I also participated in the 2019 Symposium on Statistics and Data Science. Through moderating a session on education, and a speed mentoring session, I met a ton of like-minded people. I also gained a reputation as a booster - I was as supportive as I could be of everyone in that session. CSVconf and RStudioConf were also great opportunities to meet people.\nThere are usually student volunteer opportunities at nearly every event, which let you not only meet people, but participate in events for free or for a reduced discount. Also, there are great groups such as R-Ladies and PyLadies which are great places to meet like-minded people."
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#practice-practice",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#practice-practice",
    "title": "How to Network in a Non-slimy Way",
    "section": "4. Practice, Practice",
    "text": "4. Practice, Practice\nAs an introvert, one of your strengths is that you can come prepared. Good informational interviewing takes time to learn; you need to practice at it, and especially with people you feel comfortable with. That way, if you slip up, it won’t feel as catastrophic. Being able to be gentle with yourself and laugh at your mistakes will really help you here.\nYou’ll begin to identify, through active listening, the deep connection points. It’s easier to learn this by practicing with someone you know.\nDon’t start with the cold informational interview, where you ask your neighbor about something. This is actually the highest level of difficulty, where you know nothing about someone."
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#moderate-your-expectations",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#moderate-your-expectations",
    "title": "How to Network in a Non-slimy Way",
    "section": "5. Moderate Your Expectations",
    "text": "5. Moderate Your Expectations\nDon’t put all your networking eggs in one basket. One thing I used to do was identify one or two people and put all of my networking eggs in one basket, especially if they were well known. If I couldn’t meet that person, then I thought I was wasting my time. Again, everyone is interesting, so adjust your expectations.\nNetworking can be exhausting. Don’t push yourself too hard. Set some hard limits when networking. Don’t expect to be able to talk with everyone. Maybe talking to three people at a conference is all you can do right now. That’s fine. With practice, you’ll be able to do more.\nRemember, almost everyone at conferences is in the same boat as you. Be gentle on yourself, and you’ll figure it out."
  },
  {
    "objectID": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#promoting-the-work-of-others",
    "href": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/how-to-network-in-a-non-slimy-way.html#promoting-the-work-of-others",
    "title": "How to Network in a Non-slimy Way",
    "section": "6. Promoting the work of others",
    "text": "6. Promoting the work of others\nFinally, one low-key way to make a connection with someone is to promote their work in an honest way.\nI love Twitter for this, because you can quote retweet other people’s work and provide an honest assessment of people’s work you are passionate about. Oftentimes, just a couple of comment tweets is enough to make a connection."
  },
  {
    "objectID": "articles/2020-07-08_coffee/index.html",
    "href": "articles/2020-07-08_coffee/index.html",
    "title": "Coffee Data Exploration",
    "section": "",
    "text": "Load your dataset in with the function below. The input is the date the dataset was issued. You should be able to get this from the tt_available() function.\n\ncoffee <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv')\n\nRows: 1339 Columns: 43\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "articles/2020-07-08_coffee/index.html#processing-method-dry-natural",
    "href": "articles/2020-07-08_coffee/index.html#processing-method-dry-natural",
    "title": "Coffee Data Exploration",
    "section": "Processing Method: Dry / Natural",
    "text": "Processing Method: Dry / Natural\n\ncoffee %>%\n  filter(processing_method == \"Natural / Dry\") %>%\n  mutate(country_of_origin = fct_reorder(country_of_origin, total_cup_points, median)) %>%\nggplot() + \n  aes(y=total_cup_points, x=country_of_origin, fill=country_of_origin) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle=90), legend.position = \"none\") +\n  coord_flip() +\n  labs(title=\"Tanzania leads with ratings in Natural/Dry\")"
  },
  {
    "objectID": "articles/2020-07-08_coffee/index.html#processing-method-washed-wet",
    "href": "articles/2020-07-08_coffee/index.html#processing-method-washed-wet",
    "title": "Coffee Data Exploration",
    "section": "Processing Method: Washed / Wet",
    "text": "Processing Method: Washed / Wet\n\ncoffee %>%\n  filter(processing_method == \"Washed / Wet\") %>%\n  mutate(country_of_origin = fct_reorder(country_of_origin, total_cup_points, median)) %>%\nggplot() + \n  aes(y=total_cup_points, x=country_of_origin, fill=country_of_origin) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle=90), legend.position = \"none\") +\n  coord_flip() +\n  labs(title=\"US leads in Ratings in Washed/Wet\")"
  },
  {
    "objectID": "articles/2020-07-08_coffee/index.html#mexico-processing-methods",
    "href": "articles/2020-07-08_coffee/index.html#mexico-processing-methods",
    "title": "Coffee Data Exploration",
    "section": "Mexico: Processing Methods",
    "text": "Mexico: Processing Methods\n\ncoffee %>%\n  filter(country_of_origin == \"Mexico\") %>%\n  mutate(processing_method = fct_reorder(processing_method, total_cup_points, median)) %>%\n  ggplot() +\n  aes(y=total_cup_points, x=processing_method,  fill=processing_method) +\n  geom_boxplot(color=\"black\") +\n  coord_flip()"
  },
  {
    "objectID": "articles/2021-10-19-great-pumpkins/2021-10-19-great-pumpkins.html",
    "href": "articles/2021-10-19-great-pumpkins/2021-10-19-great-pumpkins.html",
    "title": "Pumpkins, Pumpkins, Pumpkins",
    "section": "",
    "text": "Intial EDA\n\n\nRows: 28,065\nColumns: 14\n$ id                <chr> \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2…\n$ place             <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"…\n$ weight_lbs        <chr> \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1…\n$ grower_name       <chr> \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen…\n$ city              <chr> \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo…\n$ state_prov        <chr> \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc…\n$ country           <chr> \"United States\", \"United States\", \"United States\", \"…\n$ gpc_site          <chr> \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum…\n$ seed_mother       <chr> \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar…\n$ pollinator_father <chr> \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open…\n$ ott               <chr> \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", …\n$ est_weight        <chr> \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141…\n$ pct_chart         <chr> \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0…\n$ variety           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\n\n\n\nData summary\n\n\nName\npumpkins\n\n\nNumber of rows\n28065\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1\n6\n6\n0\n54\n0\n\n\ngrower_name\n0\n1\n4\n79\n0\n7982\n0\n\n\ncountry\n0\n1\n5\n79\n0\n75\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncity\n2779\n0.90\nFALSE\n3218\nSte: 292, Nap: 183, Por: 178, St.: 162\n\n\nstate_prov\n0\n1.00\nFALSE\n188\nOth: 2242, Ont: 2021, Wis: 1910, Cal: 1211\n\n\ngpc_site\n0\n1.00\nFALSE\n220\nOhi: 759, Wie: 749, Ear: 722, Bau: 548\n\n\nseed_mother\n8537\n0.70\nFALSE\n9996\nunk: 277, Unk: 260, 214: 122, 200: 104\n\n\npollinator_father\n10302\n0.63\nFALSE\n4538\nope: 2658, Ope: 2065, sel: 2020, Sel: 1875\n\n\nvariety\n27341\n0.03\nFALSE\n86\nBig: 349, Dom: 150, Del: 37, Big: 33\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nplace\n2381\n0.92\n520.66\n499.83\n1.0\n119\n265.0\n909.00\n1798.0\n▇▂▂▂▁\n\n\nweight_lbs\n5267\n0.81\n303.52\n295.23\n0.1\n70\n169.5\n526.38\n999.8\n▇▂▂▂▂\n\n\nott\n3211\n0.89\n202.46\n154.89\n0.0\n0\n233.0\n338.00\n1132.0\n▇▇▁▁▁\n\n\nest_weight\n8233\n0.71\n273.48\n314.90\n0.0\n0\n135.0\n518.00\n998.0\n▇▂▂▂▂\n\n\npct_chart\n3211\n0.89\n0.45\n17.06\n-100.0\n-3\n0.0\n3.00\n830.0\n▇▁▁▁▁\n\n\n\n\n\nUgh. We need to do some data cleaning.\n\n\n                                                                                         state_prov\n  1498 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(91 exhibition only,\\r\\n               29 damaged)\n     151 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               2 damaged)\n 1569 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(154 exhibition only,\\r\\n               24 damaged)\n     159 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(1 exhibition only,\\r\\n               2 damaged)\n     160 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(3 exhibition only,\\r\\n               2 damaged)\n 1681 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(108 exhibition only,\\r\\n               31 damaged)\n 1742 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(104 exhibition only,\\r\\n               46 damaged)\n     179 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(2 exhibition only,\\r\\n               2 damaged)\n  1798 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(99 exhibition only,\\r\\n               40 damaged)\n     185 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               4 damaged)\n 1883 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(111 exhibition only,\\r\\n               37 damaged)\n 1900 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(128 exhibition only,\\r\\n               29 damaged)\n 1905 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(103 exhibition only,\\r\\n               43 damaged)\n     192 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(2 exhibition only,\\r\\n               3 damaged)\n    194 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(10 exhibition only,\\r\\n               4 damaged)\n     197 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(7 exhibition only,\\r\\n               4 damaged)\n 1980 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(150 exhibition only,\\r\\n               32 damaged)\n     200 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(7 exhibition only,\\r\\n               4 damaged)\n    203 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(11 exhibition only,\\r\\n               2 damaged)\n     206 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(3 exhibition only,\\r\\n               3 damaged)\n     206 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               2 damaged)\n    219 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(6 exhibition only,\\r\\n               12 damaged)\n     219 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(7 exhibition only,\\r\\n               4 damaged)\n    226 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(14 exhibition only,\\r\\n               2 damaged)\n     227 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               1 damaged)\n    246 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(18 exhibition only,\\r\\n               2 damaged)\n    253 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(18 exhibition only,\\r\\n               5 damaged)\n    254 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(9 exhibition only,\\r\\n               10 damaged)\n    256 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(11 exhibition only,\\r\\n               1 damaged)\n    271 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(22 exhibition only,\\r\\n               2 damaged)\n    272 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(15 exhibition only,\\r\\n               4 damaged)\n    273 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(22 exhibition only,\\r\\n               0 damaged)\n    275 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(13 exhibition only,\\r\\n               0 damaged)\n    278 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(19 exhibition only,\\r\\n               2 damaged)\n    283 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(27 exhibition only,\\r\\n               2 damaged)\n    289 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(26 exhibition only,\\r\\n               4 damaged)\n    290 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(15 exhibition only,\\r\\n               5 damaged)\n    291 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(16 exhibition only,\\r\\n               4 damaged)\n     292 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(9 exhibition only,\\r\\n               3 damaged)\n    294 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(23 exhibition only,\\r\\n               3 damaged)\n    300 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(14 exhibition only,\\r\\n               1 damaged)\n    314 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(21 exhibition only,\\r\\n               2 damaged)\n    314 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(31 exhibition only,\\r\\n               3 damaged)\n    316 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(24 exhibition only,\\r\\n               1 damaged)\n    319 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(29 exhibition only,\\r\\n               2 damaged)\n    326 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(29 exhibition only,\\r\\n               3 damaged)\n    328 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(29 exhibition only,\\r\\n               2 damaged)\n    330 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(24 exhibition only,\\r\\n               2 damaged)\n    334 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(27 exhibition only,\\r\\n               2 damaged)\n    364 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(49 exhibition only,\\r\\n               6 damaged)\n    366 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(38 exhibition only,\\r\\n               6 damaged)\n    370 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(27 exhibition only,\\r\\n               5 damaged)\n    383 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(47 exhibition only,\\r\\n               6 damaged)\n   451 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(73 exhibition only,\\r\\n               13 damaged)\n                                                                                            Alabama\n                                                                                             Alaska\n                                                                                            Alberta\n                                                                                            Antwerp\n                                                                                             Aragon\n                                                                                            Arizona\n                                                                                           Arkansas\n                                                                                 Baden-Wuerttemberg\n                                                                                         Basilicata\n                                                                                     Basque Country\n                                                                                            Bavaria\n                                                                                             Berlin\n                                                                                               Bern\n                                                                                        Brandenburg\n                                                                                   British Columbia\n                                                                                         Burgenland\n                                                                                         California\n                                                                                           Campania\n                                                                                          Carinthia\n                                                                                          Catalonia\n                                                                                    Central Finland\n                                                                                           Colorado\n                                                                                        Connecticut\n                                                                                           Delaware\n                                                                                      East Flanders\n                                                                                     Emilia-Romagna\n                                                                                            England\n                                                                                    Flemish Brabant\n                                                                                            Florida\n                                                                                          Friesland\n                                                                                            Galicia\n                                                                                         Gelderland\n                                                                                            Georgia\n                                                                                        Graubuenden\n                                                                                     Greater Poland\n                                                                                             Hawaii\n                                                                                              Hesse\n                                                                                              Idaho\n                                                                                           Illinois\n                                                                                            Indiana\n                                                                                               Iowa\n                                                                                             Kansas\n                                                                                           Kentucky\n                                                                                        Kymenlaakso\n                                                                                           La Rioja\n                                                                                            Lapland\n                                                                                              Lazio\n                                                                                      Lesser Poland\n                                                                                            Limburg\n                                                                                           Lombardy\n                                                                                          Louisiana\n                                                                                      Lower Austria\n                                                                                       Lower Saxony\n                                                                                     Lower Silesian\n                                                                                             Lubusz\n                                                                                              Maine\n                                                                                           Manitoba\n                                                                                           Maryland\n                                                                                           Masovian\n                                                                                      Massachusetts\n                                                                             Mecklenburg-Vorpommern\n                                                                                           Michigan\n                                                                                          Minnesota\n                                                                                        Mississippi\n                                                                                           Missouri\n                                                                                            Montana\n                                                                                            Navarre\n                                                                                           Nebraska\n                                                                                             Nevada\n                                                                                      New Brunswick\n                                                                                      New Hampshire\n                                                                                         New Jersey\n                                                                                         New Mexico\n                                                                                           New York\n                                                                                      North Brabant\n                                                                                     North Carolina\n                                                                                       North Dakota\n                                                                                      North Holland\n                                                                                      North Karelia\n                                                                             North Rhine-Westphalia\n                                                                                   Northern Savonia\n                                                                                        Nova Scotia\n                                                                                               Ohio\n                                                                                           Oklahoma\n                                                                                            Ontario\n                                                                                              Opole\n                                                                                             Oregon\n                                                                                              Other\n                                                                                         Overijssel\n                                                                                  Paijanne Tavastia\n                                                                                       Pennsylvania\n                                                                                           Piedmont\n                                                                                          Pirkanmaa\n                                                                                          Podlaskie\n                                                                                         Pomeranian\n                                                                               Prince Edward Island\n                                                                                             Quebec\n                                                                               Rhineland-Palatinate\n                                                                                       Rhode Island\n                                                                                           Saarland\n                                                                                           Sardinia\n                                                                                       Saskatchewan\n                                                                                          Satakunta\n                                                                                             Saxony\n                                                                                      Saxony-Anhalt\n                                                                                           Silesian\n                                                                                     South Carolina\n                                                                                       South Dakota\n                                                                                      South Holland\n                                                                                   Southern Savonia\n                                                                                             Styria\n                                                                                      Subcarpathian\n                                                                                    Tavastia Proper\n                                                                                          Tennessee\n                                                                                              Texas\n                                                                                          Thuringia\n                                                                                              Tirol\n                                                                                            Tuscany\n                                                                                             Umbria\n                                                                                      Upper Austria\n                                                                                               Utah\n                                                                                            Utrecht\n                                                                                            Uusimaa\n                                                                                Valencian Community\n                                                                                             Veneto\n                                                                                            Vermont\n                                                                                             Vienna\n                                                                                           Virginia\n                                                                                         Vorarlberg\n                                                                                         Washington\n                                                                                      West Virginia\n                                                                                          Wisconsin\n                                                                                            Wyoming\n                                                                                            Zeeland\n    n      percent\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n    1 3.563157e-05\n   75 2.672368e-03\n   61 2.173526e-03\n  300 1.068947e-02\n  232 8.266524e-03\n   12 4.275788e-04\n   12 4.275788e-04\n  131 4.667736e-03\n  129 4.596472e-03\n    1 3.563157e-05\n    2 7.126314e-05\n  144 5.130946e-03\n    1 3.563157e-05\n   12 4.275788e-04\n  657 2.340994e-02\n  183 6.520577e-03\n    5 1.781578e-04\n 1211 4.314983e-02\n   36 1.282737e-03\n   23 8.195261e-04\n   46 1.639052e-03\n   51 1.817210e-03\n  604 2.152147e-02\n  459 1.635489e-02\n    5 1.781578e-04\n   31 1.104579e-03\n  142 5.059683e-03\n  231 8.230893e-03\n   39 1.389631e-03\n    1 3.563157e-05\n    6 2.137894e-04\n    1 3.563157e-05\n    8 2.850526e-04\n   29 1.033316e-03\n    2 7.126314e-05\n    8 2.850526e-04\n    3 1.068947e-04\n   57 2.030999e-03\n   42 1.496526e-03\n  192 6.841261e-03\n  674 2.401568e-02\n  599 2.134331e-02\n   78 2.779262e-03\n  345 1.229289e-02\n   12 4.275788e-04\n    8 2.850526e-04\n    1 3.563157e-05\n    4 1.425263e-04\n   15 5.344735e-04\n   44 1.567789e-03\n  166 5.914841e-03\n   25 8.907892e-04\n  382 1.361126e-02\n   11 3.919473e-04\n    7 2.494210e-04\n    2 7.126314e-05\n  332 1.182968e-02\n  262 9.335471e-03\n    7 2.494210e-04\n    1 3.563157e-05\n  415 1.478710e-02\n    8 2.850526e-04\n 1128 4.019241e-02\n  708 2.522715e-02\n    3 1.068947e-04\n  165 5.879209e-03\n   35 1.247105e-03\n   75 2.672368e-03\n   69 2.458578e-03\n    7 2.494210e-04\n  211 7.518261e-03\n  219 7.803314e-03\n   22 7.838945e-04\n    3 1.068947e-04\n  802 2.857652e-02\n   15 5.344735e-04\n  427 1.521468e-02\n   42 1.496526e-03\n   16 5.701051e-04\n   34 1.211473e-03\n  302 1.076073e-02\n    2 7.126314e-05\n 1049 3.737752e-02\n 1190 4.240157e-02\n   61 2.173526e-03\n 2021 7.201140e-02\n    5 1.781578e-04\n  873 3.110636e-02\n 2242 7.988598e-02\n   25 8.907892e-04\n    9 3.206841e-04\n  992 3.534652e-02\n   69 2.458578e-03\n    1 3.563157e-05\n    4 1.425263e-04\n    2 7.126314e-05\n   24 8.551577e-04\n  457 1.628363e-02\n  137 4.881525e-03\n  308 1.097452e-02\n   15 5.344735e-04\n    1 3.563157e-05\n   15 5.344735e-04\n    1 3.563157e-05\n  173 6.164262e-03\n   99 3.527525e-03\n   21 7.482630e-04\n    9 3.206841e-04\n  201 7.161945e-03\n   61 2.173526e-03\n    2 7.126314e-05\n   26 9.264208e-04\n    6 2.137894e-04\n    6 2.137894e-04\n  251 8.943524e-03\n   10 3.563157e-04\n  175 6.235525e-03\n    1 3.563157e-05\n  144 5.130946e-03\n   16 5.701051e-04\n   36 1.282737e-03\n  518 1.845715e-02\n   23 8.195261e-04\n   53 1.888473e-03\n    9 3.206841e-04\n   11 3.919473e-04\n  381 1.357563e-02\n   56 1.995368e-03\n  146 5.202209e-03\n    4 1.425263e-04\n 1118 3.983609e-02\n   89 3.171210e-03\n 1910 6.805630e-02\n   85 3.028683e-03\n    3 1.068947e-04\n\n\nLet’s filter out all of those entries that have damaged as state_prov. This should simplify our summaries.\n\n\n [1] Wisconsin    California   Ohio         Michigan     Washington  \n [6] Pennsylvania Oregon       New York     Minnesota    Indiana     \n134 Levels: Alabama Alaska Alberta Antwerp Aragon Arizona ... Zeeland\n\n\nLet’s visualize the top 10 states by median pumpkin weight in these contests. California has the highest median pumpkin weight of these states.\n\n\nWarning: Removed 2633 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nLet’s separate the pumpkins out by type, and compare the distributions over the years for United States, United Kingdom and Canada. You can click on the legend to remove or show the different countries.\n\n\nWarning: Removed 4480 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{laderas,\n  author = {Ted Laderas and Ted Laderas},\n  title = {Pumpkins, {Pumpkins,} {Pumpkins}},\n  date = {},\n  url = {https://laderast.github.io//2021-10-19-great-pumpkins.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, and Ted Laderas. n.d. “Pumpkins, Pumpkins,\nPumpkins.” https://laderast.github.io//2021-10-19-great-pumpkins.html."
  },
  {
    "objectID": "articles/2018-05-24-group-by-summarize-not-just-for-numeric-values/2018-05-24-group-by-summarize-not-just-for-numeric-values.html",
    "href": "articles/2018-05-24-group-by-summarize-not-just-for-numeric-values/2018-05-24-group-by-summarize-not-just-for-numeric-values.html",
    "title": "group_by/summarize: Not just for numeric values!",
    "section": "",
    "text": "Even though I’ve been using the tidyverse for a couple of years, there’s always a couple new applications of tidyverse verbs.\nThis one, in retrospect, is pretty simple. I had a one to many table that I wanted to collapse, tidy-style. Let’s look at the diamonds dataset:\n\ndiamonds %>% select(color, cut) %>%\n  head() %>%\n  knitr::kable()\n\n\n\n\ncolor\ncut\n\n\n\n\nE\nIdeal\n\n\nE\nPremium\n\n\nE\nGood\n\n\nI\nPremium\n\n\nJ\nGood\n\n\nJ\nVery Good\n\n\n\n\n\nWhat if we wanted to collapse all the entries for each color into a single line? There’s 7 different colors, so we can use a combination of group_by on color and use the paste() function within summarize() to get what we want, which I’ve called all_colors here. By specifying the collapse argument, we can specify the delimiter within that column:\n\ndiamonds %>% select(color, cut) %>% \n  group_by(color) %>% \n  summarize(all_colors=\n              paste(cut, collapse=\";\"))\n\n# A tibble: 7 × 2\n  color all_colors                                                              \n  <ord> <chr>                                                                   \n1 D     Very Good;Very Good;Very Good;Good;Good;Premium;Premium;Ideal;Ideal;Ver…\n2 E     Ideal;Premium;Good;Fair;Premium;Premium;Very Good;Very Good;Very Good;G…\n3 F     Premium;Very Good;Very Good;Very Good;Good;Premium;Very Good;Very Good;…\n4 G     Very Good;Ideal;Ideal;Very Good;Premium;Premium;Ideal;Very Good;Ideal;P…\n5 H     Very Good;Very Good;Very Good;Good;Good;Very Good;Good;Very Good;Very G…\n6 I     Premium;Very Good;Ideal;Good;Premium;Ideal;Ideal;Ideal;Ideal;Very Good;…\n7 J     Good;Very Good;Good;Ideal;Ideal;Good;Good;Very Good;Very Good;Very Good…\n\n\nThanks to Ken Butler, who pointed out that the tidyverse way (via stringr) is to use str_c instead:\n\ndiamonds %>% select(color, cut) %>% \n  group_by(color) %>% \n  summarize(all_colors=\n              stringr::str_c(cut, collapse=\";\")) \n\n# A tibble: 7 × 2\n  color all_colors                                                              \n  <ord> <chr>                                                                   \n1 D     Very Good;Very Good;Very Good;Good;Good;Premium;Premium;Ideal;Ideal;Ver…\n2 E     Ideal;Premium;Good;Fair;Premium;Premium;Very Good;Very Good;Very Good;G…\n3 F     Premium;Very Good;Very Good;Very Good;Good;Premium;Very Good;Very Good;…\n4 G     Very Good;Ideal;Ideal;Very Good;Premium;Premium;Ideal;Very Good;Ideal;P…\n5 H     Very Good;Very Good;Very Good;Good;Good;Very Good;Good;Very Good;Very G…\n6 I     Premium;Very Good;Ideal;Good;Premium;Ideal;Ideal;Ideal;Ideal;Very Good;…\n7 J     Good;Very Good;Good;Ideal;Ideal;Good;Good;Very Good;Very Good;Very Good…\n\n\nFinally, if we wanted to just get the unique values of the cuts in a single line, we can use unique:\n\ndiamonds %>% select(color, cut) %>% \n  group_by(color) %>% \n  summarize(all_colors=\n              paste(unique(cut), collapse=\";\")) \n\n# A tibble: 7 × 2\n  color all_colors                       \n  <ord> <chr>                            \n1 D     Very Good;Good;Premium;Ideal;Fair\n2 E     Ideal;Premium;Good;Fair;Very Good\n3 F     Premium;Very Good;Good;Fair;Ideal\n4 G     Very Good;Ideal;Premium;Good;Fair\n5 H     Very Good;Good;Premium;Fair;Ideal\n6 I     Premium;Very Good;Ideal;Good;Fair\n7 J     Good;Very Good;Ideal;Premium;Fair\n\n\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {Group\\_by/Summarize: {Not} Just for Numeric Values!},\n  date = {2018-05-24},\n  url = {https://laderast.github.io//2018-05-24-group-by-summarize-not-just-for-numeric-values.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “Group_by/Summarize: Not Just for Numeric\nValues!” May 24, 2018. https://laderast.github.io//2018-05-24-group-by-summarize-not-just-for-numeric-values.html."
  },
  {
    "objectID": "articles/2020-05-05_animal-crossing/animal-crossing.html",
    "href": "articles/2020-05-05_animal-crossing/animal-crossing.html",
    "title": "Animal Crossing",
    "section": "",
    "text": "Load your dataset in with the function below. The input is the date the dataset was issued. You should be able to get this from the tt_available() function.\n\ncritic <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv')\n\nRows: 107 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): publication, text\ndbl  (1): grade\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nuser_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')\n\nRows: 2999 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): user_name, text\ndbl  (1): grade\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nitems <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/items.csv')\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 4565 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): id, name, category, sell_currency, buy_currency, sources, recipe_i...\ndbl  (4): num_id, sell_value, buy_value, recipe\nlgl  (2): orderable, customizable\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvillagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')\n\nRows: 391 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): id, name, gender, species, birthday, personality, song, phrase, fu...\ndbl  (1): row_n\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "articles/2020-05-05_animal-crossing/animal-crossing.html#personalities-by-species",
    "href": "articles/2020-05-05_animal-crossing/animal-crossing.html#personalities-by-species",
    "title": "Animal Crossing",
    "section": "Personalities by Species",
    "text": "Personalities by Species\n\nspecies_count <- villagers %>%\n  group_by(species)  %>%\n  summarize(species_count = n()) %>%\n  arrange(species_count)\n\ndatatable(species_count)\n\n\n\n\n\nlevel_order <- villagers %>%\n  group_by(species) %>% count() %>%\n  arrange(desc(n)) %>%\n  pull(species)\n\nvillagers %>%\n  mutate(species=factor(species, levels=level_order)) %>%\n  ggplot() + aes(x=species, y=personality, color=personality) %>%\n  geom_count() + \n   theme_light() + theme(legend.position = \"none\") +\n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\nvillagers %>% select(name, species, personality, url) %>%\n  mutate(combo = paste(species, personality)) %>% select(name, combo, url) -> villager_index\n\nunique_combos <- villagers %>%\n  group_by(species, personality) %>% summarize(n=n()) %>%\n  filter(n == 1) %>% mutate(combo=paste(species, personality)) %>%\n  inner_join(y=villager_index, by=c(\"combo\")) %>% ungroup()\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nout_image <- unique_combos %>%\n  ggplot() + aes(x=species, y=personality, image=url, name=name) +\n  geom_count() +\n  geom_raster(fill=\"white\", color=\"black\") +\n  geom_image(asp=1.2, size=0.03) + \n   theme_minimal() + theme(legend.position = \"none\") +\n  theme(axis.text.x = element_text(angle = 90)) + labs(title=\"There can be only one\", subtitle = \"Unique Personality/Species combos in Animal Crossing\")\n\nWarning: Ignoring unknown parameters: colour\n\nout_image\n\n\n\nggsave(plot=out_image, filename = \"unique_animal_personalities.pdf\", width=10, height = 5)\n\n\npers_vil <- villagers %>% \n  group_by(personality, species) %>%\n  summarize(count=n()) %>%\n  #filter(count==1) %>%\n  arrange(species) \n\n`summarise()` has grouped output by 'personality'. You can override using the\n`.groups` argument.\n\npers_vil %>%\n  arrange(desc(count)) %>%\n  datatable()"
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "",
    "text": "As part of my new year’s resolution to learn new things about R, I’m trying to plug some holes in my R knowledge by writing more vignettes to explain them to myself this year.\nThis week I finally think I understand more about namespaces in R and why you should use them in your R package."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#namespaces-why-bother",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#namespaces-why-bother",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Namespaces: Why Bother?",
    "text": "Namespaces: Why Bother?\nIn short, we need namespaces because of the ambiguity of function names. Think of how many packages have a filter() function! How does R know which function from which package you’re talking about? One nightmare case is the Bioconductor exprs() method and the rlang exprs() function. These functions do very, very different things (one of them extracts an expression matrix from a Bioconductor ExpressionSet, and the other one is used for quoting multiple expressions).\nWhat happens when you call library on both of these packages? The worst case scenario is that you mean to call one package function and R executes the other one. This is called a namespace collision, and unfortunately it can break your code.\nEnter the namespace, which allows us to be package and function specific. If I want to use the function filter from dplyr, I write it as dplyr::filter. The dplyr:: part is the namespace of the dplyr package."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#description-the-gatekeeper-for-calling-packages",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#description-the-gatekeeper-for-calling-packages",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "DESCRIPTION: The gatekeeper for calling packages",
    "text": "DESCRIPTION: The gatekeeper for calling packages\nThe DESCRIPTION file is one of the gatekeepers in your package. Ever wonder how install.packages knows how to install the packages your package depends on? It’s actually because of a field in the DESCRIPTION file. Here’s the DESCRIPTION file for my burro package:\nPackage: burro\nType: Package\nTitle: Shiny App Package for setting up a data exploration session (\"burro\"w into the data)\nVersion: 0.1.0\nAuthors@R: as.person(c(\n    \"Ted Laderas <tedladeras@gmail.com> [aut, cre]\",\n    \"Jessica Minnier <minnier@ohsu.edu> [ctb]\",\n    \"Gabrielle Choonoo <choonoo@ohsu.edu> [ctb]\"\n  ))\nMaintainer: Ted Laderas <ted.laderas@gmail.com>\nDescription: Allows the teacher to deploy a simple data exploration app for exploring a dataset (mostly for teaching purposes).\nLicense: MIT LICENSE\nEncoding: UTF-8\nLazyData: true\nImports:\n    dplyr,\n    shinydashboard,\n    ggplot2,\n    visdat,\n    skimr,\n    naniar,\n    data.table,\n    magrittr,\n    glue,\n    usethis,\n    here,\n    viridis,\n    DT\nDepends:\n    shiny\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 6.1.0\nSuggests:\n    testthat\nLook at the Imports: field. You can see a list of all of the packages that burro utilizes. In ye olde days of R, we used the Depends: field. Nowadays we use the Imports: fields. Here’s a Stack Overflow post explaining why. The main reason is that Imports: requires the package to have a namespace.\nModifying the DESCRIPTION file by hand is possible, but I don’t recommend it. Instead, you can use the usethis package to modify it. For example, if I want to use dplyr in my package I can do this in the console, while I am building it.\nusethis::use_package(\"dplyr\")\nThis will add dplyr to the Imports: field of your DESCRIPTION file.\nAddition (Thanks Hao Ye, for the suggestion): If the function is in a development version (i.e., hosted on GitHub), you can use usethis::use_dev_package() to add it to your DESCRIPTION file. It will add an additional field called Remotes: to your package:\n\nusethis::use_dev_package(\"tidyverse/dplyr\")\n\nFor more info about using remotes, check out the vignette: https://remotes.r-lib.org/articles/dependencies.html"
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#using-namespaces-to-call-functions-from-other-packages",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#using-namespaces-to-call-functions-from-other-packages",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Using namespaces to call functions from other packages",
    "text": "Using namespaces to call functions from other packages\nNow that we have specified the package in our DESCRIPTION file, we can now call any function in dplyr by adding a dplyr:: before the function. So if we wanted to call mutate() in our package function we can do this:\nmutate_iris <- function(iris){\n    dplyr::mutate(iris, sepal_sum = Sepal.Length + Sepal.Width)\n}\nIn many cases, calling a function by specifying its namespace is good practice. For one, there are many functions called filter(): I can think of at least the ones that are in base and dplyr. Using the namespace makes it unambiguous to both R and other developers which filter() function you’re talking about."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#how-do-i-call-functions-from-other-packages-using-roxygen-docstrings",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#how-do-i-call-functions-from-other-packages-using-roxygen-docstrings",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "How do I call functions from other packages? Using roxygen docstrings",
    "text": "How do I call functions from other packages? Using roxygen docstrings\nUgh, I’ve already written a bunch of code and I don’t want to add the namespaces before all of the functions from other packages! How can I avoid this?\nThis is where roxygen and devtools::document() come in.\nroxygen docstrings are responsible for at least three things in your package: 1) producing the documentation (.Rd) files, but also: 2) specifying what package namespaces you want to utilize, or import in your function, and 3) whether you want to export that function (i.e., make it accessible publicly).\nHow do they accomplish 2)? When you call devtools::document() to build the documentation, they scan for multiple fields, such as @import and @importFrom in the roxygen doc strings. Then devtools::document() actually modifies the NAMESPACE file in your package."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#using-importfrom-when-you-only-need-one-function",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#using-importfrom-when-you-only-need-one-function",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Using @importFrom: When you only need one function",
    "text": "Using @importFrom: When you only need one function\nSay you just wanted to use filter from dplyr, but didn’t want to write dplyr::filter before all of your functions. You can just import the filter() function by including the following docstring:\n#' @importFrom dplyr filter\nAnd then you can just use filter() like normal in your code:\n#' @importFrom dplyr filter               #This is where you add the @importFrom\nuse_filter <- function(df, cutoff=0.5) {\n\n  filter(df, value < cutoff)\n\n}"
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#using-import-when-you-need-a-lot-of-functions-from-a-package",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#using-import-when-you-need-a-lot-of-functions-from-a-package",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Using @import: When you need a lot of functions from a package",
    "text": "Using @import: When you need a lot of functions from a package\nWhat if you had a lot of functions from one package, such as shiny, that you want to use? Do you need to add an @importFrom for each of these functions? Nope. You can just use one @import field for the whole package:\n#` @import shiny\nAnd then code like usual:\n#` @import shiny            # This is where you add the @import\nshinyUI <- function() {\n    selectInput(\"\")\n}"
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#importing-multiple-packages-just-dont-do-it-updated.",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#importing-multiple-packages-just-dont-do-it-updated.",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Importing multiple packages: just don’t do it (UPDATED).",
    "text": "Importing multiple packages: just don’t do it (UPDATED).\nJust a note to not import multiple packages using @import. As Hadley Wickham has noted, you have no control over the development of the packages you import. Just because there are no function collisions right now between the packages doesn’t mean that one of the developers may add a function down the line that might collide. So, if you need to use multiple packages, @import one package and use @importsFrom or namespaces to refer to functions in the other packages."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#exported-versus-internal-functions",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#exported-versus-internal-functions",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Exported versus Internal functions",
    "text": "Exported versus Internal functions\nRemember I mentioned ‘@export’ above? Specifying in a docstring for your function exports it. Specifying it means that ‘devtools::document()’ will add an Export directive for the function in the NAMESPACE file. That means you can access it using ‘::’. Going back to our use_filter example:\n#' @export                              #This is where you add the @export\n#' @importFrom dplyr filter\nuse_filter <- function(df, cutoff=0.5) {\n\n  filter(df, value < cutoff)\n\n}\nIf our package name is mypackage, then this function will be accessible if we use library(mypackage) or mypackage::use_filter().\nWhy is this important? You may write some internal functions that are useful in your package, but they aren’t necessarily ones you want your users to use in their daily use. @export allows you to control which functions you make publicly accessible in your package.\nFor example, try typing dplyr:: in RStudio and hit the tab key. You’ll see the usual dplyr verbs pop up. But these are only the exported functions. Now try dplyr::: and hit the tab key. You’ll see a list of functions that pop up that’s much longer - these are all the functions, including the internal functions.\nSo, if there’s a cool bit of internal code you want to use in a package, you can use ‘:::’ to specify it. Just be aware that oftentimes, internal functions may change a lot as code gets refactored, so code that utilizes them may be refactored."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#remember-to-run-devtoolsdocument",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#remember-to-run-devtoolsdocument",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Remember to run devtools::document()",
    "text": "Remember to run devtools::document()\nOnce you’ve written code and want to test it in your package, remember to run devtools::document() before you reinstall your package for testing. Otherwise, the NAMESPACE file won’t be modified, and your code won’t work.\nYou can also modify your Project Options for your package and check the Tools >> Project Options >> Build Tools >> Generate Documentation with Roxygen box. Next to it, there is also a Configure button that lets you select the option to run devtools::document() whenever you build a package for testing."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#go-forth-and-package",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#go-forth-and-package",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Go forth and Package!",
    "text": "Go forth and Package!\n\nTL;DR:\n\nUse usethis::use_package() to add packages to your DESCRIPTION file (while in the console) after you’ve created your package skeleton using use_package().\nUse namespaces (such as mypackage::usefilter()) to refer to external functions where possible to avoid collisions in function names when you are writing code.\n\nUse @imports and @importsFrom judiciously in your roxygen documentation for a function if you need to use many extenral package functions within a function. Use only 1 @import statement in a function, use namespaces/@importsFrom for the other packages.\nExpose functions you want to be made public using @export in your roxygen documentation.\nRemember to run devtools::document() in the console after you modify/add these fields to the roxygen documentation when you build for testing, or set it up in Tools >> Project Options.\n\nI hope this helps you to understand exactly the relationships between all of the components that are responsible for accessing namespaces in your package. I was confused about this for years, so writing this has helped me understand these relationships."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#further-reading",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#further-reading",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Further Reading",
    "text": "Further Reading\n\nIf you want to know how R searches for a function across its environments and why namespaces are a good thing, this is an excellent writeup: http://blog.obeautifulcode.com/R/How-R-Searches-And-Finds-Stuff/\nMore about the usethis workflow: https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/\nMore about the DESCRIPTION file: http://r-pkgs.had.co.nz/description.html\nMore about namespaces: http://r-pkgs.had.co.nz/namespace.html\nMore about roxygen: http://r-pkgs.had.co.nz/man.html#roxygen-comments\nAn alternative workflow to modify the DESCRIPTION file: You can use attachment::att_to_description() to scan code and add packages to the file after coding (thanks, Sébastien!)."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#acknowledgements",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#acknowledgements",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Hao Ye, Sébastien Rochette, Michael Chirico, Tim Hesterberg, and Hadley Wickham for their comments and questions. I’ve incorporated your suggestions."
  },
  {
    "objectID": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#note",
    "href": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together.html#note",
    "title": "Package Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together",
    "section": "Note",
    "text": "Note\nI want this to be clear and correct. Please email me if there are any mistakes I’ve made."
  },
  {
    "objectID": "articles/2021-10-12-seafood/2021-10-12-seafood.html",
    "href": "articles/2021-10-12-seafood/2021-10-12-seafood.html",
    "title": "Tidy Tuesday: Seafood Production and Consumption",
    "section": "",
    "text": "For Tidy Tuesday, this week I decided to tackle a relatively easy question this week by understanding seafood production over the years. Since I am a big octopus fan. Which countries were responsible for the top production of cephalopods over the years?"
  },
  {
    "objectID": "articles/2021-10-12-seafood/2021-10-12-seafood.html#loading-the-data",
    "href": "articles/2021-10-12-seafood/2021-10-12-seafood.html#loading-the-data",
    "title": "Tidy Tuesday: Seafood Production and Consumption",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nfarmed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/aquaculture-farmed-fish-production.csv')\n\nRows: 11657 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (2): Year, Aquaculture production (metric tons)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nconsumption <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/fish-and-seafood-consumption-per-capita.csv')\n\nRows: 11028 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (2): Year, Fish, Seafood- Food supply quantity (kg/capita/yr) (FAO, 2020)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproduction <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/seafood-and-fish-production-thousand-tonnes.csv')\n\nRows: 10326 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (8): Year, Commodity Balances - Livestock and Fish Primary Equivalent - ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nproduction <- janitor::clean_names(production)\nskimr::skim(production)\n\n\nData summary\n\n\nName\nproduction\n\n\nNumber of rows\n10326\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nentity\n0\n1.00\n4\n39\n0\n215\n0\n\n\ncode\n1734\n0.83\n3\n8\n0\n181\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1987.70\n15.35\n1961\n1974.0\n1988.0\n2001.00\n2013\n▇▆▇▇▇\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_pelagic_fish_2763_production_5510_tonnes\n1586\n0.85\n877685.06\n3244678.56\n0\n1300.0\n35667.0\n330158.00\n43756110\n▇▁▁▁▁\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_crustaceans_2765_production_5510_tonnes\n1146\n0.89\n128038.08\n680253.04\n0\n97.0\n3076.0\n29500.00\n12607540\n▇▁▁▁▁\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_cephalopods_2766_production_5510_tonnes\n2836\n0.73\n67800.81\n291850.11\n0\n0.0\n546.0\n12795.00\n4285298\n▇▁▁▁▁\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_demersal_fish_2762_production_5510_tonnes\n1822\n0.82\n498916.69\n1831355.82\n0\n799.5\n19591.5\n191905.50\n22261372\n▇▁▁▁▁\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_freshwater_fish_2761_production_5510_tonnes\n527\n0.95\n465803.05\n2569244.83\n0\n681.5\n10600.0\n78768.50\n52335573\n▇▁▁▁▁\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_molluscs_other_2767_production_5510_tonnes\n2860\n0.72\n238475.03\n1343302.70\n0\n5.0\n1394.0\n37754.25\n17952945\n▇▁▁▁▁\n\n\ncommodity_balances_livestock_and_fish_primary_equivalent_marine_fish_other_2764_production_5510_tonnes\n1670\n0.84\n214545.61\n935412.65\n0\n939.0\n5700.0\n45368.50\n10865669\n▇▁▁▁▁\n\n\n\n\ncolnames(production) <- stringr::str_replace(colnames(production), \"commodity_balances_livestock_and_fish_primary_equivalent_\",replacement = \"\")\n\ncolnames(production) <- stringr::str_replace(colnames(production), \"_production_5510_tonnes\", replacement=\"\")\n\nskimr::skim(production)\n\n\nData summary\n\n\nName\nproduction\n\n\nNumber of rows\n10326\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nentity\n0\n1.00\n4\n39\n0\n215\n0\n\n\ncode\n1734\n0.83\n3\n8\n0\n181\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1987.70\n15.35\n1961\n1974.0\n1988.0\n2001.00\n2013\n▇▆▇▇▇\n\n\npelagic_fish_2763\n1586\n0.85\n877685.06\n3244678.56\n0\n1300.0\n35667.0\n330158.00\n43756110\n▇▁▁▁▁\n\n\ncrustaceans_2765\n1146\n0.89\n128038.08\n680253.04\n0\n97.0\n3076.0\n29500.00\n12607540\n▇▁▁▁▁\n\n\ncephalopods_2766\n2836\n0.73\n67800.81\n291850.11\n0\n0.0\n546.0\n12795.00\n4285298\n▇▁▁▁▁\n\n\ndemersal_fish_2762\n1822\n0.82\n498916.69\n1831355.82\n0\n799.5\n19591.5\n191905.50\n22261372\n▇▁▁▁▁\n\n\nfreshwater_fish_2761\n527\n0.95\n465803.05\n2569244.83\n0\n681.5\n10600.0\n78768.50\n52335573\n▇▁▁▁▁\n\n\nmolluscs_other_2767\n2860\n0.72\n238475.03\n1343302.70\n0\n5.0\n1394.0\n37754.25\n17952945\n▇▁▁▁▁\n\n\nmarine_fish_other_2764\n1670\n0.84\n214545.61\n935412.65\n0\n939.0\n5700.0\n45368.50\n10865669\n▇▁▁▁▁\n\n\n\n\n\nI’ll pivot the production data frame to a longer one using pivot_longer().\n\nproduction_long <- production %>% tidyr::pivot_longer(cols=contains(\"_\"), names_to = \"seafood_type\",values_to = \"production\")\n\nhead(production_long)\n\n# A tibble: 6 × 5\n  entity      code   year seafood_type         production\n  <chr>       <chr> <dbl> <chr>                     <dbl>\n1 Afghanistan AFG    1961 pelagic_fish_2763            NA\n2 Afghanistan AFG    1961 crustaceans_2765             NA\n3 Afghanistan AFG    1961 cephalopods_2766             NA\n4 Afghanistan AFG    1961 demersal_fish_2762           NA\n5 Afghanistan AFG    1961 freshwater_fish_2761        300\n6 Afghanistan AFG    1961 molluscs_other_2767          NA\n\n\nNow we have the long form data frame, we can now ask some interesting time questions and compare across categories. As you can see below, total seafood production has risen steadily over the years.\n\nproduction_long %>%\n  group_by(year, seafood_type) %>%\n  summarize(production = mean(production, na.rm=TRUE)) %>%\n  ggplot() + \n  aes(x=year, y=production, fill=seafood_type) +\n  geom_area() + \n  viridis::scale_fill_viridis(discrete=TRUE) +\n  hrbrthemes::theme_ipsum() + \n  ggtitle(\"Production of Seafood has Risen Steadily over the Years\")\n\n`summarise()` has grouped output by 'year'. You can override using the `.groups`\nargument.\n\n\n\n\n\nDrilling into the cephalpods, I’m interested in percent production of the total for the top 10 producing countries.\nInteresting that Japan’s share of production has decreased steadily, and that China is a leading producer lately.\n\nproduction_long <- production %>% \n  tidyr::pivot_longer(cols=contains(\"_\"), \n  names_to = \"seafood_type\",values_to = \"production\")\n\ntop_squid_eaters <- production_long %>%\n  filter(seafood_type == \"cephalopods_2766\") %>%\n  filter(code != \"OWID_WRL\") %>%\n  group_by(code) %>%\n  summarize(total_eating = sum(production)) %>%\n  arrange(desc(total_eating)) %>%\n  slice(1:10) %>%\n  pull(code)\n\ntotal_production <- production_long %>%\n  filter(seafood_type == \"cephalopods_2766\") %>%\n  filter(code != \"OWID_WRL\") %>%\n  filter(code %in% top_squid_eaters) %>%\n  group_by(year) %>%\n  summarize(total_eating = sum(production, na.rm=TRUE)) \n\ntotal_ceph <- production_long %>%\n  filter(seafood_type == \"cephalopods_2766\") %>%\n  filter(code %in% top_squid_eaters) %>%\n  group_by(year, code) %>%\n  summarize(production = mean(production, na.rm=TRUE), entity) %>%\n  left_join(y=total_production, by=\"year\") %>%\n  mutate(percent = production/total_eating * 100) %>%\n  ggplot() + \n  aes(x=year, y=percent, fill=entity) +\n  geom_area() + \n  viridis::scale_fill_viridis(discrete=TRUE, option=\"plasma\") +\n  hrbrthemes::theme_ipsum() \n\n`summarise()` has grouped output by 'year'. You can override using the `.groups`\nargument.\n\ntotal_ceph\n\n\n\n\n\ntotal_ceph + annotate(geom=\"text\", x= 1969, y=60, label = \"Japan\", colour=\"lightgrey\", size=4) +\n  annotate(geom=\"text\", x=2005, y=80, label=\"China\", colour=\"lightgrey\", size=4) + \n  annotate(geom=\"text\", x=1991, y=44, label=\"South Korea\", color=\"lightgrey\", size=4) +\n    annotate(geom=\"text\", x=2008, y=40, label=\"Peru\", color=\"lightgrey\", size=4) +\n  labs(title=\"Top 10 cephalopod producers\", subtitle = \"Japan, South Korea, Peru, and China compete for top market share\") + scale_x_continuous(breaks = c(1960, 1970, 1980, 1990, 2000, 2010))\n\n\n\nggsave(\"top_mollusk_production.jpg\")\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "articles/2018-01-05-so-you-ve-accidentally-checked-in-a-large-file-into-git/index.html",
    "href": "articles/2018-01-05-so-you-ve-accidentally-checked-in-a-large-file-into-git/index.html",
    "title": "So You’ve Accidentally Checked in a Large File Into Git",
    "section": "",
    "text": "Note: after posting this, I heard back from Roberto Tyley, the creator of the BFG. I’d like to note that the BFG actually does its job really well. I was mostly really frustrated about how Git/GitHub doesn’t prevent a user from doing something that’s hard to undo. So my frustration is really about that, not really about the BFG. This post has been edited to reflect that.\nGreg Wilson first said it, but I’ve come to agree. Git is an aggressively antisocial piece of software. Git is a piece of software that can make developers with any amount of experience feel dumb.\nRecently, I accidentally checked a large file (greater than 100 Megs) into my local repo. When I tried to push to GitHub, of course, it refused it (I know about git large file storage, but I don’t have any).\nSo my local repo was screwed up. Of course, I did what seemed like the rational thing and deleted the file from my repo and recommitted. More than once. This is a mistake I’ve done more than once. So you need to scrub your git history with BFG so that GitHub will accept your lowly commits again.\nThe BFG documentation specifies how to fix a remote repo. I would say that this situation is much less common than the local situation. So I just decided to share how I got the BFG to work for the local repo situation.\nI usually install the BFG through homebrew, using brew install bfg. When you install it this way, you can just run BFG with bfg. You can download it from the website, but you’ll have to call java -jar bfg[VERSION].jar to run it.\nSay you’ve accidently checked in a large file into your current repo. The first thing to do is to clone your local repo:\ngit clone --mirror local_repo\nThis will create another folder called local_repo.git that you will do all the BFG magic on. This local_rep.git is what is called a bare repo. I want to remove any files larger than 100 Megs, so I do this:\nbfg -b 100M local_repo.git\nIf this doesn’t return an error, you can move on. However, I got the dreaded error:\nWarning : no large blobs matching criteria found in packfiles - \ndoes the repo need to be packed?\nAugh. Ok, some googling later I found that I needed to pack my orignal repo:\ncd local_repo\ngit repack\nOkay, we need to get rid of our cloned repo and redo the last few steps.\nrm -rf local_repo.git\ngit clone --mirror local_repo\nbfg -b 100M local_repo.git\nThen comes some git commands that no one has bothered to explain to me (UPDATE: I forgot to add changing directories into local_repo.git - sorry about that!).\ncd local_repo.git\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive\ngit push\nUh oh, I get a remote: error: refusing to update checked out branch: refs/heads/master error! More ugh.\nHere’s the trick. Since you cloned a local repo, you need to set the origin of your current repo (local_repo.git) to the GitHub remote. Still in our local_repo.git directory, first we remove the current origin, and then add back our remote.\ngit remote rm origin\ngit remote add origin https://github.com/laderast/remote_repo\nFinally, after much gnashing of the teeth, we can\ngit push\nDon’t forget to remove your now dirty local_repo, and the mirrored copy, and then pull a fresh copy down!\n##remove both original local repo and altered bare repo\nrm -rf local_repo\nrm -rf local_repo.git\n##clone a fresh copy from GitHub\ngit clone https://github.com/laderast/remote_repo\nThis may be obvious to the 10 people in the world who have read all of the git documentation, but I am not one of them. I’m stuck with git, unfortunately. I’m writing this post to remind me of what to do when I innocently do something like commit a large file to my repo.\n\n\n\nCitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {So {You’ve} {Accidentally} {Checked} in a {Large} {File}\n    {Into} {Git}},\n  date = {2018-01-05},\n  url = {https://laderast.github.io//articles/2018-01-05-so-you-ve-accidentally-checked-in-a-large-file-into-git},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “So You’ve Accidentally Checked in a Large File\nInto Git.” January 5, 2018. https://laderast.github.io//articles/2018-01-05-so-you-ve-accidentally-checked-in-a-large-file-into-git."
  },
  {
    "objectID": "articles/2018-08-07-shiny-and-tidyeval/2018-08-07-shiny-and-tidyeval.html",
    "href": "articles/2018-08-07-shiny-and-tidyeval/2018-08-07-shiny-and-tidyeval.html",
    "title": "Shiny and Tidyeval Part 1",
    "section": "",
    "text": "Note: sometimes I write these posts to teach myself a better way to do things in R.\nI have been avoiding tidyeval somewhat, because I seem to have a bit of a learning block about it. I’m going to try to write some posts that help me understand what’s going on with Tidy Evaluation."
  },
  {
    "objectID": "articles/2018-08-07-shiny-and-tidyeval/2018-08-07-shiny-and-tidyeval.html#using-sym-in-a-shiny-app",
    "href": "articles/2018-08-07-shiny-and-tidyeval/2018-08-07-shiny-and-tidyeval.html#using-sym-in-a-shiny-app",
    "title": "Shiny and Tidyeval Part 1",
    "section": "Using sym() in a Shiny App",
    "text": "Using sym() in a Shiny App\nOne fairly simple Shiny Application might be selecting a column of the dataset and then doing something with it, such as using it in a select() or filter() statement. Say we had a simple app to produce histograms, and we wanted to change the column that is being displayed on the histogram.\nTry this app out by running the following command. The code is here.\n\nrunGist(\"https://gist.github.com/laderast/a5205554324306e642b2df9f80ed6409\", display.mode=\"showcase\")\n\nOur input is a select input called numeric_var, which returns a single column name as a character In our server logic, we’ve built a reactive called selected_data, which returns the selected column as a vector using pull().\n\n  selected_data <- reactive({\n    ## input$numeric_var is a character, so we cast it to symbol\n    var_name <- sym(input$numeric_var)\n\n    ## Now we evaluate it with !!\n    out_col <- iris %>% pull(!!var_name)\n  })\n\nThe question is: how do we pass the input value into pull()? We first have to use rlang::sym() to pass our character in as a symbol that we’re calling var_name. But the issue is that our reactive doesn’t know which environment to look in.\nWe want our reactive to look for the column name within the environment of the iris tibble. This is where the !! (bang-bang) comes in. It says, ‘look for the value’ within the tibble."
  },
  {
    "objectID": "articles/2018-08-07-shiny-and-tidyeval/2018-08-07-shiny-and-tidyeval.html#using-syms-in-a-shiny-app",
    "href": "articles/2018-08-07-shiny-and-tidyeval/2018-08-07-shiny-and-tidyeval.html#using-syms-in-a-shiny-app",
    "title": "Shiny and Tidyeval Part 1",
    "section": "Using syms() in a Shiny App",
    "text": "Using syms() in a Shiny App\nWhat if wanted to pass in multiple variables from a select box? We’ll need to wrap our input with syms(), which takes a list.\nLet’s do a slightly different version where we’re visualizing a box plot and we want to select multiple columns to display in our dataset from a selectInput where we’ve specified the multiple=TRUE argument.\nOur setup is similar, but different. Because we have multiple values, we have to use syms() to wrap the input from input$numeric_vars. Then we can evaluate it with !!! (the triple bang).\n\n  selected_data <- reactive({\n    ## input$numeric_var is a character vector, so we cast it to symbol\n    var_list <- syms(input$numeric_vars)\n\n    ## Now we evaluate it with !!!\n    out_col <- iris %>% select(!!!var_list)\n  })\n\nTry this app out. The Code is here.\n\nrunGist(\"https://gist.github.com/laderast/952120ac46d1f27c2d2dba5bd1ab5d10\", display.mode=\"showcase\")"
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "",
    "text": "Full Disclosure: my department paid for the training and two of the certification exams. I did the Shiny exam for free with the stipulation that I would provide feedback on the exam itself.\nI recently became a certified RStudio Instructor in both Shiny and the Tidyverse. I thought I would write a little about the experience. I haven’t really had any formal pedagogical training, and having some of the state of the art and evidence-based practices were really helpful in extending my approaches to teaching."
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#instructor-training",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#instructor-training",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "Instructor Training",
    "text": "Instructor Training\nThe instructor training is online, but delivered via Zoom. The session themselves are 4 hours apiece and at set times, with a small cohort overall (I think mine was about 14). Greg Wilson delivered a short set of slides for each activity, and depending on the activity, we were sent out into smaller groups to discuss the topic. I liked how the course was paced - I felt like having activities almost every half hour definitely kept me awake.\nThe training for me (I am on PST) was from 8 AM to 12 PM two days in a row, and my cohort was from all over the US/Canada.\nHere are the parts of the instructor training that I found especially useful. Note that since Greg also helped develop the Carpentries instructor training, there is some overlap between the two training programs (in fact, if you’ve taught a Carpentries course and done the Carpentries training, you can waive the instructor training altogether).\nBuilding concept maps. If you can, you should map out the concepts you want to teach, along with how these concepts are related. This concept map is important in helping your students build the mental models that will guide them towards mastery. I did a concept map of Tidy data for my example. Concept maps themselves are a bit tricky to build, but I think they help clarify your understanding of what you are teaching, and thus your lessons will be more understandable.\nThinking about cognitive overload. This was a call to think about limiting the number of concepts and connections covered per unit. Humans have a short term memory of about 5 +/- 2 things at a time. This is an extremely helpful guideline to consider when developing training units, as your course objectives and training should keep these in mind. Also, there are different kinds of cognitive overload, and some of them (like extraneous details), you can remove to help with the other kinds. There are lots of ways to teach coding, such as Parsons Problems (where students have to put lines of code in order), or faded examples (fill in the blank style coding assignments), that can focus on particular aspects of coding. (As an unrelated topic, I have been thinking about reading code as being an essential skill and I try to teach students to learn how to read code.)\nTeaching needs to be dynamic. As instructors, part of our value lies in tailoring our training for our students and their previous knowledge. Unlike YouTube videos, we can assess what our students already know, and focus on the concepts that they don’t. In order to do this, though, we need to have ways of assessing what our students know. Formative assessments (assessments during a lesson) are essential in figuring out whether you can proceed, or whether you need to spend more time.\nProviding feedback. We were to teach each other a short lesson of our choosing, which was pretty fun. After a test run with a larger group, my partner and I got to teach each other - she taught me about making jam (and the food safety issues) and I taught her about making cornbread. (It was close to lunchtime when we did this, I think we were both hungry.) The exercise was super useful in helping us to not be afraid of giving feedback and to be aware of what kinds of feedback to give.\nWhat demotivates students. This was an important section, and a bit emotionally difficult. I have been guilty of some of the teaching behavior that demotivates students, such as taking over their keyboard, instead of letting them fix it. If we are to be inclusive and welcoming to everyone, we need to be aware of these (sometimes subtle) behaviors that can discourage and demotivate all of our students.\nOther tips from other instructors. I really enjoyed hearing other instructor’s experiences and I had a lot of respect for my cohort. There were lots of little tips we shared with each other that were super helpful, such as the types of mics to use. I also thought that it was helpful when we shared our struggles with each other. It made me feel much more as part of a group of instructors, which was very helpful for me."
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#instructor-certification-exam",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#instructor-certification-exam",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "Instructor Certification Exam",
    "text": "Instructor Certification Exam\nThe instructor certification covered the pedagogical techniques we discussed in the instructor training. This exam, like the others, was open book, open note, open internet, but not open person (no lifeline, no talking with others).\nAs part of the exam, we were required to develop and deliver a lesson. I ended up reworking one of my SQL lessons in my Analytics course. I found it a really useful exercise in rethinking my examples discussing left and inner joins, especially in my table examples. You can see my lesson here: SQL Joins in R.\nThis part of the exam is only about 10 minutes long, but you should be prepared, especially in figuring out how you will assess the learning of the students."
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#tidyverse-certification-exam",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#tidyverse-certification-exam",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "Tidyverse Certification Exam",
    "text": "Tidyverse Certification Exam\nAfter I had done my instructor certification, I had to do the technical qualifications. This was also done online, with Greg delivering the exam over Zoom. Most of the exam had to be done via RStudio on my computer. This exam, like the others, was open book, open note, open internet, but not open person (no lifeline, no talking with others).\nWhat did the exam cover? Roughly, it covered almost all of R for Data Science and was very task-focused. There was a focus on debugging, data transformation, markdown and visualization. The tasks were mostly problem driven, on the order of recreating a figure, or parametrizing a workflow. It was emphasized that we should solve the problems the way we usually work and talk out our problems aloud. For one of the problems, I actually had to search and teach myself on the fly. This was actually a good way to confirm that my conceptual models of the tidyverse were helpful.\nOne tip: 90 minutes is a long amount of time for an exam, especially where you are actively thinking and talking out loud. You should have something to eat by your side so you can replenish your blood glucose. It is also slightly unnerving to have someone watch you actively solve problems and program. If you get flustered, take a breath. There’s not usually a single way to solve the problems."
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#shiny-certification-exam",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#shiny-certification-exam",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "Shiny Certification Exam",
    "text": "Shiny Certification Exam\nA few weeks later, I took the Shiny certification. In terms of coverage, there is not currently a good text to study for the Shiny Certification exam (Mastering Shiny is probably going to be the reference to study when it is finished). In order to prepare, I went over the RStudio articles about Shiny, and looked over the RStudio Shiny Gallery to bone up on the different programming techniques used in Shiny, such as UI elements, reactives, the observe/update pattern, and modules.\nAgain, the exam covered debugging, including fixing applications, understanding control logic between ui and server, and building an app from scratch. I would say much of the exam covers how well established your own mental model of Shiny progamming is.\nOne thing to remember: as the technology and code changes, the certification exams will change and you may have to get re-certified. So my experience of these exams may be different than your experience."
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#overall-experience",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#overall-experience",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "Overall Experience",
    "text": "Overall Experience\nWould I recommend the training? I thought it was extremely useful, especially in terms of learning more about evidence based methods in effective education. As someone who is thinking about starting a consulting group focusing on training, having the certifications are extremely valuable to me and my future career. I look forward to teaching and training more people in R."
  },
  {
    "objectID": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#for-more-info",
    "href": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/2019-11-15-my-experience-with-rstudio-instructor-training.html#for-more-info",
    "title": "Notes on the RStudio Instructor Training Experience",
    "section": "For More Info",
    "text": "For More Info\nPlease consult the RStudio Trainer Website for info on how to register for the instructor training."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "",
    "text": "The learnr package is a wonderful way to package your tutorials. Anyone can download a learnr package of tutorials, and use learnr::run_tutorial() to run them on their own personal system.\nSince learnr is based on shiny, these tutorials can also be published to a Shiny server such as shinyapps.io. However, one drawback to this is that the more popular the tutorial, the more access time you may be on the hook for, which can rapidly become expensive.\nEnter mybinder.org, which is a way of running reproducible analyses and tutorials. In short, you give a mybinder.org server some information about the software environment needed to run your code, and you can run code based on your repo. Mybinder.org servers are a little limited, (1 Gb RAM), but this is more than capable of running most learnr tutorials.\nRecently, they added Shiny as a deployable format, which means you can run shiny apps, including learnr tutorial packages off their servers.\nI’m starting a learnr tutorial package called tidyowl and I decided to share what I’ve learned."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#whats-the-problem",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#whats-the-problem",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "What’s the problem?",
    "text": "What’s the problem?\nHowever, there are some differences between the file structure of a learnr package and the expected file structure of a shiny-ready mybinder.org repository.\nCan we get both setups to work at the same time? Yes, we can."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#why-would-we-want-to-run-learnr-tutorials-off-mybinder.org",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#why-would-we-want-to-run-learnr-tutorials-off-mybinder.org",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Why would we want to run learnr tutorials off mybinder.org?",
    "text": "Why would we want to run learnr tutorials off mybinder.org?\nThe short answer is making your material accessible to as many people as possible. learnr tutorials can be run from a phone or tablet, and running your tutorial for a lot of people doesn’t cost you any bandwidth or usage costs, as you’re using the same infrastructure that mybinder.org provides."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#quick-review-of-learnr-package-structure",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#quick-review-of-learnr-package-structure",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Quick Review of learnr package structure",
    "text": "Quick Review of learnr package structure\nFor a package, learnr tutorials are stored in the following folder:\ninst/tutorials/TUTORIAL_NAME\nSo, if you had a tutorial named learning_shiny it would live in\ninst/tutorials/learning_shiny/\nand the .Rmd file containing the tutorial should be named learning_shiny.Rmd as well.\nThe problem I encountered is that mybinder.org expects your tutorial to exist as a folder in the root of the repo. In other words, it needs to see\nlearning_shiny/ in the root of the repository to run.\nWe can fix this by adding a file called postBuild that gives instructions to run after the software environment is built. We’ll use it to copy the tutorials into the root folder."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#making-your-tutorial-mybinder.org-ready",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#making-your-tutorial-mybinder.org-ready",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Making your tutorial mybinder.org ready",
    "text": "Making your tutorial mybinder.org ready\nIn short, you’ll need 3 files to make your learnr tutorial mybinder.org ready: a runtime.txt, a install.R, and a postBuild file in order to make your learnr package compatible with mybinder.org. Let’s go through the steps:\nStep 1. Specify a runtime.txt file in your root folder. You’ll need a file called runtime.txt that contains a single line:\nr-3.6-2020-08-01\nThis gives mybinder.org the signal that the Docker image needs to have R installed. You can see that I specified a version (3.6) and a snapshot date (2020-08-01). These should be a valid version and date for the snapshot - check the MRAN pages for more info: https://mran.microsoft.com/\nNote: When R 4.0 and greater is available in MRAN, you should move to it. It includes RStudio Package Manager, which installs the binary images rather than installing from source code which speeds up building the Docker images by quite a bit.\nAnother Note: I tried to get this to work with a Dockerfile using the rocker/binder images, but I couldn’t get this image to work. If anyone has gotten this working, I’d appreciate you sharing how you did it.\nStep 2. Specify package dependencies using install.R in your root folder - in this file, you’ll need to specify all the packages your tutorial is dependent on using install.packages() commands. Here’s the contents of my install.R file:\ninstall.packages(\"learnr\")\ninstall.packages(\"here\")\ninstall.packages(\"tidyverse\")\nNote: getting the dependencies right in package building can be major headaches to getting your binder container to work. You may have to specify some system dependencies in your apt.txt file for certain packages. This information is available here: https://github.com/rstudio/r-system-requirements\nStep 3. Specify moving the tutorials in inst/tutorials/ to the repository root folder using the postBuild folder. These commands are run after the container is built and will make the tutorials accessible via Binder.\nFor example, for the tidyowl package, I have these mv commands in my postBuild:\nmv inst/tutorials/learning_tidyselect/ .\nmv inst/tutorials/learning_rowwise/ .\nYou’ll need a mv line for each tutorial that your package contains.\n\nNote: Using holepunch\nI believe you can also use holepunch to make setup a little easier. https://github.com/karthik/holepunch\nI haven’t tried it yet, but will update this when I do."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#build-the-docker-image-for-your-tutorial",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#build-the-docker-image-for-your-tutorial",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Build the Docker Image for your tutorial",
    "text": "Build the Docker Image for your tutorial\nOkay, almost there! Now we’re going to go to mybinder.org to build your Docker image. This is the software environment that your tutorial will run off of. This image will have shiny-server and RStudio installed on it automatically, which makes debugging your package easier.\nWhen you’re ready, go to https://mybinder.org and put in the public location of your repository. Then click the “Launch” button.\nNow your container will build. Note that this will take a little while (10+ minutes), especially if you need to install something like tidyverse. Note that this can be one of the hardest steps to get going, especially if you need packages such as sf (see above for a link to system dependencies).\n\nWhen it’s done building, you’ll be at a Jupyter page. Click “New >> RStudio” to open up your image with RStudio.\n\nYou should see that your tutorial folders have been moved to the root folders. This is good confirmation that the mv statements of postBuild work. I personally like to have individual data/ folders in each tutorial, as it makes making them a little easier to deploy.\nTest out running the tutorial by going to the .Rmd file and running it.\n\nIf you’ve setup everything right, you should see your learnr tutorial popup. I will say that this is usually the fine tuning step that takes the longest.\nNote: there is a GitHub action (https://github.com/jupyterhub/repo2docker-action) to rebuild your Docker image on new commits. I’ll be looking into this in the future."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#specify-the-url-for-running-your-tutorial.",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#specify-the-url-for-running-your-tutorial.",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Specify the URL for running your tutorial.",
    "text": "Specify the URL for running your tutorial.\nEach tutorial in your package will need its own URL to run.\nYou’ll add the following to your mybinder.org link:\n?urlpath=shiny/learning_tidyselect/\nThe urlpath is a signal to mybinder that it will need to run shiny, and you’ll put the name of your tutorial folder instead of learning_tidyselect. Note the trailing slash after learning_tidyselect.\nSo, my final URL for the learning_tidyselect tutorial is this (click it and try it out):\nhttps://mybinder.org/v2/gh/laderast/tidyowl/master?urlpath=shiny/learning_tidyselect/\nYou can now send this link out and nearly anyone in the world can run your learnr tutorial without installing R and not using up precious Shinyapps.io CPU time!"
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#caveats",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#caveats",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Caveats",
    "text": "Caveats\nShiny apps may suddenly disconnect - have students reload the page if that happens.\nImages get deleted off the mybinder.org servers within a week, so it is worth automating your container build to do so every week so that your students don’t have to wait for your container image to rebuild.\nAlso, progress is not saved, because the final url is different each time you run it off mybinder.org servers."
  },
  {
    "objectID": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#acknowledgements",
    "href": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org.html#acknowledgements",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks so much to the mybinder.org team, what they do is beyond awesome.\nThanks to Sang Yun Oh, whose repository helped me figure more of these details out. https://github.com/syoh/learnr-tutorial"
  },
  {
    "objectID": "talks/2021-03-22-the-md-in-rmd-teaching-clinicians-analytics-usin-r/index.html",
    "href": "talks/2021-03-22-the-md-in-rmd-teaching-clinicians-analytics-usin-r/index.html",
    "title": "The MD in .Rmd: Teaching Clinicians Analytics using R",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {The {MD} in {.Rmd:} {Teaching} {Clinicians} {Analytics} Using\n    {R}},\n  date = {2021-03-22},\n  url = {https://laderast.github.io//talks/2021-03-22-the-md-in-rmd-teaching-clinicians-analytics-usin-r},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “The MD in .Rmd: Teaching Clinicians Analytics\nUsing R.” March 22, 2021. https://laderast.github.io//talks/2021-03-22-the-md-in-rmd-teaching-clinicians-analytics-usin-r."
  },
  {
    "objectID": "talks/2021-03-22-object-oriented-systems-in-r/index.html",
    "href": "talks/2021-03-22-object-oriented-systems-in-r/index.html",
    "title": "Object Oriented Systems in R",
    "section": "",
    "text": "Description\nThis is a talk/workshop summarizing three different object oriented programming systems in R: S3, S4, and R6. It is meant for intermediate R Programmers.\nThis talk contains two notebooks explaining S4 and R6 that can be found at the repository link below.\n\n\nIntroductory Slides\n\n\n\n\n\n\nLinks\nWorkshop Repository\n\n\n\n\nCitationBibTeX citation:@online{laderas2019,\n  author = {Ted Laderas and Scott Chamberlain},\n  title = {Object {Oriented} {Systems} in {R}},\n  date = {2019-05-21},\n  url = {https://laderast.github.io//talks/2021-03-22-object-oriented-systems-in-r},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas, and Scott Chamberlain. 2019. “Object Oriented Systems\nin R.” May 21, 2019. https://laderast.github.io//talks/2021-03-22-object-oriented-systems-in-r."
  },
  {
    "objectID": "talks/2021-03-22-psychological-safety/index.html",
    "href": "talks/2021-03-22-psychological-safety/index.html",
    "title": "Psychological Safety",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Psychological {Safety}},\n  date = {2021-03-22},\n  url = {https://laderast.github.io//talks/2021-03-22-psychological-safety},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Psychological Safety.” March 22, 2021.\nhttps://laderast.github.io//talks/2021-03-22-psychological-safety."
  },
  {
    "objectID": "talks/2021-03-20-system-science-and-data-science/index.html",
    "href": "talks/2021-03-20-system-science-and-data-science/index.html",
    "title": "System Science and Data Science",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2018,\n  author = {Ted Laderas},\n  title = {System {Science} and {Data} {Science}},\n  date = {2018-02-15},\n  url = {https://laderast.github.io//talks/2021-03-20-system-science-and-data-science},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2018. “System Science and Data Science.”\nFebruary 15, 2018. https://laderast.github.io//talks/2021-03-20-system-science-and-data-science."
  },
  {
    "objectID": "talks/2021-03-23-data-scavenger-hunt-exploring-data-together/index.html",
    "href": "talks/2021-03-23-data-scavenger-hunt-exploring-data-together/index.html",
    "title": "Data Scavenger Hunt: Exploring Data Together",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2019,\n  author = {Ted Laderas},\n  title = {Data {Scavenger} {Hunt:} {Exploring} {Data} {Together}},\n  date = {2019-05-09},\n  url = {https://laderast.github.io//talks/2021-03-23-data-scavenger-hunt-exploring-data-together},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2019. “Data Scavenger Hunt: Exploring Data\nTogether.” May 9, 2019. https://laderast.github.io//talks/2021-03-23-data-scavenger-hunt-exploring-data-together."
  },
  {
    "objectID": "talks/2023-02-13-compassion-data-science/index.html",
    "href": "talks/2023-02-13-compassion-data-science/index.html",
    "title": "The Value of Compassion in Learning Data Science",
    "section": "",
    "text": "Keynote for University of Pittburgh’s Love Data Week.\nLearning data science skills effectively requires a culture of compassion and psychological safety. However, this culture is not guaranteed within academia. In this talk, I will share effective strategies for building a culture of compassion, starting with yourself, and building up a safe learning environment for others. I will highlight this with examples on my own learning and teaching journey. I will show you strategies for learning, teaching, and building an inclusive learning environment for you and your peers.\n\n\n\n\n\n\n\n\n\n\n\nSlides Slide Repo YouTube Video\n\n\n\nCitationBibTeX citation:@online{laderas,\n  author = {Ted Laderas},\n  title = {The {Value} of {Compassion} in {Learning} {Data} {Science}},\n  url = {https://laderast.github.io//talks/2023-02-13-compassion-data-science},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. n.d. “The Value of Compassion in Learning Data\nScience.” https://laderast.github.io//talks/2023-02-13-compassion-data-science."
  },
  {
    "objectID": "talks/2021-06-06-why-shiny-modules/index.html",
    "href": "talks/2021-06-06-why-shiny-modules/index.html",
    "title": "Why Shiny Modules",
    "section": "",
    "text": "Slides\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n\nLinks\nRepository\n\n\n\n\nCitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Why {Shiny} {Modules}},\n  date = {2021-06-06},\n  url = {https://laderast.github.io//talks/2021-06-06-why-shiny-modules},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Why Shiny Modules.” June 6, 2021. https://laderast.github.io//talks/2021-06-06-why-shiny-modules."
  },
  {
    "objectID": "talks/2021-03-20-data-storytelling/index.html",
    "href": "talks/2021-03-20-data-storytelling/index.html",
    "title": "Data Storytelling",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Data {Storytelling}},\n  date = {2021-03-20},\n  url = {https://laderast.github.io//talks/2021-03-20-data-storytelling},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Data Storytelling.” March 20, 2021. https://laderast.github.io//talks/2021-03-20-data-storytelling."
  },
  {
    "objectID": "talks/2021-03-22-conversations-about-sleep-clinical-data-wrangling/index.html",
    "href": "talks/2021-03-22-conversations-about-sleep-clinical-data-wrangling/index.html",
    "title": "Conversations about Sleep: Clinical Data Wrangling",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Conversations about {Sleep:} {Clinical} {Data} {Wrangling}},\n  date = {2021-03-22},\n  url = {https://laderast.github.io//talks/2021-03-22-conversations-about-sleep-clinical-data-wrangling},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Conversations about Sleep: Clinical Data\nWrangling.” March 22, 2021. https://laderast.github.io//talks/2021-03-22-conversations-about-sleep-clinical-data-wrangling."
  },
  {
    "objectID": "talks/2022-09-14-qmd-rmd/index.html",
    "href": "talks/2022-09-14-qmd-rmd/index.html",
    "title": "Quarto vs. RMarkdown - What’s Different?",
    "section": "",
    "text": "Talk given for FCT Abuja useR group. Learn about:\n\nWhat Quarto Is\nWhy switch to Quarto?\nWorkflow for switching to Quarto from RMarkdown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{laderas2022,\n  author = {Ted Laderas},\n  title = {Quarto Vs. {RMarkdown} - {What’s} {Different?}},\n  date = {2022-09-15},\n  url = {https://laderast.github.io//talks/2022-09-14-qmd-rmd},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2022. “Quarto Vs. RMarkdown - What’s\nDifferent?” September 15, 2022. https://laderast.github.io//talks/2022-09-14-qmd-rmd."
  },
  {
    "objectID": "talks/2022-10-7-using-cognitive-science/index.html",
    "href": "talks/2022-10-7-using-cognitive-science/index.html",
    "title": "Using Cognitive Science to Make Training Stick",
    "section": "",
    "text": "Talk given for DNAnexus Science Deep Dive.\n\nDescribe the central tenet of cognitive load theory\nDescribe the process behind instructional design and how it benefits students\nExplain why designing active Learning exercises are difficult\n\n\n\nSlides (PDF)\n\n\n\nCitationBibTeX citation:@online{laderas2022,\n  author = {Ted Laderas},\n  title = {Using {Cognitive} {Science} to {Make} {Training} {Stick}},\n  date = {2022-10-07},\n  url = {https://laderast.github.io//talks/2022-10-7-using-cognitive-science},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2022. “Using Cognitive Science to Make Training\nStick.” October 7, 2022. https://laderast.github.io//talks/2022-10-7-using-cognitive-science."
  },
  {
    "objectID": "talks/2021-06-06-using-gratitude-to-learn-the-tidyverse-together/index.html",
    "href": "talks/2021-06-06-using-gratitude-to-learn-the-tidyverse-together/index.html",
    "title": "Using gRatitude to learn the tidyverse together",
    "section": "",
    "text": "CitationBibTeX citation:@online{laderas2021,\n  author = {Ted Laderas},\n  title = {Using {gRatitude} to Learn the `Tidyverse` Together},\n  date = {2021-06-04},\n  url = {https://laderast.github.io//talks/2021-06-06-using-gratitude-to-learn-the-tidyverse-together},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTed Laderas. 2021. “Using gRatitude to Learn the `Tidyverse`\nTogether.” June 4, 2021. https://laderast.github.io//talks/2021-06-06-using-gratitude-to-learn-the-tidyverse-together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ted Laderas, PhD",
    "section": "",
    "text": "I am a bioinformatics trainer for DNANexus, formerly an Assistant Professor in Biomedical Informatics at OHSU. I train bioinformaticians how to use the DNAnexus cloud-based platform. This includes giving webinars for the UK Biobank Research Analysis Platform. I am also a Data Science Mentor for Posit Academy, helping data apprenctices learn the tidyverse.\nCollaboration, training, and team science are passions of mine. I believe that research should not be lonely, and building communities of practice in science and research that are psychologically safe and inclusive are the key to doing better, more robust science. To this end, I use evidence based pedagogy, especially utilizing the concepts of psychological safety and active learning. I am a certified instructor for both The Carpentries and Posit Academy.\nMy free Ready for R course currently is at over 1500 external users, and I have built a learning community around it. The course has been lauded by users as being inclusive and less intimidating than other R courses. Our Data Analytics course recently won an award (Sakai Torchbearer Award) from multiple student nominations."
  },
  {
    "objectID": "2022-retrospective.html",
    "href": "2022-retrospective.html",
    "title": "Ted Laderas, PhD",
    "section": "",
    "text": "Ted Laderas\n2022 was a fairly busy year for me, not just with Academy engagements, but also in outreach and working on improving the user experience with effective workshops, courses, and developing optimal workflows for getting the main tasks on the platform done.\nI've achieved this by observing user experiences and questions from the UKB RAP community, and from questions customers have asked during training sessions.\n\n\nOne of my major passions in my work is to improve the overall user experience of the DNAnexus platform by establishing and suggesting effective workflows and best practices for getting tasks accomplished on the platform. These tasks include: executing batch jobs on the platform, working with RStudio Effectively on RAP, working with Docker, decoding pheno data from raw values, and using bash effectively on RAP.\nEach of these accomplishments has stemmed from observing gaps in customer's knowledge in the platform. I believe that one of the biggest barriers to using the platform is still the feeling of impostor syndrome and I am passionate about giving customers the knowledge they need in formats that are helpful for them.\n\n104 - Cloud Computing for HPC Users. This was a course and > webinar that I developed with Anastazie Sedlakova and Scott > Funkhouser. It attempts to directly map skills and concepts that > incoming users already have with on-premise HPC and directly > translates these skills to cloud HPC. It has been rated highly by > both UKB RAP users and other customers.\n312 - JSON for the DNAnexus Platform - This is a course that I > developed that fills in a lot of the gaps with utilizing JSON > effectively on the DNAnexus platform. It helps users who are > unfamiliar with JSON read and modify JSON for use in applets, but > also in parsing JSON responses with the platform using jq, and > doing batch submissions with jq.\nBash for > Bioinformatics - > In response to many of the challenges that I have seen with > customers and app development and running jobs, I put together a > \"missing manual\" that addresses the gaps in knowledge of bash > scripting and where this knowledge is helpful on the platform. It > has gotten positive reception not only on the UKB RAP community, > but also by Solution Science as a resource they can point > potential customers to.\nRStudio for RAP webinar - With Anastazie Sedlakova, I developed > a webinar that outlined how to work reproducibly with RStudio on > UKB RAP. This involved not only showing users how to use RStudio > Projects, but also how to save software environments for > reproducible research, and run RStudio/Bioconductor via Docker > containers.\nDocker for RAP webinar - This was a webinar that I developed > with Ondrej Klempir on how to effectively use Docker on UKB RAP > for batch processing. It provides specific bash recipes for saving > docker image files, extending docker images, and running batch > jobs with Docker image files.\ndxhelper - > this is an R Package I have developed to help R users with > decoding Pheno data extracted from the new dx extract_dataset > functionality in dx-toolkit. R users are a major percentage of UKB > RAP users, and this package will helps them with their overall > user experience with the pheno data.\n\n\n\n\nI have helped onboard a number of our new employees through both Bootcamp and a formal set of courses. This was in direct response to Dick's call for more internal training specifically for our sales team.\n\nOngoing training with Alex Edwards. Alex and I continue to > cross-train each other on various features of the platform, > including Apollo specific features. He's now ready to train OFH > alpha users on these features. I hope to learn more about the > ingestion process next year.\nTrained 2 incoming groups on platform, including Apollo > features. We have increased our overall training in our > onboarding program, including incoming members of the Sales team.\n\nThis has included frank discussions and addressing questions the > Sales team has about the product, and helping them develop the > language to effectively communicate about the platform.\nI believe this has helped not only in increasing knowledge, but > also communication between various groups at DNAnexus.\n\nGave Science Deep Dive talk on Teaching: Using Cognitive > Science to Make Training > Stick\nTalk for xVantage Day: Using Quarto: > https://laderast.github.io/qmd_rmd/\nTrained VN team on platform. As part of the Vietnam Team's > onboarding, we delivered the Titan/Apollo courses to them.\nOutreach to other groups. I continue to work and communicate > with members of Customer Care, Translational Medicine, Customer > Success, UKB RAP, and Solution Science\n\n\n\n\nI want others to effectively utilize the platform and accomplish work with it. To this end, I have helped with outreach to a number of new audiences with Ben Busby and the UKB RAP team. I\n\nRAP getting started workshops. I sat in on 5 sessions that were > a combination of workshop and Q&A for new UKB RAP users. I was > actively answering questions in the chat and providing links to > material.\nHDR UK - I was part of a group of educators talking about TREs > and their role in working with clinical data. Specifically I > talked about the role of UKB RAP and how it fulfilled the > requirements of a TRE (patient security) in spite of it being a > cloud-based solution\nImaging Workshop - I helped Renee Qian with setup and running > the MATLAB container on UKB RAP for the imaging workshop, > including showing her how to use dxFUSE for her demo.\nISCB Academy - Ben and I presented various UKB RAP features as a > workshop for ISCB Academy, and shared Bash for Bioinformatics as a > resource for others.\nI developed and shared an R specific workflow for UKB RAP for > exporting, decoding, and working with Pheno data using my dxhelper > package. It was used for the in-person UKB RAP workshop this > December.\n\n\n\n\n\n\n\nAs Academy, we continue to innovate and deliver effective training through our use of active learning exercises and other modalities.\nOne of the ongoing challenges is training customers on the different versions of the platforms (OFH Alpha/Beta, Optum Clinico-Genomics, UKB RAP, and the GA platform).\nOne of our goals is to integrate new features as shared by product as quickly into course materials as possible. Sometimes the material provided by product needs improvement in terms of user experience, and we aim to deliver improved notebooks so we can better teach the respective topic.\nThis is a list of the courses that I have actively developed this year:\n\n\n\n\n\n\n\n\nCourses\nPlatform\nDescription of work\n\n\n\n\n\n**103 Cloud > computing > for\n\n\nScientists**\n\n\n103 Cloud > computing > for > Scientists > (On > Demand)\n103 Cloud > computing > for > Scientists > (RAP)\n\nCore\n\nDevelopment of webinar > version (for RAP)\nDevelopment of on-demand > course. Currently in > testing.\n\n\n\n\n104 > DNAnexus > for HPC > Users\n104 > DNAnexus > for HPC > Users > (RAP)\n\nCore/RAP\n\nDevelopment of material and > webinar for RAP.\nCourse has also been adapted > as a GA-specific course > with active learning > exercises showing both > running single and batch > jobs using the dx-toolkit.\n\n\n\n\n272 Complex > Cohort > Analysis\n\nCore\n\nExercises have been updated > with complete answers > (including cohort diagrams) > and opportunities for > discussion\nExercises have been added for > cohort combine > functionality of Cohort > Browser\n\n\n\n\n340R\n\n\nIntroduction\n\n> to WDL for\n> DNAnexus\n> users (RAP)\nRAP\n\nAdapted existing WDL course > for a 2 session webinar for > RAP.\n\n\n\n\nDocker for > UKB RAP > Webinar\n\nRAP\n\nBest practices for running > docker images within > applications, including\n\nPulling/saving docker > image files\nUsing docker image files > in applets\nExtending docker images > using interactive mode > and Dockerfiles\n\n\n\n\n\n302 (On > Demand) - > Command > Line for > UKB RAP\n\nRAP\n\nDevelopment of videos and > exercises showing effective > ways to work with the > command-line on RAP\n\n\n\n\n270 > JupyterLabs > update\n\nCore\n\nThe JupyterLabs course has > been updated to pheno data > extraction using dx > extract_data. R and Python > Specific notebooks have > been added for working with > the raw extracted data.\nNotebooks have also been > added to show how to > extract the data using a > Spark dxjupyter cluster.\n\n\n\n\n101 Optum\n170 Optum\n270 Optum\n\nOptum Cli nicogenomics Platform\n\nThe Apollo training program > was adapted to incorporate > Clinicogenomic specific > features and restrictions > (specifically download > restrictions).\nThis especially impacts how > users use JupyterLab > (download restrictions > prevent opening notebooks > directly on the platform)\n\n\n\n\n101 OFH\n170 OFH\n270 OFH\n\nOFH Alpha platform\n\nApollo training program was > adapted for OFH Alpha > specific features, > including > restricted/unrestricted > projects and > download/upload > restrictions\n\n\n\n\n**312\n\n\nIntroduction\n\n> to JSON**\nAll\n\nDevelopment of course > exploring the use of JSON > on the platform.\nExercises processing JSON > responses using jq have > been added\nCourse to be used as a > prerequisite for App > Building course (310) where > necessary\n\n\n\n\n313 (RAP) - > Running > Apps on UKB > RAP\n314 (RAP) - > Building > Apps on UKB > RAP\n\nRAP\n\nAdded exercises in running > existing apps on RAP, > including batch mode\nAdded exercises in building > applets that map closely to > the overall development > process"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Ted Laderas, PhD",
    "section": "",
    "text": "Ted Laderas\n\n\n2022 was a fairly busy year for me, not just with Academy engagements, but also in outreach and working on improving the user experience with effective workshops, courses, and developing optimal workflows for getting the main tasks on the platform done. \n\nI’ve achieved this by observing user experiences and questions from the UKB RAP community, and from questions customers have asked during training sessions.\n\n\n\nOne of my major passions in my work is to improve the overall user experience of the DNAnexus platform by establishing and suggesting effective workflows and best practices for getting tasks accomplished on the platform. These tasks include: executing batch jobs on the platform, working with RStudio Effectively on RAP, working with Docker, decoding pheno data from raw values, and using bash effectively on RAP.\n\nEach of these accomplishments has stemmed from observing gaps in customer’s knowledge in the platform. I believe that one of the biggest barriers to using the platform is still the feeling of impostor syndrome and I am passionate about giving customers the knowledge they need in formats that are helpful for them.\n\n\n104 - Cloud Computing for HPC Users. This was a course and webinar that I developed with Anastazie Sedlakova and Scott Funkhouser. It attempts to directly map skills and concepts that incoming users already have with on-premise HPC and directly translates these skills to cloud HPC. It has been rated highly by both UKB RAP users and other customers.  \n312 - JSON for the DNAnexus Platform - This is a course that I developed that fills in a lot of the gaps with utilizing JSON effectively on the DNAnexus platform. It helps users who are unfamiliar with JSON read and modify JSON for use in applets, but also in parsing JSON responses with the platform using jq, and doing batch submissions with jq.\nBash for Bioinformatics - In response to many of the challenges that I have seen with customers and app development and running jobs, I put together a “missing manual” that addresses the gaps in knowledge of bash scripting and where this knowledge is helpful on the platform. It has gotten positive reception not only on the UKB RAP community, but also by Solution Science as a resource they can point potential customers to.  \nRStudio for RAP webinar - With Anastazie Sedlakova, I developed a webinar that outlined how to work reproducibly with RStudio on UKB RAP. This involved not only showing users how to use RStudio Projects, but also how to save software environments for reproducible research, and run RStudio/Bioconductor via Docker containers. \nDocker for RAP webinar - This was a webinar that I developed with Ondrej Klempir on how to effectively use Docker on UKB RAP for batch processing. It provides specific bash recipes for saving docker image files, extending docker images, and running batch jobs with Docker image files.\ndxhelper - this is an R Package I have developed to help R users with decoding Pheno data extracted from the new dx extract_dataset functionality in dx-toolkit. R users are a major percentage of UKB RAP users, and this package will helps them with their overall user experience with the pheno data.\n\n\n\n\n\n\nI have helped onboard a number of our new employees through both Bootcamp and a formal set of courses. This was in direct response to Dick’s call for more internal training specifically for our sales team.\n\n\nOngoing training with Alex Edwards. Alex and I continue to cross-train each other on various features of the platform, including Apollo specific features. He’s now ready to train OFH alpha users on these features. I hope to learn more about the ingestion process next year.\nTrained 2 incoming groups on platform, including Apollo features. We have increased our overall training in our onboarding program, including incoming members of the Sales team. \n\n\n\nThis has included frank discussions and addressing questions the Sales team has about the product, and helping them develop the language to effectively communicate about the platform. \nI believe this has helped not only in increasing knowledge, but also communication between various groups at DNAnexus. \n\n\n\nGave Science Deep Dive talk on Teaching: Using Cognitive Science to Make Training Stick\nTalk for xVantage Day: Using Quarto: https://laderast.github.io/qmd_rmd/ \nTrained VN team on platform. As part of the Vietnam Team’s onboarding, we delivered the Titan/Apollo courses to them.\nOutreach to other groups. I continue to work and communicate with members of Customer Care, Translational Medicine, Customer Success, UKB RAP, and Solution Science          \n\n\n\n\n\n\nI want others to effectively utilize the platform and accomplish work with it. To this end, I have helped with outreach to a number of new audiences with Ben Busby and the UKB RAP team. I \n\n\nRAP getting started workshops. I sat in on 5 sessions that were a combination of workshop and Q&A for new UKB RAP users. I was actively answering questions in the chat and providing links to material.\nHDR UK - I was part of a group of educators talking about TREs and their role in working with clinical data. Specifically I talked about the role of UKB RAP and how it fulfilled the requirements of a TRE (patient security) in spite of it being a cloud-based solution\nImaging Workshop - I helped Renee Qian with setup and running the MATLAB container on UKB RAP for the imaging workshop, including showing her how to use dxFUSE for her demo.\nISCB Academy - Ben and I presented various UKB RAP features as a workshop for ISCB Academy, and shared Bash for Bioinformatics as a resource for others.\nI developed and shared an R specific workflow for UKB RAP for exporting, decoding, and working with Pheno data using my dxhelper package. It was used for the in-person UKB RAP workshop this December.\n\n\n\n\n\n\n\n\n\nAs Academy, we continue to innovate and deliver effective training through our use of active learning exercises and other modalities. \n\nOne of the ongoing challenges is training customers on the different versions of the platforms (OFH Alpha/Beta, Optum Clinico-Genomics, UKB RAP, and the GA platform). \n\nOne of our goals is to integrate new features as shared by product as quickly into course materials as possible. Sometimes the material provided by product needs improvement in terms of user experience, and we aim to deliver improved notebooks so we can better teach the respective topic.\n\nThis is a list of the courses that I have actively developed this year:\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourses\n\n\n\n\nPlatform\n\n\n\n\nDescription of work\n\n\n\n\n\n\n\n103 Cloud computing for Scientists\n\n\n103 Cloud computing for Scientists (On Demand)\n\n\n103 Cloud computing for Scientists (RAP)\n\n\n\n\n\nCore\n\n\n\n\n\n\n\n\nDevelopment of webinar version (for RAP) \n\n\nDevelopment of on-demand course. Currently in testing.\n\n\n\n\n\n\n\n\n104 DNAnexus for HPC Users\n\n\n104 DNAnexus for HPC Users (RAP)\n\n\n\n\n\nCore/RAP\n\n\n\n\n\nDevelopment of material and webinar for RAP. \n\n\nCourse has also been adapted as a GA-specific course with active learning exercises showing both running single and batch jobs using the dx-toolkit.\n\n\n\n\n\n\n\n\n272 Complex Cohort Analysis\n\n\n\n\n\n\n\n\n\n\n\nCore\n\n\n\n\n\nExercises have been updated with complete answers (including cohort diagrams) and opportunities for discussion\n\n\nExercises have been added for cohort combine functionality of Cohort Browser\n\n\n\n\n\n\n\n\n\n\n\n340R Introduction to WDL for DNAnexus users (RAP)\n\n\n\n\n\n\n\n\nRAP\n\n\n\n\n\nAdapted existing WDL course for a 2 session webinar for RAP.\n\n\n\n\n\n\n\n\nDocker for UKB RAP Webinar\n\n\n\n\n\nRAP\n\n\n\n\n\nBest practices for running docker images within applications, including \n\n\n\n\nPulling/saving docker image files\n\n\nUsing docker image files in applets\n\n\nExtending docker images using interactive mode and Dockerfiles\n\n\n\n\n\n\n\n\n302 (On Demand) - Command Line for UKB RAP\n\n\n\n\n\nRAP\n\n\n\n\n\nDevelopment of videos and exercises showing effective ways to work with the command-line on RAP\n\n\n\n\n\n\n\n\n270 JupyterLabs update \n\n\n\n\n\nCore\n\n\n\n\n\nThe JupyterLabs course has been updated to pheno data extraction using dx extract_data. R and Python Specific notebooks have been added for working with the raw extracted data. \n\n\nNotebooks have also been added to show how to extract the data using a Spark dxjupyter cluster.\n\n\n\n\n\n\n\n\n101 Optum\n\n\n170 Optum\n\n\n270 Optum\n\n\n\n\n\nOptum Clinicogenomics Platform\n\n\n\n\n\nThe Apollo training program was adapted to incorporate Clinicogenomic specific features and restrictions (specifically download restrictions).\n\n\nThis especially impacts how users use JupyterLab (download restrictions prevent opening notebooks directly on the platform)\n\n\n\n\n\n\n\n\n101 OFH\n\n\n170 OFH\n\n\n270 OFH\n\n\n\n\n\nOFH Alpha platform\n\n\n\n\n\nApollo training program was adapted for OFH Alpha specific features, including restricted/unrestricted projects and download/upload restrictions\n\n\n\n\n\n\n\n\n312 Introduction to JSON\n\n\n\n\n\nAll\n\n\n\n\n\nDevelopment of course exploring the use of JSON on the platform. \n\n\nExercises processing JSON responses using jq have been added \n\n\nCourse to be used as a prerequisite for App Building course (310) where necessary\n\n\n\n\n\n\n\n\n313 (RAP) - Running Apps on UKB RAP\n\n\n314 (RAP)  - Building Apps on UKB RAP\n\n\n\n\n\nRAP\n\n\n\n\n\nAdded exercises in running existing apps on RAP, including batch mode\n\n\nAdded exercises in building applets that map closely to the overall development process\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoundary setting\nPositive interactions with Academy\nStronger sense of empowerment\nWeekly report\nInternal Training - Science Deep Dive"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Articles (usually instructional) are listed here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Data Science is Hard (and why that matters)\n\n\n\n\n\n\n\n\n\n\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Seafood Production and Consumption\n\n\n\n\n\nUnderstanding global cephalopod production.\n\n\n\n\n\n\nOct 12, 2021\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegistered Nurses in the United States and Territories\n\n\n\n\n\nUnderstanding wages for Registered Nurses.\n\n\n\n\n\n\nOct 5, 2021\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Network in a Non-slimy Way\n\n\n\n\n\nUse your curiosity to connect with others in a meaningful way.\n\n\n\n\n\n\nSep 13, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderrated Tidyverse Functions\n\n\n\n\n\nLearn about our assignment to teach the tidyverse to each other.\n\n\n\n\n\n\nDec 1, 2020\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nGetting LearnR tutorials to run on mybinder.org\n\n\n\n\n\nGetting Shiny, LearnR, and Mybinder.org to play together nicely.\n\n\n\n\n\n\nSep 15, 2020\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Crop Production\n\n\n\n\n\nUnderstanding crop production across the world.\n\n\n\n\n\n\nSep 3, 2020\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment analysis of Avatar\n\n\n\n\n\nUnderstanding Characters through Avatar Episode Scripts.\n\n\n\n\n\n\nAug 11, 2020\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee Data Exploration\n\n\n\n\n\nUnderstanding coffee production and consumption across the world.\n\n\n\n\n\n\nJul 8, 2020\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal Crossing\n\n\n\n\n\nLooking at the unique animal personalities in Animal Crossing.\n\n\n\n\n\n\nMay 5, 2020\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on the RStudio Instructor Training Experience\n\n\n\n\n\nNotes on the RStudio Instructor Training and certification exams.\n\n\n\n\n\n\nNov 15, 2019\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nPackage Building: How DESCRIPTION, NAMESPACE, roxygen, and devtools::document work together\n\n\n\n\n\nSome thoughts the package building process and how devtools::document() is at the center of it..\n\n\n\n\n\n\nFeb 12, 2019\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReframing Impostor Syndrome as the Road to Mastery\n\n\n\n\n\nSome thoughts about impostor syndrome.\n\n\n\n\n\n\nDec 13, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny and Tidyeval Part 1\n\n\n\n\n\nHow do you incorporate tidyeval and shiny together?\n\n\n\n\n\n\nAug 7, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup_by/summarize: Not just for numeric values!\n\n\n\n\n\nLearn some more about the many uses of group_by()/summarize().\n\n\n\n\n\n\nMay 24, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat We learned teaching Python to Neuroscience Students\n\n\n\n\n\nOur team-taught class introducing Neuroscience Graduate Program students to Python.\n\n\n\n\n\n\nJan 17, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPumpkins, Pumpkins, Pumpkins\n\n\n\n\n\nLearning about giant pumpkin contest winners.\n\n\n\n\n\n\nInvalid Date\n\n\nTed Laderas, Ted Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo You’ve Accidentally Checked in a Large File Into Git\n\n\n\n\n\nDon’t panic. I’ll show you how to scrub your Git history and get rid of it.\n\n\n\n\n\n\nJan 5, 2018\n\n\nTed Laderas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Retrospective and Themes for 2022\n\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nRStudio Conf 2022 Highlights: Days 1 and 2\n\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2022\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‘Yes, and…’ is the foundation of collaboration\n\n\n\n\n\nHow the rules of comedy improv can improve your collaborations.\n\n\n\n\n\n\nFeb 27, 2022\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLife After Academia\n\n\n\n\n\nAfter 3 months leaving academia, how am I doing?\n\n\n\n\n\n\nJul 20, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoving On and Upwards\n\n\n\n\n\nI’m leaving OHSU. The reasons why.\n\n\n\n\n\n\nApr 29, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Blog\n\n\n\n\n\nReasons why I moved over to {distill} for my blog.\n\n\n\n\n\n\nMar 18, 2021\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting Boundaries and Saying No\n\n\n\n\n\n\n\nself-care\n\n\n\n\nHow saying no can be the best self-care.\n\n\n\n\n\n\nOct 14, 2020\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKindness at the Cost of Yourself\n\n\n\n\n\n\n\nself-care\n\n\n\n\nBeing overdrawn at the bank of kindness and self-care.\n\n\n\n\n\n\nSep 23, 2019\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBurning Out and Recovering (slowly)\n\n\n\n\n\n\n\n\n\n\nHow I’m recovering from burnout.\n\n\n\n\n\n\nJul 3, 2019\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nRstudio Conf 2019: Education and Organizations\n\n\n\n\n\n\n\nconference\n\n\n\n\nSome notes about RStudioConf 2019.\n\n\n\n\n\n\nJan 24, 2019\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nThings we learned teaching clinical data wrangling\n\n\n\n\n\n\n\nvisualization\n\n\nR\n\n\n\n\nMore information about managing depression and anxiety.\n\n\n\n\n\n\nOct 15, 2018\n\n\nTed Laderas, Eilis Boudreau, Ted Laderas, and Nicole Weiskopf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurning down the noise\n\n\n\n\n\n\n\n\n\n\nMore information about managing depression and anxiety.\n\n\n\n\n\n\nAug 2, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsking for help to get better\n\n\n\n\n\n\n\n\n\n\nLearning more about managing burnout.\n\n\n\n\n\n\nJun 25, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nCascadia-R 2018: How we planned it and the reasons why\n\n\n\n\n\n\n\nconference\n\n\n\n\nMore information about the 2018 Cascadia-R conference.\n\n\n\n\n\n\nJun 9, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf Care and Self Compassion in Academia\n\n\n\n\n\n\n\n\n\n\nWhy self-care is important in academia.\n\n\n\n\n\n\nJun 5, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Introvert’s Survival Guide to Conferences\n\n\n\n\n\n\n\nsocial\n\n\nnetworking\n\n\n\n\nAre you an introvert? Here’s how to make conferences easier.\n\n\n\n\n\n\nMay 17, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Informatics Course Materials\n\n\n\n\n\n\n\noer\n\n\nopen-education\n\n\ninformatics\n\n\n\n\nLinks to my lectures for our HSMP410 course.\n\n\n\n\n\n\nApr 25, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nA Simple Intro to Genome Wide Association\n\n\n\n\n\n\n\nteaching\n\n\n\n\nNotes on our preprint about synthetic clinical data.\n\n\n\n\n\n\nApr 4, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Notes from the Evidence to Scholarship Conference\n\n\n\n\n\n\n\n\n\n\nNotes on the Evidence to Scholarship Conference at Reed College.\n\n\n\n\n\n\nMar 19, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science and Systems Science\n\n\n\n\n\n\n\ndata-science\n\n\nsystems\n\n\n\n\nA talk about how system science and data science are connected.\n\n\n\n\n\n\nFeb 28, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n  \n\n\n\n\nA gRadual Introduction to Shiny\n\n\n\n\n\n\n\nvisualization\n\n\nR\n\n\n\n\nAn workshop introduction to Shiny\n\n\n\n\n\n\nJan 24, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to talk with me for an informational interview\n\n\n\n\n\n\n\ntalking\n\n\n\n\nI get a lot of requests for informational interviews. Here’s how to actually get one.\n\n\n\n\n\n\nJan 15, 2018\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Tidy Evaluation in R\n\n\n\n\n\nA metaphor for understanding Tidy Evaluation\n\n\n\n\n\n\nDec 19, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Synthetic Data for Teaching Data Science\n\n\n\n\n\nNotes on our preprint about synthetic clinical data.\n\n\n\n\n\n\nDec 14, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Open Data Science Conference West 2017\n\n\n\n\n\nSome notes on ODSC West 2017.\n\n\n\n\n\n\nNov 7, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteresting useR 2017 Talks\n\n\n\n\n\nuseR 2017 talks and links.\n\n\n\n\n\n\nJul 5, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Not Be Afraid of Your Data\n\n\n\n\n\nA Past Workshop about EDA and data exploration.\n\n\n\n\n\n\nJun 28, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Lessons We Learned Running Cascadia-R 2017\n\n\n\n\n\nHow we started and ran the first PNW Regional R Conference, Cascadia-R\n\n\n\n\n\n\nJun 7, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Breadth and Depth in Your Academic Career\n\n\n\n\n\nWhy Learning and hobbies outside of your field are important.\n\n\n\n\n\n\nApr 19, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFostering a Peer Mentoring Culture\n\n\n\n\n\nA bit about BioData Club\n\n\n\n\n\n\nApr 17, 2017\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurrogate Oncogene Paper is Published\n\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2015\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatcheck Interview\n\n\n\n\n\nInteresting interview with the developer of statcheck\n\n\n\n\n\n\nNov 19, 2015\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSomatic Mutations in Skin Paper\n\n\n\n\n\nMore about somatic mutations in skin cancer.\n\n\n\n\n\n\nMay 26, 2015\n\n\nTed Laderas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh Impact Factor Journals Have Higher Retraction Rates\n\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2015\n\n\nTed Laderas\n\n\n\n\n\n\nNo matching items"
  }
]